[{"title":"工业界推荐系统时长预估的常用方案","url":"/f5a7a06e/","content":"\n<!-- toc -->\n\n# 背景\n介绍几种工业界比较常用的时长建模方案。\n\n# Weighted Logloss\nWeighted Logloss是YouTube论文[<sup>1</sup>](#refer-anchor-1)提出的一种方法。将时长$t$转换成$\\frac{t}{t+1}$，把它当做正类label，那么负类label就是$\\frac{1}{t+1}$，可以套用交叉熵损失。\n$$\n\\begin{equation}\n\\label{weighted_logloss}\n\\tag{1}\n\\begin{aligned}\n& 真实时长：t \\in [0, +\\infty)\n\\\\ & 预估值：p = sigmoid(logit) \\in [0, 1]\n\\\\ & L_{weighted \\: logloss} = -\\frac{t}{t+1} \\log(p) - \\frac{1}{t+1} \\log(1-p)\n\\\\ & \\frac{\\partial{loss}}{\\partial{p}}=0 \\Longrightarrow 最优解：p = \\frac{t}{t+1}\n\\end{aligned}\n\\end{equation}\n$$\n\n实际中一般将loss中分母的$t+1$去掉，不影响以上推导过程，既：\n$$\n\\begin{equation}\n\\label{weighted_logloss2}\n\\tag{2}\n\\begin{aligned}\nL_{weighted \\quad logloss} = -t \\log(p) - \\log(1-p)\n\\end{aligned}\n\\end{equation}\n$$\n\n线上预估时，需要将预估的$p$转换成时长。通过$p = \\frac{t}{t+1}$得到$t=\\exp(logit)$，既用$\\exp(logit)$作为预估时长。\n$$\n\\begin{equation}\n\\label{weighted_logloss_serving}\n\\tag{3}\n\\begin{aligned}\nt_{restoration} &= \\frac{p}{1-p}\n\\\\ &= \\exp(logit)\n\\end{aligned}\n\\end{equation}\n$$\n\n\n# 时长归一化\n长视频的消费时长往往更高，所以Weighted Logloss中长视频的比重更大，更倾向于优化长视频的效果。所以，为了消除视频本身长短带来的偏差，可以先将训练样本按video duration分桶，比如0-1分钟的一档，1-5分钟的一档等等，然后每档内部按消费时长分段，最后将消费时长归一化到0到1内的值，用这个归一化后的值做label进行训练。\n\n$$\n\\begin{equation}\n\\label{normalized_logloss}\n\\tag{4}\n\\begin{aligned}\n& 真实时长：t \\in [0, +\\infty)\n\\\\ & 归一化后的时长：\\hat{t} \\in [0, 1]\n\\\\ & 预估值：p = sigmoid(logit) \\in [0, 1]\n\\\\ & L_{logloss} = -\\hat{t} \\log(p) - (1-\\hat{t}) \\log(1-p)\n\\\\ & \\frac{\\partial{loss}}{\\partial{p}}=0 \\Longrightarrow 最优解：p = t\n\\end{aligned}\n\\end{equation}\n$$\n\n最优解$p=t$，线上预估时，将预估的$p$转换成真实时长。先按照video duration找到在哪个分桶，再按分桶内的时长分段，将$p$逆操作得到真实时长。\n\n这里如果用Weighted Logloss是否可以呢？\n$$\n\\begin{equation}\n\\label{normalized_weighted_logloss}\n\\tag{5}\n\\begin{aligned}\n& 真实时长：t \\in [0, +\\infty)\n\\\\ & 归一化后的时长：\\hat{t} \\in [0, 1]\n\\\\ & 预估值：p = sigmoid(logit) \\in [0, 1]\n\\\\ & L_{weighted \\: logloss} = -\\hat{t} \\log(p) - \\log(1-p)\n\\\\ & \\frac{\\partial{loss}}{\\partial{p}}=0 \\Longrightarrow 最优解：p = \\frac{\\hat{t}}{\\hat{t}+1}\n\\end{aligned}\n\\end{equation}\n$$\n\n那么，\n$$\n\\begin{equation}\n\\label{normalized_weighted_logloss_serving}\n\\tag{6}\n\\begin{aligned}\n\\hat{t} = \\frac{p}{1-p}\n\\end{aligned}\n\\end{equation}\n$$\n但是，$\\hat{t} \\in [0, 1]$，只有当$p \\le 0.5$的情况下$\\hat{t}$才是有意义的，模型预估并不能保证这一点。当然，得到$p$后可以人工变换到$p \\le 0.5$，比如用$\\frac{p}{2}$。所以工程上用Weighted Logloss也是可以的，但理论上还是存在一定缺陷。\n\n另外，时长归一化在后面的几种方法里都可以叠加使用。\n\n分桶归一化的时长变换逻辑：\n```python\nclass TimeTrans(object):\n    def __init__(self):\n        self.videoDurationTable = [0, 17.03, 23.7, 31.33, 36.07, 42.25, 49.86, 61.5, 85.72, 142.83]\n        self.timePercentileTable = [\n            [0, 3.24, 3.49, 3.76, 4.04, 4.34, 4.66, 4.98, 5.33, 5.69, 6.06, 6.45, 6.84, 7.24, 7.65, 8.06, 8.48, 8.9, 9.32, 9.73, 10.15, 10.57, 10.98, 11.35, 11.71, 12.04, 12.39, 12.74, 13.09, 13.44, 13.79, 14.13, 14.47, 14.82, 15.17, 15.53, 15.89, 16.22, 16.55, 16.87, 17.21, 17.56, 17.94, 18.48, 19.34, 20.54, 22.06, 24.17, 27.17, 31.48], \n            [0, 3.25, 3.53, 3.83, 4.15, 4.5, 4.88, 5.28, 5.72, 6.19, 6.68, 7.19, 7.73, 8.29, 8.88, 9.47, 10.08, 10.71, 11.33, 11.94, 12.57, 13.22, 13.86, 14.48, 15.08, 15.69, 16.3, 16.89, 17.43, 17.93, 18.31, 18.65, 18.98, 19.31, 19.66, 20.03, 20.42, 20.82, 21.25, 21.7, 22.15, 22.63, 23.16, 23.75, 24.43, 25.39, 27.0, 29.48, 33.24, 39.08], \n            [0, 3.3, 3.64, 4.0, 4.4, 4.85, 5.35, 5.89, 6.46, 7.08, 7.74, 8.48, 9.31, 10.16, 11.06, 12.05, 13.05, 14.06, 15.08, 16.08, 17.08, 18.13, 19.2, 20.27, 21.32, 22.27, 23.15, 23.93, 24.62, 25.19, 25.71, 26.22, 26.73, 27.25, 27.75, 28.25, 28.74, 29.27, 29.8, 30.35, 30.86, 31.29, 31.68, 32.05, 32.51, 33.34, 34.77, 37.09, 41.32, 49.85], \n            [0, 3.31, 3.63, 3.99, 4.38, 4.83, 5.32, 5.85, 6.44, 7.13, 7.86, 8.66, 9.53, 10.49, 11.53, 12.65, 13.85, 15.11, 16.41, 17.68, 18.86, 20.01, 21.27, 22.61, 23.98, 25.42, 26.86, 28.26, 29.44, 30.49, 31.38, 32.06, 32.53, 32.9, 33.24, 33.57, 33.9, 34.24, 34.59, 34.95, 35.31, 35.67, 36.07, 36.49, 36.94, 37.58, 38.74, 40.76, 44.55, 53.51], \n            [0, 3.3, 3.63, 3.99, 4.4, 4.85, 5.34, 5.91, 6.55, 7.26, 8.04, 8.93, 9.89, 10.95, 12.1, 13.36, 14.71, 16.13, 17.65, 19.26, 20.74, 22.08, 23.42, 24.88, 26.45, 28.06, 29.69, 31.36, 32.91, 34.3, 35.47, 36.44, 37.17, 37.66, 38.06, 38.45, 38.86, 39.26, 39.67, 40.1, 40.55, 41.01, 41.45, 41.92, 42.45, 43.07, 43.99, 45.91, 49.7, 58.93], \n            [0, 3.31, 3.66, 4.05, 4.48, 4.95, 5.49, 6.09, 6.76, 7.51, 8.31, 9.19, 10.19, 11.33, 12.55, 13.9, 15.38, 17.02, 18.79, 20.53, 22.37, 24.08, 25.67, 27.27, 29.08, 30.94, 32.92, 34.98, 37.05, 38.98, 40.67, 42.04, 43.06, 43.69, 44.22, 44.73, 45.23, 45.74, 46.23, 46.68, 47.15, 47.65, 48.15, 48.67, 49.26, 49.92, 50.66, 52.08, 55.46, 64.44], \n            [0, 3.32, 3.68, 4.08, 4.53, 5.04, 5.61, 6.26, 7.0, 7.81, 8.73, 9.77, 10.95, 12.23, 13.64, 15.17, 16.85, 18.65, 20.67, 22.78, 25.1, 27.37, 29.4, 31.43, 33.41, 35.59, 37.93, 40.3, 42.77, 45.24, 47.51, 49.34, 50.78, 51.69, 52.44, 53.15, 53.83, 54.53, 55.23, 55.96, 56.68, 57.43, 58.21, 59.03, 59.93, 60.88, 61.85, 63.03, 66.14, 75.23], \n            [0, 3.3, 3.64, 4.01, 4.43, 4.9, 5.43, 6.03, 6.71, 7.45, 8.3, 9.25, 10.34, 11.59, 12.99, 14.53, 16.27, 18.22, 20.3, 22.63, 25.02, 27.66, 30.41, 33.04, 35.62, 38.13, 40.73, 43.38, 46.17, 49.16, 52.42, 55.62, 58.59, 61.22, 63.15, 64.39, 65.57, 66.68, 67.79, 69.01, 70.32, 71.94, 73.74, 75.51, 77.27, 79.06, 81.03, 83.48, 86.07, 93.3], \n            [0, 3.28, 3.6, 3.97, 4.38, 4.85, 5.38, 5.98, 6.64, 7.42, 8.29, 9.28, 10.41, 11.71, 13.21, 14.9, 16.82, 19.05, 21.56, 24.21, 27.18, 30.42, 33.93, 37.56, 41.35, 45.3, 49.08, 52.89, 56.73, 60.71, 64.86, 69.1, 73.47, 78.19, 82.89, 86.82, 89.24, 91.37, 93.9, 96.49, 99.29, 102.51, 106.25, 109.97, 114.23, 119.12, 123.95, 129.03, 135.51, 142.95], \n            [0, 3.3, 3.64, 4.03, 4.47, 4.97, 5.54, 6.19, 6.96, 7.85, 8.85, 10.02, 11.35, 12.91, 14.73, 16.85, 19.3, 22.08, 25.26, 28.72, 32.59, 36.7, 41.39, 46.35, 51.64, 57.39, 63.42, 69.59, 76.3, 83.1, 90.2, 97.96, 106.55, 115.28, 124.49, 134.38, 144.24, 150.26, 157.35, 165.06, 173.25, 182.13, 192.08, 202.93, 215.55, 229.49, 243.75, 266.22, 293.19, 338.96]\n        ]\n        self.maxReadTime = 3 # 阅读时长不能超过视频时长的3倍\n\n    def getIdx(self, percentileList, score):\n        if score >= percentileList[-1]:\n            return len(percentileList) - 1\n        left = 0\n        right = len(percentileList)\n        while left <= right:\n            mid = (left + right) // 2\n            if percentileList[mid] < score:\n                left = mid + 1\n            elif percentileList[mid] > score:\n                right = mid - 1\n            else:\n                return mid\n        return right\n    \n    def encode(self, videoTime, readTime):\n        if videoTime <= 0 or readTime < 0:\n            videoTime = 0\n            readTime = 0\n        durationBuk = self.getIdx(self.videoDurationTable, videoTime)\n        timePercentileList = self.timePercentileTable[durationBuk]\n        bukIdx = self.getIdx(timePercentileList, readTime)\n        if bukIdx == len(timePercentileList) - 1:\n            lowerLabel = 1 / len(timePercentileList) * bukIdx\n            upperLabel = 1\n            label = lowerLabel + (readTime - timePercentileList[bukIdx]) * (upperLabel - lowerLabel) / (self.maxReadTime * videoTime - timePercentileList[bukIdx])\n        else:\n            lowerLabel = 1 / len(timePercentileList) * bukIdx\n            upperLabel = 1 / len(timePercentileList) * (bukIdx + 1)\n            label = lowerLabel + (readTime - timePercentileList[bukIdx]) * (upperLabel - lowerLabel) / (timePercentileList[bukIdx+1] - timePercentileList[bukIdx])\n        return label\n\n    def decode(self, videoTime, pScore):\n        if videoTime <= 0 or pScore <= 0:\n            return 0\n        elif pScore >= 1:\n            return self.maxReadTime * videoTime\n        durationBuk = self.getIdx(self.videoDurationTable, videoTime)   \n        timePercentileList = self.timePercentileTable[durationBuk]\n        bukIdx = int(pScore * len(timePercentileList))\n        if bukIdx == len(timePercentileList) - 1:\n            lowerLabel = 1 / len(timePercentileList) * bukIdx\n            upperLabel = 1\n            readTime = timePercentileList[bukIdx] + (pScore - lowerLabel) * (self.maxReadTime * videoTime - timePercentileList[bukIdx]) / (upperLabel - lowerLabel)\n        else:\n            lowerLabel = 1 / len(timePercentileList) * bukIdx\n            upperLabel = 1 / len(timePercentileList) * (bukIdx + 1)\n            readTime = timePercentileList[bukIdx] + (pScore - lowerLabel) * (timePercentileList[bukIdx+1] - timePercentileList[bukIdx]) / (upperLabel - lowerLabel)\n        return readTime \n\n# 测试\ntimeTrans = TimeTrans()\nvideoTime = 16\nreadTime = 8\ntimeLabel = timeTrans.encode(videoTime, readTime)\nprint(timeLabel)\nreadTimeRevocery = timeTrans.decode(videoTime, timeLabel)\nprint(readTimeRevocery)\n```\n\n# CREAD\nCREAD是快手的论文[<sup>2</sup>](#refer-anchor-2)中提出的方法。\n<div style=\"text-align: center\">\n<img src=\"/images/kuaishou_cread.png\" alt=\"图片替换文本\" width=\"420\" height=\"600\" align=\"middle\" />\n</div>\n将消费时长从小到大排序，比如是$[0, t_{max}]$，然后通过m+1个阈值$[t_0, t_1, t_2, ..., t_m]$，其中$t_0=0, \\quad t_m<t_{max}, \\quad t_{i} < t_{i+1}$，将时长分成m+1个区间，每个区间就可以得到一个二分类任务：$P(y>t_i | x)$，共m个二分类任务。线上预估时，可以通过以下公式还原成时长。\n$$\n\\begin{equation}\n\\label{cread_serving}\n\\tag{7}\n\\begin{aligned}\nt_{restoration} = \\sum_{i=1}^{m} (t_i-t_{i-1}) P(y>t_i|x)\n\\end{aligned}\n\\end{equation}\n$$\n\n实际上，训练时除了m个分类任务，还增加了两个其他损失，一个是还原时长与真实时长的偏差（采用Huber loss），另一个是$P(y>t_i|x) > P(y>t_{i+1}|x)$的序关系（采用Hinge loss）。\n$$\n\\begin{equation}\n\\label{cread_loss}\n\\tag{8}\n\\begin{aligned}\n& L_{cread} = \\lambda_{ce} L_{cr} + \\lambda_{restore} L_{restore} + \\lambda_{ord} L_{ord}\n\\\\ & 其中，\n\\\\ & L_{cr} = \\sum_{i=1}^{m} [ -y_i \\log P(y>t_i|x) - (1-y_i) \\log (1 - P(y>t_i|x))]\n\\\\ & L_{restore} = \\ell(t_{restoration}, t)\n\\\\ & L_{ord} = \\sum_{i=1}^{m-1} \\max (P(y>t_{i+1}|x)-P(y>t_i|x), \\quad 0)\n\\end{aligned}\n\\end{equation}\n$$\n\nCREAD的时长变换逻辑：\n```python\nclass TimeTrans(object):\n    def __init__(self):\n        self.videoDurationTable = [0, 17.03, 23.7, 31.33, 36.07, 42.25, 49.86, 61.5, 85.72, 142.83]\n        self.timePercentileTable = [\n            [0, 3.24, 3.49, 3.76, 4.04, 4.34, 4.66, 4.98, 5.33, 5.69, 6.06, 6.45, 6.84, 7.24, 7.65, 8.06, 8.48, 8.9, 9.32, 9.73, 10.15, 10.57, 10.98, 11.35, 11.71, 12.04, 12.39, 12.74, 13.09, 13.44, 13.79, 14.13, 14.47, 14.82, 15.17, 15.53, 15.89, 16.22, 16.55, 16.87, 17.21, 17.56, 17.94, 18.48, 19.34, 20.54, 22.06, 24.17, 27.17, 31.48], \n            [0, 3.25, 3.53, 3.83, 4.15, 4.5, 4.88, 5.28, 5.72, 6.19, 6.68, 7.19, 7.73, 8.29, 8.88, 9.47, 10.08, 10.71, 11.33, 11.94, 12.57, 13.22, 13.86, 14.48, 15.08, 15.69, 16.3, 16.89, 17.43, 17.93, 18.31, 18.65, 18.98, 19.31, 19.66, 20.03, 20.42, 20.82, 21.25, 21.7, 22.15, 22.63, 23.16, 23.75, 24.43, 25.39, 27.0, 29.48, 33.24, 39.08], \n            [0, 3.3, 3.64, 4.0, 4.4, 4.85, 5.35, 5.89, 6.46, 7.08, 7.74, 8.48, 9.31, 10.16, 11.06, 12.05, 13.05, 14.06, 15.08, 16.08, 17.08, 18.13, 19.2, 20.27, 21.32, 22.27, 23.15, 23.93, 24.62, 25.19, 25.71, 26.22, 26.73, 27.25, 27.75, 28.25, 28.74, 29.27, 29.8, 30.35, 30.86, 31.29, 31.68, 32.05, 32.51, 33.34, 34.77, 37.09, 41.32, 49.85], \n            [0, 3.31, 3.63, 3.99, 4.38, 4.83, 5.32, 5.85, 6.44, 7.13, 7.86, 8.66, 9.53, 10.49, 11.53, 12.65, 13.85, 15.11, 16.41, 17.68, 18.86, 20.01, 21.27, 22.61, 23.98, 25.42, 26.86, 28.26, 29.44, 30.49, 31.38, 32.06, 32.53, 32.9, 33.24, 33.57, 33.9, 34.24, 34.59, 34.95, 35.31, 35.67, 36.07, 36.49, 36.94, 37.58, 38.74, 40.76, 44.55, 53.51], \n            [0, 3.3, 3.63, 3.99, 4.4, 4.85, 5.34, 5.91, 6.55, 7.26, 8.04, 8.93, 9.89, 10.95, 12.1, 13.36, 14.71, 16.13, 17.65, 19.26, 20.74, 22.08, 23.42, 24.88, 26.45, 28.06, 29.69, 31.36, 32.91, 34.3, 35.47, 36.44, 37.17, 37.66, 38.06, 38.45, 38.86, 39.26, 39.67, 40.1, 40.55, 41.01, 41.45, 41.92, 42.45, 43.07, 43.99, 45.91, 49.7, 58.93], \n            [0, 3.31, 3.66, 4.05, 4.48, 4.95, 5.49, 6.09, 6.76, 7.51, 8.31, 9.19, 10.19, 11.33, 12.55, 13.9, 15.38, 17.02, 18.79, 20.53, 22.37, 24.08, 25.67, 27.27, 29.08, 30.94, 32.92, 34.98, 37.05, 38.98, 40.67, 42.04, 43.06, 43.69, 44.22, 44.73, 45.23, 45.74, 46.23, 46.68, 47.15, 47.65, 48.15, 48.67, 49.26, 49.92, 50.66, 52.08, 55.46, 64.44], \n            [0, 3.32, 3.68, 4.08, 4.53, 5.04, 5.61, 6.26, 7.0, 7.81, 8.73, 9.77, 10.95, 12.23, 13.64, 15.17, 16.85, 18.65, 20.67, 22.78, 25.1, 27.37, 29.4, 31.43, 33.41, 35.59, 37.93, 40.3, 42.77, 45.24, 47.51, 49.34, 50.78, 51.69, 52.44, 53.15, 53.83, 54.53, 55.23, 55.96, 56.68, 57.43, 58.21, 59.03, 59.93, 60.88, 61.85, 63.03, 66.14, 75.23], \n            [0, 3.3, 3.64, 4.01, 4.43, 4.9, 5.43, 6.03, 6.71, 7.45, 8.3, 9.25, 10.34, 11.59, 12.99, 14.53, 16.27, 18.22, 20.3, 22.63, 25.02, 27.66, 30.41, 33.04, 35.62, 38.13, 40.73, 43.38, 46.17, 49.16, 52.42, 55.62, 58.59, 61.22, 63.15, 64.39, 65.57, 66.68, 67.79, 69.01, 70.32, 71.94, 73.74, 75.51, 77.27, 79.06, 81.03, 83.48, 86.07, 93.3], \n            [0, 3.28, 3.6, 3.97, 4.38, 4.85, 5.38, 5.98, 6.64, 7.42, 8.29, 9.28, 10.41, 11.71, 13.21, 14.9, 16.82, 19.05, 21.56, 24.21, 27.18, 30.42, 33.93, 37.56, 41.35, 45.3, 49.08, 52.89, 56.73, 60.71, 64.86, 69.1, 73.47, 78.19, 82.89, 86.82, 89.24, 91.37, 93.9, 96.49, 99.29, 102.51, 106.25, 109.97, 114.23, 119.12, 123.95, 129.03, 135.51, 142.95], \n            [0, 3.3, 3.64, 4.03, 4.47, 4.97, 5.54, 6.19, 6.96, 7.85, 8.85, 10.02, 11.35, 12.91, 14.73, 16.85, 19.3, 22.08, 25.26, 28.72, 32.59, 36.7, 41.39, 46.35, 51.64, 57.39, 63.42, 69.59, 76.3, 83.1, 90.2, 97.96, 106.55, 115.28, 124.49, 134.38, 144.24, 150.26, 157.35, 165.06, 173.25, 182.13, 192.08, 202.93, 215.55, 229.49, 243.75, 266.22, 293.19, 338.96]\n        ]\n        self.maxReadTime = 3\n\n    def getIdx(self, percentileList, score):\n        if score >= percentileList[-1]:\n            return len(percentileList) - 1\n        left = 0\n        right = len(percentileList)\n        while left <= right:\n            mid = (left + right) // 2\n            if percentileList[mid] < score:\n                left = mid + 1\n            elif percentileList[mid] > score:\n                right = mid - 1\n            else:\n                return mid\n        return right\n    \n    def encode(self, videoTime, readTime):\n        if videoTime <= 0 or readTime <= 0:\n            videoTime = 1e-8\n            readTime = 1e-8\n        elif readTime > self.maxReadTime * videoTime:\n            readTime = self.maxReadTime * videoTime\n        durationBuk = self.getIdx(self.videoDurationTable, videoTime)\n        timePercentileList = self.timePercentileTable[durationBuk]\n        label = [1 if readTime > thr else 0 for thr in timePercentileList]\n        return label\n\n    def decode(self, videoTime, pScore):\n        if videoTime <= 0:\n            return 0\n        durationBuk = self.getIdx(self.videoDurationTable, videoTime)   \n        timePercentileList = self.timePercentileTable[durationBuk]\n        readTime = 0\n        for i in range(1, len(timePercentileList)):\n            readTime += (timePercentileList[i] - timePercentileList[i-1]) * pScore[i]\n        return readTime\n    \n# 测试\ntimeTrans = TimeTrans()\nvideoTime = 16\nreadTime = 8\ntimeLabel = timeTrans.encode(videoTime, readTime)\nprint(timeLabel)\nreadTimeRevocery = timeTrans.decode(videoTime, timeLabel)\nprint(readTimeRevocery)\n```\n\n# Earth Mover’s Distance\n传统的多分类任务，label中只有一个是正类，其他是负类，类别之前没有关系。但是在某些任务中，类别之间是有一定关系的。下图是收入预估的一个例子，adult是真实label，AB两种分布的loss是一样的，但明显B比A更合理一点。在时长预估上也有同样的道理，把时长分成10个区间，真实label在区间5，做多分类任务，我们希望5区间的预估概率最高，向左向右的概率平滑的降低。\n<div style=\"text-align: center\">\n<img src=\"/images/emd_loss.png\" alt=\"图片替换文本\" width=\"500\" height=\"550\" align=\"middle\" />\n</div>\n\n论文[<sup>3</sup>](#refer-anchor-3)使用的是Squared EMD Loss：\n$$\n\\begin{equation}\n\\label{emd_loss}\n\\tag{9}\n\\begin{aligned}\n& 多分类label：y = [y_0, y_1, y_2, ..., y_n]，其中只有一个y_i=1，其他为0。\n\\\\ & 每个类别的预估概率：p = [p_0, p_1, p_2, ..., p_n]，全部加起来等于1。\n\\\\ & L_{squared \\: emd} = \\sum_{i=0}^{n} (CDF_i(p)-CDF_i(y))^{2} \n\\\\ & 其中，\n\\\\ & CDF_i(p) = \\sum_{j=0}^{i} p_j\n\\\\ & CDF_i(y) = \\sum_{j=0}^{i} y_j\n\\end{aligned}\n\\end{equation}\n$$\n\n论文[<sup>3</sup>](#refer-anchor-3)不是做时长预估的，我们这里为时长预估任务稍微改动一下。\n和CREAD类似，将消费时长从小到大排序，比如是$[0, t_{max}]$，然后通过m+1个阈值$[t_0, t_1, t_2, ..., t_m]$，其中$t_0=0, \\quad t_m<t_{max}, \\quad t_{i} < t_{i+1}$，将时长分成m+1个区间。\n$$\n\\begin{equation}\n\\label{time_emd_loss}\n\\tag{10}\n\\begin{aligned}\n& 多分类label：y = [y_0, y_1, y_2, ..., y_m]，其中y_i表示处于时间区间[t_i, t_{i+1}]，只有一个y_i=1，其他为0。\n\\\\ & 每个类别的预估概率：p = [p_0, p_1, p_2, ..., p_m]，全部加起来等于1。\n\\\\ & L_{logloss \\: emd} = \\sum_{i=0}^{m} [- CDF_i(y) \\log CDF_i(p)]\n\\\\ & 其中，\n\\\\ & CDF_i(p) = \\sum_{j=0}^{i} p_j，累积概率，表示预估时长大于t_i的概率。\n\\\\ & CDF_i(y) = \\sum_{j=0}^{i} y_j\n\\end{aligned}\n\\end{equation}\n$$\n\n线上预估时，可以通过以下公式还原成时长，和CREAD其实很像。\n$$\n\\begin{equation}\n\\label{time_emd_serving}\n\\tag{11}\n\\begin{aligned}\nt_{restoration} = \\sum_{i=1}^{m} (t_i-t_{i-1}) (1 - CDF_i(p))\n\\end{aligned}\n\\end{equation}\n$$\n\nEMD和CREAD的实现其实本质一样，只是出发角度不同。$1 - CDF_i(p)$相当于CREAD里的$P(y>t_i|x)$，$CDF_i(y)$相当于CREAD里的二分类label。但是，EMD相比于CREAD有几个好处。第一，不需要$L_{ord}$了；第二，CREAD是多个二分类损失叠加，尺度较大，与其他目标做多目标联合训练时不好平衡其他目标，而EMD是一个多分类的loss，尺度比较小；第三，EMD可以和接下来要介绍的Distill Softmax合起来用，效果更佳。\n\nEMD的时长变换逻辑：\n```python\nclass TimeTrans(object):\n    def __init__(self):\n        self.videoDurationTable = [0, 17.03, 23.7, 31.33, 36.07, 42.25, 49.86, 61.5, 85.72, 142.83]\n        self.timePercentileTable = [\n            [0, 3.24, 3.49, 3.76, 4.04, 4.34, 4.66, 4.98, 5.33, 5.69, 6.06, 6.45, 6.84, 7.24, 7.65, 8.06, 8.48, 8.9, 9.32, 9.73, 10.15, 10.57, 10.98, 11.35, 11.71, 12.04, 12.39, 12.74, 13.09, 13.44, 13.79, 14.13, 14.47, 14.82, 15.17, 15.53, 15.89, 16.22, 16.55, 16.87, 17.21, 17.56, 17.94, 18.48, 19.34, 20.54, 22.06, 24.17, 27.17, 31.48], \n            [0, 3.25, 3.53, 3.83, 4.15, 4.5, 4.88, 5.28, 5.72, 6.19, 6.68, 7.19, 7.73, 8.29, 8.88, 9.47, 10.08, 10.71, 11.33, 11.94, 12.57, 13.22, 13.86, 14.48, 15.08, 15.69, 16.3, 16.89, 17.43, 17.93, 18.31, 18.65, 18.98, 19.31, 19.66, 20.03, 20.42, 20.82, 21.25, 21.7, 22.15, 22.63, 23.16, 23.75, 24.43, 25.39, 27.0, 29.48, 33.24, 39.08], \n            [0, 3.3, 3.64, 4.0, 4.4, 4.85, 5.35, 5.89, 6.46, 7.08, 7.74, 8.48, 9.31, 10.16, 11.06, 12.05, 13.05, 14.06, 15.08, 16.08, 17.08, 18.13, 19.2, 20.27, 21.32, 22.27, 23.15, 23.93, 24.62, 25.19, 25.71, 26.22, 26.73, 27.25, 27.75, 28.25, 28.74, 29.27, 29.8, 30.35, 30.86, 31.29, 31.68, 32.05, 32.51, 33.34, 34.77, 37.09, 41.32, 49.85], \n            [0, 3.31, 3.63, 3.99, 4.38, 4.83, 5.32, 5.85, 6.44, 7.13, 7.86, 8.66, 9.53, 10.49, 11.53, 12.65, 13.85, 15.11, 16.41, 17.68, 18.86, 20.01, 21.27, 22.61, 23.98, 25.42, 26.86, 28.26, 29.44, 30.49, 31.38, 32.06, 32.53, 32.9, 33.24, 33.57, 33.9, 34.24, 34.59, 34.95, 35.31, 35.67, 36.07, 36.49, 36.94, 37.58, 38.74, 40.76, 44.55, 53.51], \n            [0, 3.3, 3.63, 3.99, 4.4, 4.85, 5.34, 5.91, 6.55, 7.26, 8.04, 8.93, 9.89, 10.95, 12.1, 13.36, 14.71, 16.13, 17.65, 19.26, 20.74, 22.08, 23.42, 24.88, 26.45, 28.06, 29.69, 31.36, 32.91, 34.3, 35.47, 36.44, 37.17, 37.66, 38.06, 38.45, 38.86, 39.26, 39.67, 40.1, 40.55, 41.01, 41.45, 41.92, 42.45, 43.07, 43.99, 45.91, 49.7, 58.93], \n            [0, 3.31, 3.66, 4.05, 4.48, 4.95, 5.49, 6.09, 6.76, 7.51, 8.31, 9.19, 10.19, 11.33, 12.55, 13.9, 15.38, 17.02, 18.79, 20.53, 22.37, 24.08, 25.67, 27.27, 29.08, 30.94, 32.92, 34.98, 37.05, 38.98, 40.67, 42.04, 43.06, 43.69, 44.22, 44.73, 45.23, 45.74, 46.23, 46.68, 47.15, 47.65, 48.15, 48.67, 49.26, 49.92, 50.66, 52.08, 55.46, 64.44], \n            [0, 3.32, 3.68, 4.08, 4.53, 5.04, 5.61, 6.26, 7.0, 7.81, 8.73, 9.77, 10.95, 12.23, 13.64, 15.17, 16.85, 18.65, 20.67, 22.78, 25.1, 27.37, 29.4, 31.43, 33.41, 35.59, 37.93, 40.3, 42.77, 45.24, 47.51, 49.34, 50.78, 51.69, 52.44, 53.15, 53.83, 54.53, 55.23, 55.96, 56.68, 57.43, 58.21, 59.03, 59.93, 60.88, 61.85, 63.03, 66.14, 75.23], \n            [0, 3.3, 3.64, 4.01, 4.43, 4.9, 5.43, 6.03, 6.71, 7.45, 8.3, 9.25, 10.34, 11.59, 12.99, 14.53, 16.27, 18.22, 20.3, 22.63, 25.02, 27.66, 30.41, 33.04, 35.62, 38.13, 40.73, 43.38, 46.17, 49.16, 52.42, 55.62, 58.59, 61.22, 63.15, 64.39, 65.57, 66.68, 67.79, 69.01, 70.32, 71.94, 73.74, 75.51, 77.27, 79.06, 81.03, 83.48, 86.07, 93.3], \n            [0, 3.28, 3.6, 3.97, 4.38, 4.85, 5.38, 5.98, 6.64, 7.42, 8.29, 9.28, 10.41, 11.71, 13.21, 14.9, 16.82, 19.05, 21.56, 24.21, 27.18, 30.42, 33.93, 37.56, 41.35, 45.3, 49.08, 52.89, 56.73, 60.71, 64.86, 69.1, 73.47, 78.19, 82.89, 86.82, 89.24, 91.37, 93.9, 96.49, 99.29, 102.51, 106.25, 109.97, 114.23, 119.12, 123.95, 129.03, 135.51, 142.95], \n            [0, 3.3, 3.64, 4.03, 4.47, 4.97, 5.54, 6.19, 6.96, 7.85, 8.85, 10.02, 11.35, 12.91, 14.73, 16.85, 19.3, 22.08, 25.26, 28.72, 32.59, 36.7, 41.39, 46.35, 51.64, 57.39, 63.42, 69.59, 76.3, 83.1, 90.2, 97.96, 106.55, 115.28, 124.49, 134.38, 144.24, 150.26, 157.35, 165.06, 173.25, 182.13, 192.08, 202.93, 215.55, 229.49, 243.75, 266.22, 293.19, 338.96]\n        ]\n        self.maxReadTime = 3 # 阅读时长不能超过视频时长的3倍\n\n    def getIdx(self, percentileList, score):\n        if score >= percentileList[-1]:\n            return len(percentileList) - 1\n        left = 0\n        right = len(percentileList)\n        while left <= right:\n            mid = (left + right) // 2\n            if percentileList[mid] < score:\n                left = mid + 1\n            elif percentileList[mid] > score:\n                right = mid - 1\n            else:\n                return mid\n        return right\n    \n    def cdf(self, scores):\n        result = []\n        cur_sum = 0\n        for score in scores:\n            cur_sum += score\n            result.append(cur_sum)\n        return result\n    \n    def encode(self, videoTime, readTime):\n        if videoTime <= 0 or readTime < 0:\n            videoTime = 1e-8\n            readTime = 1e-8\n        elif readTime > self.maxReadTime * videoTime:\n            readTime = self.maxReadTime * videoTime\n        durationBuk = self.getIdx(self.videoDurationTable, videoTime)\n        timePercentileList = self.timePercentileTable[durationBuk]\n        bukIdx = self.getIdx(timePercentileList, readTime)\n        label = [1 if i == bukIdx else 0 for i in range(len(timePercentileList))]\n        return label\n\n    def decode(self, videoTime, pScore):\n        if videoTime <= 0:\n            return 0\n        durationBuk = self.getIdx(self.videoDurationTable, videoTime)   \n        timePercentileList = self.timePercentileTable[durationBuk]\n        cdf_result = self.cdf(pScore)\n        readTime = 0\n        for i in range(1, len(timePercentileList)):\n            readTime += (timePercentileList[i] - timePercentileList[i-1]) * (1 - cdf_result[i])\n        return readTime\n    \n# 测试\ntimeTrans = TimeTrans()\nvideoTime = 16\nreadTime = 8\ntimeLabel = timeTrans.encode(videoTime, readTime)\nprint(timeLabel)\nreadTimeRevocery = timeTrans.decode(videoTime, timeLabel)\nprint(readTimeRevocery)\n```\n\n# Distill Softmax\n和EMD有点类似，只不过EMD从预估值分布角度考虑，而Distill Softmax是从label角度考虑，直接将时长做成了平滑的多分类soft-label。比如，把时长分成10个区间，真实label在区间5，本来的多分类label是[0,0,0,0,1,0,0,0,0,0]，这里对时长做了平滑，希望5区间的预估概率最高，向左向右的概率平滑的降低，做成[0.001,0.005,0.03,0.1,0.7,0.1,0.05,0.008,0.005,0.001]。\n\n具体做法，时长先做下laplace分布变换，得到每个区间的概率值，越靠近真实label的区间概率越大。\n\n损失函数就是多分类交叉熵：\n$$\n\\begin{equation}\n\\label{distill_softmax}\n\\tag{12}\n\\begin{aligned}\n& 多分类soft \\: label：y = [y_0, y_1, y_2, ..., y_m]，其中y_i表示处于时间区间[t_i, t_{i+1}]的概率，全部加起来等于1。\n\\\\ & 每个类别的预估概率：p = [p_0, p_1, p_2, ..., p_m]，全部加起来等于1。\n\\\\ & L_{distill \\: softmax} = \\sum_{i=0}^{m} - y_i \\log p_i\n\\end{aligned}\n\\end{equation}\n$$\n\n线上预估时，可以通过以下公式还原成时长，和EMD一样。\n$$\n\\begin{equation}\n\\label{distill_softmax_serving}\n\\tag{13}\n\\begin{aligned}\nt_{restoration} = \\sum_{i=1}^{m} (t_i-t_{i-1}) (1 - CDF_i(p))\n\\end{aligned}\n\\end{equation}\n$$\n\nDistill Softmax也能和EMD一起用，既先做soft-label变换后，再使用EMD Loss。\n\nDistill Softmax的时长变换逻辑：\n```python\nclass TimeTrans(object):\n    def __init__(self):\n        self.videoDurationTable = [0, 17.03, 23.7, 31.33, 36.07, 42.25, 49.86, 61.5, 85.72, 142.83]\n        self.timePercentileTable = [\n            [0, 3.24, 3.49, 3.76, 4.04, 4.34, 4.66, 4.98, 5.33, 5.69, 6.06, 6.45, 6.84, 7.24, 7.65, 8.06, 8.48, 8.9, 9.32, 9.73, 10.15, 10.57, 10.98, 11.35, 11.71, 12.04, 12.39, 12.74, 13.09, 13.44, 13.79, 14.13, 14.47, 14.82, 15.17, 15.53, 15.89, 16.22, 16.55, 16.87, 17.21, 17.56, 17.94, 18.48, 19.34, 20.54, 22.06, 24.17, 27.17, 31.48], \n            [0, 3.25, 3.53, 3.83, 4.15, 4.5, 4.88, 5.28, 5.72, 6.19, 6.68, 7.19, 7.73, 8.29, 8.88, 9.47, 10.08, 10.71, 11.33, 11.94, 12.57, 13.22, 13.86, 14.48, 15.08, 15.69, 16.3, 16.89, 17.43, 17.93, 18.31, 18.65, 18.98, 19.31, 19.66, 20.03, 20.42, 20.82, 21.25, 21.7, 22.15, 22.63, 23.16, 23.75, 24.43, 25.39, 27.0, 29.48, 33.24, 39.08], \n            [0, 3.3, 3.64, 4.0, 4.4, 4.85, 5.35, 5.89, 6.46, 7.08, 7.74, 8.48, 9.31, 10.16, 11.06, 12.05, 13.05, 14.06, 15.08, 16.08, 17.08, 18.13, 19.2, 20.27, 21.32, 22.27, 23.15, 23.93, 24.62, 25.19, 25.71, 26.22, 26.73, 27.25, 27.75, 28.25, 28.74, 29.27, 29.8, 30.35, 30.86, 31.29, 31.68, 32.05, 32.51, 33.34, 34.77, 37.09, 41.32, 49.85], \n            [0, 3.31, 3.63, 3.99, 4.38, 4.83, 5.32, 5.85, 6.44, 7.13, 7.86, 8.66, 9.53, 10.49, 11.53, 12.65, 13.85, 15.11, 16.41, 17.68, 18.86, 20.01, 21.27, 22.61, 23.98, 25.42, 26.86, 28.26, 29.44, 30.49, 31.38, 32.06, 32.53, 32.9, 33.24, 33.57, 33.9, 34.24, 34.59, 34.95, 35.31, 35.67, 36.07, 36.49, 36.94, 37.58, 38.74, 40.76, 44.55, 53.51], \n            [0, 3.3, 3.63, 3.99, 4.4, 4.85, 5.34, 5.91, 6.55, 7.26, 8.04, 8.93, 9.89, 10.95, 12.1, 13.36, 14.71, 16.13, 17.65, 19.26, 20.74, 22.08, 23.42, 24.88, 26.45, 28.06, 29.69, 31.36, 32.91, 34.3, 35.47, 36.44, 37.17, 37.66, 38.06, 38.45, 38.86, 39.26, 39.67, 40.1, 40.55, 41.01, 41.45, 41.92, 42.45, 43.07, 43.99, 45.91, 49.7, 58.93], \n            [0, 3.31, 3.66, 4.05, 4.48, 4.95, 5.49, 6.09, 6.76, 7.51, 8.31, 9.19, 10.19, 11.33, 12.55, 13.9, 15.38, 17.02, 18.79, 20.53, 22.37, 24.08, 25.67, 27.27, 29.08, 30.94, 32.92, 34.98, 37.05, 38.98, 40.67, 42.04, 43.06, 43.69, 44.22, 44.73, 45.23, 45.74, 46.23, 46.68, 47.15, 47.65, 48.15, 48.67, 49.26, 49.92, 50.66, 52.08, 55.46, 64.44], \n            [0, 3.32, 3.68, 4.08, 4.53, 5.04, 5.61, 6.26, 7.0, 7.81, 8.73, 9.77, 10.95, 12.23, 13.64, 15.17, 16.85, 18.65, 20.67, 22.78, 25.1, 27.37, 29.4, 31.43, 33.41, 35.59, 37.93, 40.3, 42.77, 45.24, 47.51, 49.34, 50.78, 51.69, 52.44, 53.15, 53.83, 54.53, 55.23, 55.96, 56.68, 57.43, 58.21, 59.03, 59.93, 60.88, 61.85, 63.03, 66.14, 75.23], \n            [0, 3.3, 3.64, 4.01, 4.43, 4.9, 5.43, 6.03, 6.71, 7.45, 8.3, 9.25, 10.34, 11.59, 12.99, 14.53, 16.27, 18.22, 20.3, 22.63, 25.02, 27.66, 30.41, 33.04, 35.62, 38.13, 40.73, 43.38, 46.17, 49.16, 52.42, 55.62, 58.59, 61.22, 63.15, 64.39, 65.57, 66.68, 67.79, 69.01, 70.32, 71.94, 73.74, 75.51, 77.27, 79.06, 81.03, 83.48, 86.07, 93.3], \n            [0, 3.28, 3.6, 3.97, 4.38, 4.85, 5.38, 5.98, 6.64, 7.42, 8.29, 9.28, 10.41, 11.71, 13.21, 14.9, 16.82, 19.05, 21.56, 24.21, 27.18, 30.42, 33.93, 37.56, 41.35, 45.3, 49.08, 52.89, 56.73, 60.71, 64.86, 69.1, 73.47, 78.19, 82.89, 86.82, 89.24, 91.37, 93.9, 96.49, 99.29, 102.51, 106.25, 109.97, 114.23, 119.12, 123.95, 129.03, 135.51, 142.95], \n            [0, 3.3, 3.64, 4.03, 4.47, 4.97, 5.54, 6.19, 6.96, 7.85, 8.85, 10.02, 11.35, 12.91, 14.73, 16.85, 19.3, 22.08, 25.26, 28.72, 32.59, 36.7, 41.39, 46.35, 51.64, 57.39, 63.42, 69.59, 76.3, 83.1, 90.2, 97.96, 106.55, 115.28, 124.49, 134.38, 144.24, 150.26, 157.35, 165.06, 173.25, 182.13, 192.08, 202.93, 215.55, 229.49, 243.75, 266.22, 293.19, 338.96]\n        ]\n        self.maxReadTime = 3 # 阅读时长不能超过视频时长的3倍\n\n    def getIdx(self, percentileList, score):\n        if score >= percentileList[-1]:\n            return len(percentileList) - 1\n        left = 0\n        right = len(percentileList)\n        while left <= right:\n            mid = (left + right) // 2\n            if percentileList[mid] < score:\n                left = mid + 1\n            elif percentileList[mid] > score:\n                right = mid - 1\n            else:\n                return mid\n        return right\n    \n    def cdf(self, scores):\n        result = []\n        cur_sum = 0\n        for score in scores:\n            cur_sum += score\n            result.append(cur_sum)\n        return result\n    \n    def laplace_pdf(self, x, mean, b=0.05):\n        variance = b * x\n        return 1.0 / (2.0 * variance) * np.exp(-1.0 * np.abs(np.array(x) - np.array(mean)) / variance)\n\n    def distill_label(self, timeLabel, labelPercentileList):\n        probs = self.laplace_pdf(timeLabel, labelPercentileList)\n        probs_sum = np.sum(probs)\n        soft_label = probs / probs_sum\n        return soft_label\n    \n    def encode(self, videoTime, readTime):\n        if videoTime <= 0 or readTime <= 0:\n            videoTime = 1e-8\n            readTime = 1e-8\n        elif readTime > self.maxReadTime * videoTime:\n            readTime = self.maxReadTime * videoTime\n        durationBuk = self.getIdx(self.videoDurationTable, videoTime)\n        timePercentileList = self.timePercentileTable[durationBuk]\n        label = self.distill_label(readTime, timePercentileList)\n        return label\n\n    def decode(self, videoTime, pScore):\n        if videoTime <= 0:\n            return 0\n        durationBuk = self.getIdx(self.videoDurationTable, videoTime)   \n        timePercentileList = self.timePercentileTable[durationBuk]\n        cdf_result = self.cdf(pScore)\n        readTime = 0\n        for i in range(1, len(timePercentileList)):\n            readTime += (timePercentileList[i] - timePercentileList[i-1]) * (1 - cdf_result[i])\n        return readTime\n    \n# 测试\ntimeTrans = TimeTrans()\nvideoTime = 16\nreadTime = 8\ntimeLabel = timeTrans.encode(videoTime, readTime)\nprint(timeLabel)\nreadTimeRevocery = timeTrans.decode(videoTime, timeLabel)\nprint(readTimeRevocery)\n```\n\n# 参考\n<div id=\"refer-anchor-1\"></div>\n- [1][Recommending What Video to Watch Next: A Multitask Ranking System](https://dl.acm.org/doi/10.1145/3298689.3346997)\n<div id=\"refer-anchor-2\"></div>\n- [2][CREAD: A Classification-Restoration Framework with Error Adaptive Discretization for Watch Time Prediction in Video Recommender Systems](http://arxiv.org/abs/2401.07521)\n<div id=\"refer-anchor-3\"></div>\n- [3][Squared Earth Mover’s Distance-based Loss for Training Deep Neural Networks](http://arxiv.org/abs/1611.05916)\n","tags":["Recommendation","Time Prediction"],"categories":["tech"]},{"title":"延迟反馈建模 - 基于样本回补的方案","url":"/5f8a539b/","content":"\n<!-- toc -->\n\n# 背景\n为了追求模型的时效性，在线学习已经成为主流。在线学习使用最近时间窗口内的样本进行实时模型训练，由于窗口设定往往比较短，所以有些样本的真实label并未到达，造成延迟反馈问题。比如某次商品点击之后，用户可能会反复对比其他商品，在几十分钟之后才进行下单，有时候会放入购物车，等价格降低或促销时才下单，要想拿到真实的转化label可能需要等待多天。对比转化延迟，点击延迟往往比较短，但点击label在窗口外到达的情况仍然存在。延迟反馈建模有多种方法，其中基于样本回补的方式大概这样：等待窗口内如果能拿到正反馈就作为正例训练，否则当做负例，在等待窗口之外，如果正反馈到来，则进行样本回补训练。对于怎么回补样本，有多种方式，在不同的回补方式上，纠偏方案也各不相同，本文对它们进行详细介绍。\n\n# 符号定义\n$$\n\\begin{equation}\n\\label{formulation}\n\\begin{aligned}\n& w_1：离线label归因的窗口。比如某次点击之后购买在7天内发生的都算此次点击带来，归因窗口既7天。& \\\\\n& w_0：在线学习时label等待的窗口，往往比w_1小。& \\\\\n& N_{tp}：true \\quad positive，真实正例的数量。& \\\\\n& N_{tn}：true \\quad negative，真实负例的数量。& \\\\\n& N_{fn}：false \\quad negative，伪负例的数量。& \\\\\n& p(x, y)：真实样本分布。& \\\\\n& q(x, y)：带有样本回补的样本分布。&\n\\end{aligned}\n\\end{equation}\n$$\n\n\n# 回补方式一\n论文[<sup>1</sup>](#refer-anchor-1)，设定等待窗口$w_0=0$，负例往往比正例早到达，负例到达时立即训练，正例到达时再作为正例回补。这样就存在伪负例问题。为了消除伪负例影响，有如下几种方法进行纠偏。\n\n## Importance Sampling\n其中，$f(y|x,\\theta)$是模型，$\\theta$是可学习参数，它学习真实分布$p(y|x)$。\n$$\n\\begin{equation}\n\\label{importance_sampling}\n\\tag{1}\n\\begin{aligned}\nLoss &= -\\mathbb{E}_{(x, y) \\sim p(x, y)} \\log f(y|x,\\theta)\n\\\\\n&= - \\sum_{i=1}^{N} [p(x_i,y_i) \\cdot \\log f(y_i|x_i,\\theta)]\n\\\\\n&= - \\sum_{i=1}^{N} [q(x_i,y_i) \\cdot \\frac{p(x_i,y_i)}{q(x_i,y_i)} \\cdot \\log f(y_i|x_i,\\theta)]\n\\\\\n&= -\\mathbb{E}_{(x, y) \\sim q(x, y)} \\frac{p(x,y)}{q(x,y)} \\cdot \\log{f(y|x,\\theta)}\n\\end{aligned}\n\\end{equation}\n$$\n\n假设$p(x) \\approx q(x)$的话，公式$\\eqref{importance_sampling}$可以简化为如下形式：\n$$\n\\begin{equation}\n\\label{importance_sampling_approx}\n\\tag{2}\n\\begin{aligned}\nLoss &= -\\mathbb{E}_{(x, y) \\sim q(x, y)} \\frac{p(x,y)}{q(x,y)} \\cdot \\log{f(y|x,\\theta)}\n\\\\\n&= -\\mathbb{E}_{(x, y) \\sim q(x, y)} \\frac{p(y|x) \\cdot p(x)}{q(y|x) \\cdot q(x)} \\cdot \\log{p(y|x,\\theta)}\n\\\\\n& \\approx -\\mathbb{E}_{(x, y) \\sim q(x, y)} \\frac{p(y|x)}{q(y|x)} \\cdot \\log{f(y|x,\\theta)}\n\\end{aligned}\n\\end{equation}\n$$\n\n由于正例都先作为负例，所以$N_{fn}=N_{tp}$，那么$p(y|x)$和$q(y|x)$可以得到如下关系：\n$$\n\\begin{equation}\n\\label{p_q_relation}\n\\tag{3}\n\\begin{aligned}\np(1|x) &= \\frac{N_{tp}}{N_{tp} + N_{tn}}\n\\\\\n&= \\frac{1}{1 + \\frac{N_{tn}}{N_{tp}}}\n\\\\\n\\\\\nq(1|x) & = \\frac{N_{tp}}{N_{tp} + N_{tn} + N_{fn}}\n\\\\\n&= \\frac{N_{tp}}{2 N_{tp} + N_{tn}}\n\\\\\n&= \\frac{1}{2 + \\frac{N_{tn}}{N_{tp}}}\n\\\\\n&= \\frac{p(1|x)}{1 + p(1|x)}\n\\\\\n\\\\\np(0|x) &= \\frac{N_{tn}}{N_{tp} + N_{tn}}\n\\\\\n&= \\frac{1}{1 + \\frac{N_{tp}}{N_{tn}}}\n\\\\\n\\\\\nq(0|x) & = \\frac{N_{tn} + N_{fn}}{N_{tp} + N_{tn} + N_{fn}}\n\\\\\n&= \\frac{N_{tn} + N_{tp}}{2 N_{tp} + N_{tn}}\n\\\\\n&= \\frac{1 + \\frac{N_{tp}}{N_{tn}}}{2 \\frac{N_{tp}}{N_{tn}} + 1}\n\\\\\n&= \\frac{1}{2 - p(0|x)}\n\\end{aligned}\n\\end{equation}\n$$\n\n将公式$\\eqref{p_q_relation}$代入$\\eqref{importance_sampling_approx}$，得到最终的损失函数：\n$$\n\\begin{equation}\n\\label{inportance_sampling_loss_implementation}\n\\tag{4}\n\\begin{aligned}\nLoss &= -\\mathbb{E}_{(x, y) \\sim q(x, y)} \\frac{p(y|x)}{q(y|x)} \\cdot \\log{f(y|x,\\theta )}\n\\\\\n& \\approx - \\frac{1}{N} \\sum_{i=1}^{N} [\\frac{p(1|x_i)}{q(1|x_i)} \\cdot y_i \\cdot \\log f(1|x_i,\\theta) + \\frac{p(0|x_i)}{q(0|x_i)} \\cdot (1 - y_i) \\cdot \\log f(0|x_i,\\theta)]\n\\\\\n&= - \\frac{1}{N} \\sum_{i=1}^{N} [(1 + p(1|x_i)) \\cdot y_i \\cdot \\log f(1|x_i,\\theta) + (p(0|x_i) \\cdot (2 - p(0|x_i))) \\cdot (1 - y_i) \\cdot \\log f(0|x_i,\\theta)]\n\\\\\n& \\approx - \\frac{1}{N} \\sum_{i=1}^{N} [ \\left \\langle 1 + f(1|x_i,\\theta) \\right \\rangle \\cdot y_i \\cdot \\log f(1|x_i,\\theta) + \\left \\langle f(0|x_i,\\theta) \\cdot (2 - f(0|x_i,\\theta)) \\right \\rangle \\cdot (1 - y_i) \\cdot \\log f(0|x_i,\\theta)]\n\\\\\n&= - \\frac{1}{N} \\sum_{i=1}^{N} [ \\left \\langle 1 + f(1|x_i,\\theta) \\right \\rangle \\cdot y_i \\cdot \\log f(1|x_i,\\theta) + \\left \\langle (1 + f(1|x_i,\\theta)) \\cdot (1 - f(1|x_i,\\theta)) \\right \\rangle \\cdot (1 - y_i) \\cdot \\log (1 - f(1|x_i,\\theta))]\n\\end{aligned}\n\\end{equation}\n$$\n其中，第四步假设$f(y|x,\\theta) \\propto p(y|x)$。$\\left \\langle \\right \\rangle$表示停止梯度回传，这个条件下，论文证明了$f(y|x,\\theta)$可以收敛于$p(y|x)$。\n\n## 预估值纠偏\n根据公式$\\eqref{p_q_relation}$可以得到：\n$$\n\\begin{equation}\n\\label{p_q_relation_2}\n\\tag{5}\n\\begin{aligned}\np(1|x) &= \\frac{q(1|x)}{1 - q(1|x)}\n\\end{aligned}\n\\end{equation}\n$$\n\n损失函数为常规的交叉熵，线上预估时用公式$\\eqref{p_q_relation_2}$做下校准就可以了。\n$$\n\\begin{equation}\n\\label{calibration_loss_implementation}\n\\tag{6}\n\\begin{aligned}\nLoss &= -\\mathbb{E}_{(x, y) \\sim q(x, y)} \\log{f(y|x,\\theta )}\n\\\\\n&= - \\frac{1}{N} \\sum_{i=1}^{N} [ y_i \\cdot \\log f(1|x_i,\\theta) + (1 - y_i) \\cdot \\log (1 - f(1|x_i,\\theta))]\n\\end{aligned}\n\\end{equation}\n$$\n\n## PU Loss\n将样本当做Positive和Unlabeled进行学习，既Positive-Unlabeled Learning。\n$$\n\\begin{equation}\n\\label{pu_loss}\n\\tag{7}\n\\begin{aligned}\nLoss &= -\\mathbb{E}_{(x, y) \\sim p(x, y)} \\log{f(y|x,\\theta)}\n\\\\\n&= - \\sum_{i=1}^{N} [p(x_i,y_i=1) \\cdot \\log f(y_i=1|x_i,\\theta) + p(x_i,y_i=0) \\cdot \\log f(y_i=0|x_i,\\theta)]\n\\\\\n&= - \\sum_{i=1}^{N} [p(x_i,y_i=1) \\cdot \\log f(y_i=1|x_i,\\theta) + (p(x_i) - p(x_i,y_i=1)) \\cdot \\log (1 - f(y_i=1|x_i,\\theta))]\n\\\\\n&= - \\sum_{i=1}^{N} [p(x_i,y_i=1) \\cdot [\\log f(y_i=1|x_i,\\theta) - \\log (1 - f(y_i=1|x_i,\\theta))] + p(x_i) \\cdot \\log (1 - f(y_i=1|x_i,\\theta))]\n\\\\\n&= -\\mathbb{E}_{(x, y) \\sim p(x, y=1)} [\\log f(y=1|x,\\theta) - \\log (1 - f(y=1|x,\\theta))] - \\mathbb{E}_{(x) \\sim p(x)} \\log (1 - p(y=1|x,\\theta)\n\\end{aligned}\n\\end{equation}\n$$\n\n将负例当做Unlabeled数据，既上面的第二项。延迟正例过来后，在之前作为FN样本的梯度方向上反转，对梯度做下反向补偿即可。\n$$\n\\begin{equation}\n\\label{pu_loss_implementation}\n\\tag{8}\n\\begin{aligned}\nLoss &= - \\frac{1}{N} \\sum_{i=1}^{N} [ y_i \\cdot (\\log f(1|x_i,\\theta) - \\log (1 - f(1|x_i,\\theta))) + (1 - y_i) \\cdot \\log (1 - f(1|x_i,\\theta))]\n\\end{aligned}\n\\end{equation}\n$$\n\n# 回补方式二\n论文[<sup>2</sup>](#refer-anchor-2)，设定等待窗口$0 < w_0 < w_1$，在等待窗口内如果有正例到达则作为正例训练，没有的话当做负例训练，窗口外如果有正例延迟到达时则回补这些延迟正例。\n\n## Importance Sampling\n因为$N_{fn} \\neq N_{tp}$，所以$p(y|x)$和$q(y|x)$的关系与回补方式一$\\eqref{p_q_relation}$不一样，需要重新推导。\n$$\n\\begin{equation}\n\\label{p_q_relation_method_2}\n\\tag{9}\n\\begin{aligned}\np(1|x) &= \\frac{N_{tp}}{N_{tp} + N_{tn}}\n\\\\\n&= \\frac{1}{1 + \\frac{N_{tn}}{N_{tp}}}\n\\\\\n\\\\\nq(1|x) & = \\frac{N_{tp}}{N_{tp} + N_{tn} + N_{fn}}\n\\\\\n&= \\frac{1}{1 + \\frac{N_{tn}}{N_{tp}} + \\frac{N_{fn}}{N_{tp}}}\n\\\\\n&= \\frac{1}{\\frac{1}{p(1|x)} + \\frac{N_{fn}}{N_{tp}}}\n\\\\\n\\\\\np(0|x) &= \\frac{N_{tn}}{N_{tp} + N_{tn}}\n\\\\\n&= \\frac{\\frac{N_{tn}}{N_{tp}}}{1 + \\frac{N_{tn}}{N_{tp}}}\n\\\\\n\\\\\nq(0|x) & = \\frac{N_{tn} + N_{fn}}{N_{tp} + N_{tn} + N_{fn}}\n\\\\\n&= \\frac{\\frac{N_{tn}}{N_{tp}} + \\frac{N_{fn}}{N_{tp}}}{1 + \\frac{N_{tn}}{N_{tp}} + \\frac{N_{fn}}{N_{tp}}}\n\\\\\n&= \\frac{\\frac{p(0|x)}{1-p(0|x)} + \\frac{N_{fn}}{N_{tp}}}{\\frac{1}{1-p(0|x)} + \\frac{N_{fn}}{N_{tp}}}\n\\end{aligned}\n\\end{equation}\n$$\n$\\frac{N_{fn}}{N_{tp}}$表示正例成为伪负例的概率。\n\n损失函数为：\n$$\n\\begin{equation}\n\\label{inportance_sampling_loss_implementation_2}\n\\tag{10}\n\\begin{aligned}\nLoss &= -\\mathbb{E}_{(x, y) \\sim q(x, y)} \\frac{p(y|x)}{q(y|x)} \\cdot \\log{f(y|x,\\theta )}\n\\\\\n& \\approx - \\frac{1}{N} \\sum_{i=1}^{N} [\\frac{p(1|x_i)}{q(1|x_i)} \\cdot y_i \\cdot \\log f(1|x_i,\\theta) + \\frac{p(0|x_i)}{q(0|x_i)} \\cdot (1 - y_i) \\cdot \\log f(0|x_i,\\theta)]\n\\\\\n&= - \\frac{1}{N} \\sum_{i=1}^{N} [ (1 + p(1|x_i) \\frac{N_{fn}}{N_{tp}}) \\cdot y_i \\cdot \\log f(1|x_i,\\theta) + \\frac{1 + (1-p(0|x_i)) \\frac{N_{fn}}{N_{tp}}}{1 + \\frac{1 - p(0|x_i)}{p(0|x_i)} \\frac{N_{fn}}{N_{tp}}} \\cdot (1 - y_i) \\cdot \\log f(0|x_i,\\theta)]\n\\\\\n& \\approx - \\frac{1}{N} \\sum_{i=1}^{N} [ \\left \\langle (1 + f(1|x_i,\\theta) \\frac{N_{fn}}{N_{tp}}) \\right \\rangle \\cdot y_i \\cdot \\log f(1|x_i,\\theta) + \\left \\langle \\frac{1 + f(1|x_i,\\theta) \\frac{N_{fn}}{N_{tp}}}{1 + \\frac{f(1|x_i,\\theta)}{1 - f(1|x_i,\\theta)} \\frac{N_{fn}}{N_{tp}}} \\right \\rangle \\cdot (1 - y_i) \\cdot \\log (1 - f(1|x_i,\\theta))]\n\\end{aligned}\n\\end{equation}\n$$\n其中，$\\frac{N_{fn}}{N_{tp}}$是正例成为伪负例的概率。简单考虑的话，可以假设均匀分布，这样就可以统计出一个固定数值。复杂点的话，可以作为单独的模型$g(t > w_0|x,y=1,{\\theta}')$去学习，独立训练或者和主目标联合训练都可以，表示窗口之外发生转化的概率，其中${\\theta}'$是可学习参数。\n如果$\\frac{N_{fn}}{N_{tp}}=1$，公式$\\eqref{inportance_sampling_loss_implementation_2}$就退化为$\\eqref{inportance_sampling_loss_implementation}$，如果$\\frac{N_{fn}}{N_{tp}}=0$，公式$\\eqref{inportance_sampling_loss_implementation_2}$就退化为常规的交叉熵$\\eqref{calibration_loss_implementation}$。\n\n## PU Loss\n类似方案一。\n$$\n\\begin{equation}\n\\label{pu_loss_implementation_2}\n\\tag{11}\nlogloss =\n\\begin{cases}\n- \\log f(1|x_i,\\theta) & \\text{if $(x, y)$ is a not delayed positive example}\n\\\\\n- (\\log f(1|x_i,\\theta) - \\log (1 - f(1|x_i,\\theta))) & \\text{if $(x, y)$ is a delayed positive example}\n\\\\\n- \\log (1 - f(1|x_i,\\theta)) & \\text{if $(x, y)$ is a negative example}\n\\end{cases}\n\\end{equation}\n$$\n\n# 回补方式三\n前两种回补方式只回补正样本，既FN对应的样本被训练了两次，其他样本训练了一次，所以$p(x) \\neq q(x)$。那Loss推导中的假设条件$p(x)=q(x)$必然不成立，影响模型效果。\n论文[<sup>3</sup>](#refer-anchor-3)，为了使得条件$p(x)=q(x)$成立，可以对其他样本也训练两次，这样每个样本x都能看到两次，假设条件便成立了。\n\n* 真实分布\n  * 正例：$N_{tp}$\n  * 负例：$N_{tn}$\n* 回补方式一和二\n  * 正例：$N_{tp}$\n  * 负例：$N_{tn} + N_{fn}$\n* 回补方式三\n  * 正例：$N_{tp} + N_{tp} - N_{fn}$\n  * 负例：$N_{tn} + N_{fn} + N_{tn}$\n\n## Importance Sampling\n$p(y|x)$和$q(y|x)$的关系推导如下：\n$$\n\\begin{equation}\n\\label{p_q_relation_method_3}\n\\tag{12}\n\\begin{aligned}\np(1|x) &= \\frac{N_{tp}}{N_{tp} + N_{tn}}\n\\\\\n&= \\frac{1}{1 + \\frac{N_{tn}}{N_{tp}}}\n\\\\\n\\\\\nq(1|x) & = \\frac{N_{tp} + N_{tp} - N_{fn}}{2 N_{tp} + 2 N_{tn}}\n\\\\\n&= p(1|x) - \\frac{p(1|x)}{2} \\frac{N_{fn}}{N_{tp}}\n\\\\\n\\\\\np(0|x) &= \\frac{N_{tn}}{N_{tp} + N_{tn}}\n\\\\\n&= \\frac{\\frac{N_{tn}}{N_{tp}}}{1 + \\frac{N_{tn}}{N_{tp}}}\n\\\\\n\\\\\nq(0|x) & = \\frac{2 N_{tn} + N_{fn}}{2 N_{tp} + 2 N_{tn}}\n\\\\\n&= p(0|x) + \\frac{1-p(0|x)}{2} \\frac{N_{fn}}{N_{tp}}\n\\end{aligned}\n\\end{equation}\n$$\n\n损失函数为：\n$$\n\\begin{equation}\n\\label{inportance_sampling_loss_implementation_3}\n\\tag{13}\n\\begin{aligned}\nLoss &= -\\mathbb{E}_{(x, y) \\sim q(x, y)} \\frac{p(y|x)}{q(y|x)} \\cdot \\log{f(y|x,\\theta )}\n\\\\\n& \\approx - \\frac{1}{N} \\sum_{i=1}^{N} [\\frac{p(1|x_i)}{q(1|x_i)} \\cdot y_i \\cdot \\log f(1|x_i,\\theta) + \\frac{p(0|x_i)}{q(y_i=0|x_i)} \\cdot (1 - y_i) \\cdot \\log f(0|x_i,\\theta)]\n\\\\\n&= - \\frac{1}{N} \\sum_{i=1}^{N} [\\frac{1}{1 - \\frac{1}{2} \\frac{N_{fn}}{N_{tp}}} \\cdot y_i \\cdot \\log f(1|x_i,\\theta) + \\frac{1}{1 + \\frac{1-p(0|x_i)}{2p(0|x_i)} \\frac{N_{fn}}{N_{tp}}}  \\cdot (1 - y_i) \\cdot \\log f(0|x_i,\\theta)]\n\\\\\n& \\approx - \\frac{1}{N} \\sum_{i=1}^{N} [ \\left \\langle \\frac{1}{1 - \\frac{1}{2} \\frac{N_{fn}}{N_{tp}}} \\right \\rangle \\cdot y_i \\cdot \\log f(1|x_i,\\theta) + \\left \\langle \\frac{1}{1 + \\frac{1}{2} \\frac{f(1|x_i,\\theta)}{1 - f(1|x_i,\\theta)} \\frac{N_{fn}}{N_{tp}}} \\right \\rangle \\cdot (1 - y_i) \\cdot \\log (1-f(1|x_i,\\theta))]\n\\end{aligned}\n\\end{equation}\n$$\n这里同方案二类似，也需要建模$\\frac{N_{fn}}{N_{tp}}$。因为需要回补真负例，理论上需要等到归因窗口才能拿到真实数据，但时效性明显不能这样做，可以通过设置一个处于$w_0$和$w_1$之间的窗口来近似获得这批回补数据。\n\n# 总结\n方案二和三有个等待窗口用于获取真实label，所以比方案一要准确一点，但方案一的时效性更强。另外，方案二和三因为$N_{tp} \\neq N_{fn}$，在Loss中需要引入一个单独的模型来预测正例成为伪负例的概率，比方法一要复杂很多。\n\n|  方法   | Importance Sampling  | 预估值纠偏  |  PU Loss  |\n|  ----  | ----           | ----      | ----      |\n| 回补方法一    | $- \\frac{1}{N} \\sum_{i=1}^{N} [ \\left \\langle 1 + f(1|x_i,\\theta) \\right \\rangle \\cdot y_i \\cdot \\log f(1|x_i,\\theta) + \\left \\langle (1 + f(1|x_i,\\theta)) \\cdot (1 - f(1|x_i,\\theta)) \\right \\rangle \\cdot (1 - y_i) \\cdot \\log (1 - f(1|x_i,\\theta))]$ | $p(1|x) = \\frac{q(1|x)}{1 - q(1|x)}$ | $- \\frac{1}{N} \\sum_{i=1}^{N} [ y_i \\cdot (\\log f(1|x_i,\\theta) - \\log (1 - f(1|x_i,\\theta))) + (1 - y_i) \\cdot \\log (1 - f(1|x_i,\\theta))]$ | \n| 回补方法二    | $- \\frac{1}{N} \\sum_{i=1}^{N} [ \\left \\langle (1 + f(1|x_i,\\theta) \\frac{N_{fn}}{N_{tp}}) \\right \\rangle \\cdot y_i \\cdot \\log f(1|x_i,\\theta) + \\left \\langle \\frac{1 + f(1|x_i,\\theta) \\frac{N_{fn}}{N_{tp}}}{1 + \\frac{f(1|x_i,\\theta)}{1 - f(1|x_i,\\theta)} \\frac{N_{fn}}{N_{tp}}} \\right \\rangle \\cdot (1 - y_i) \\cdot \\log (1 - f(1|x_i,\\theta))]$ | 无 | 类似方案一 | \n| 回补方法三    | $- \\frac{1}{N} \\sum_{i=1}^{N} [ \\left \\langle \\frac{1}{1 - \\frac{1}{2} \\frac{N_{fn}}{N_{tp}}} \\right \\rangle \\cdot y_i \\cdot \\log f(1|x_i,\\theta) + \\left \\langle \\frac{1}{1 + \\frac{1}{2} \\frac{f(1|x_i,\\theta)}{1 - f(1|x_i,\\theta)} \\frac{N_{fn}}{N_{tp}}} \\right \\rangle \\cdot (1 - y_i) \\cdot \\log (1-f(1|x_i,\\theta))]$ | 无 | 无 |\n\n# 参考资料\n<div id=\"refer-anchor-1\"></div>\n- [1][Addressing Delayed Feedback for Continuous Training with Neural Networks in CTR prediction](http://arxiv.org/abs/1907.06558)\n<div id=\"refer-anchor-2\"></div>\n- [2][Capturing Delayed Feedback in Conversion Rate Prediction via Elapsed-Time Sampling](http://arxiv.org/abs/2012.03245)\n<div id=\"refer-anchor-3\"></div>\n- [3][Real Negatives Matter: Continuous Training with Real Negatives for Delayed Feedback Modeling](http://arxiv.org/abs/2104.14121)\n","tags":["转化率预估","在线学习"],"categories":["tech"]},{"title":"点击率模型负采样后校准方法","url":"/38510d63/","content":"\n<!-- toc -->\n\n# 背景\n点击率模型训练时，由于性能或其他原因，对负样本经常会降采样，导致预测值与真实值偏差，而线上排序时很多情况下需要真实值，所以需要进行校准。比较简单的校准方法有三种，分别介绍一下：\n\n* 负样本加权\n* 在logit上校准\n* 在预测值上校准\n\n# 符号定义\n\n$$\n\\begin{equation}\n\\label{formulation}\n\\begin{aligned}\n& N_{p}：正样本量。& \\\\\n& N_{n}：负样本降采样后的负样本量。& \\\\\n& \\alpha：负采样降采样比例。&\n\\end{aligned}\n\\end{equation}\n$$\n\n# 一、负样本加权\n训练时，在负样本上加权。\n$$\n\\begin{equation}\n\\label{negative_sample_calibration}\n\\begin{aligned}\nloss = - label \\cdot  \\ln p(1|x) - \\frac{1}{\\alpha} \\cdot (1 - label) \\cdot \\ln (1 - p(1|x))\n\\end{aligned}\n\\end{equation}\n$$\n\n# 二、在logit上校准\n在$logit$上加上$\\ln \\alpha$可得到校准后的$logit_{calibration}$，后面用$logit_{calibration}$做计算预估值即可。\n$$\n\\begin{equation}\n\\label{logit_calibration}\n\\begin{aligned}\np(1|x) &= \\frac{N_{p}}{N_{p} + N_{n}}\n\\\\ &= \\frac{1}{1 + \\frac{N_{n}}{N_{p}}}\n\\\\ &= \\frac{1}{1 + \\exp^{-logit}}\n\\\\\n\\\\\nlogit &= \\ln \\frac{N_{p}}{N_{n}}\n\\\\\n\\\\\nlogit_{calibration} &= \\ln \\frac{N_{p}}{\\frac{N_{n}}{\\alpha}}\n\\\\ &= \\ln \\frac{N_{p}}{N_{n}} + \\ln \\alpha\n\\\\ &= logit + \\ln \\alpha\n\\end{aligned}\n\\end{equation}\n$$\n\n# 三、在预测值上校准\n$p(1|x)$做下变换即可得到校准后的$p_{calibration}(1|x)$。\n$$\n\\begin{equation}\n\\label{output_calibration}\n\\begin{aligned}\np(1|x) &= \\frac{N_{p}}{N_{p} + N_{n}}\n\\\\ &= \\frac{1}{1 + \\frac{N_{n}}{N_{p}}}\n\\\\\n\\\\\n\\frac{N_{n}}{N_{p}} &= \\frac{1}{p(1|x)} - 1\n\\\\ &= \\frac{1 - p(1|x)}{p(1|x)}\n\\\\\n\\\\\np_{calibration}(1|x) &= \\frac{N_{p}}{N_{p} + \\frac{N_{n}}{\\alpha}}\n\\\\ &= \\frac{1}{1 + \\frac{1}{\\alpha} \\frac{N_{n}}{N_{p}}}\n\\\\ &= \\frac{1}{1 + \\frac{1}{\\alpha} \\frac{1 - p(1|x)}{p(1|x)}}\n\\\\ &= \\frac{p(1|x)}{p(1|x) + \\frac{1 - p(1|x)}{\\alpha}}\n\\end{aligned}\n\\end{equation}\n$$\n\n# 对比\n\n* 方法一作用到训练上，方法二和三作用到预估时。\n* 方法二和三在理论推导上是一样的，校准后的预估值应该一样。\n* 三个方法校准后评估集PCOC应该都可以接近1.0，但方法一只作用于负样本，通过打压负样本的预估值将PCOC校准到1.0，而方法二和三作用于所有样本，对正样本和负样本的预估值进行了不同程度的打压，使得PCOC接近1.0。\n* 三个方法在评估集GAUC上应该差别不大，具体哪个最好可以在具体业务上尝试一下。\n","tags":["点击率预估","精排"],"categories":["tech"]},{"title":"精排GAUC与业务指标不一致问题","url":"/3d4b29dd/","content":"\n<!-- toc -->\n\n# 背景\n在精排模型迭代时，经常会遇到在离线指标不一致的问题，这里根据过往经验总结下遇到的问题及原因。\n\n# 离线GAUC和在线GAUC不一致\n\n## 场景一：离线GAUC涨，在线GAUC降\n模型主场效应\n\n训练样本一般是曝光空间，而线上预估时打分空间比曝光集合要大几个数量级，模型在不一致的空间上泛化性多多少少会有些问题。在这部分未曝光样本空间里，有些候选会被高估进而得到曝光机会，这些曝光就会获得用户的反馈，进而进入训练集通过模型学习来修正。实验模型只有很少的流量，所以获得的反馈很少，而基线模型流量大，获得的反馈更多，所以实验模型一直打不过基线。\n\n解决办法：\n1）增大实验流量；\n2）增大实验模型训练周期\n\n这种情况经常发生在模型打分分布变化较大时，比如框架迁移、模型重大升级这类动作，因为打分分布变化大会导致曝光差异变大。\n\n## 场景二：离线GAUC降，在线GAUC涨\n\n打分靠前的曝光机会更多，更易获得用户反馈，所以模型在自身的流量上的评估结果可能会更好。\n\n比如以下案例：\n\n基线：A模型，在线90%流量。\n\n实验：B模型，在线10%流量。\n\n精排排序候选1000个，[I1, I2, ..., I1000]，打分排序后吐出10个展示给用户。\n\nA展示了[I1, I3, I4, I6, I38, I194, I285, I396, I823, I956]，其中获得点击的是[I1, I38]。\n\nB展示了[I3, I5, I194, I38, I396, I285, I1, I733, I823, I956]，其中获得点击的是[I3, I94]。\n\n那么A模型的在线GAUC是0.8125，B模型的在线GAUC是0.9375，既实验模型在线GAUC更高。\n\n离线的话，A在实验流量上的GAUC是0.8125，B在基线流量上的GAUC是0.625。因为离线测试集由90%基线样本和10%实验样本组成，所以A在测试集上的GAUC是`0.8125*0.9+0.8125*0.1=0.8125`，B在测试集上的GAUC是`0.625*0.9+0.9375*0.1=0.65625`，既实验模型离线GAUC更低。\n\n解决办法：可以构建无偏的离线测试集（不经过精排，直接由粗排给出排序结果），如果在无偏测试集上GAUC涨的话就上实验。如果无偏测试集不好构造，并且你对实验模型比较有信仰时，可以直接上实验看效果。\n\n\n# 在线GAUC和业务效果指标不一致\n\n## 场景一：在线GAUC涨，业务效果指标降\n业务指标，比如电商类的GMV、广告类的ECPM、社区类的用户时长/点击率之类。打分分布变化往往会导致曝光集合的分布漂移，比如类目分布漂移、价格分布漂移、时效性分布漂移之类，往往会导致AUC和业务指标效果不一致。举个例子，社区场景下视频比图文GAUC高，但视频点击率低，视频比例提升会带动整体在线GAUC涨，但整体点击率下降。\n\n## 场景二：在线GAUC降，业务效果指标涨\n与上面相反。\n\n解决办法：融合公式、重排逻辑等调参，将曝光分布尽可能调平。在调平的基础上，看哪些业务指标有收益，如果主业务目标没涨其他业务指标涨了，再通过调参将其他业务指标收益兑换到主业务目标上。比如，线上ctr涨了但用户时长没涨，那就把视频多出一点图文少出一点，达到ctr持平和用户时长上涨的目的。\n\n# 总结\n离线和在线GAUC都是中间指标，和最终的业务效果指标并不一定强一致，如果遇到不一致问题，可以从多维度进行下钻分析。\n","tags":["点击率预估","精排"],"categories":["tech"]},{"title":"点击率预估中的用户行为序列建模","url":"/8d9c775e/","content":"\n<!-- toc -->\n\n# 背景\n特征工程是CTR建模中最重要的问题。DNN时代之前，主要是人工构造组合特征和FM类自动交叉。迁移DNN初期，主要是FNN、PNN、DeppFM、Wide&Deep这类模型，特点是特征交互基本沿用了浅层模型的方法，区别是后接了MLP。此后，DNN在CTR领域站稳脚跟后，才开始真正的面向DNN，思考怎样进行特征交互建模，比如吸收了Attention等NLP领域的技术，这一阶段主要思考通用的特征交互方法。近几年，又开始面向某类特征，设计专用的特征交互方法，值得一提的是一系列用户行为序列建模的方法在工业界取得了非常大的收益。本文对用户行为序列建模做个概要的思路整理。\n\n# 用户行为序列建模\n\n从用户行为序列和Target Item做交互的粒度这个角度考虑，我把方案划分为以下几种：Element-Wise Interaction、Group-Wise Interaction、Interest-Wise Interaction。当然，也可以按其他维度划分。\n\n## Element-Wise Interaction\n每个序列中的元素与Target Item进行交互，对序列中元素赋予不同权重然后根据权重进行Weighted Sum-Pooling，或者像RNN一样按时间顺序交互。\n\n### DIN\n此前在DNN内使用序列特征大都是直接Pooling，阿里妈妈发的这篇Deep Interest Network[<sup>1</sup>](#refer-anchor-1)可以算是引领用户序列建模潮流的开山之作。论文出发点是序列里不同元素的权重不应该一样，与Target Item关系更近的应该赋予更大权重。权重通过外积接MLP得到，然后对序列进行Weighted Sum-Pooling。\n\n<div style=\"text-align: center\">\n<img src=\"/images/din_model.png\" width=\"900\"/>\n</div>\n\n### DIEN & BST\nDIN没有考虑到行为序列内部的关系，如果考虑序列内部关系，一种是LSTM按顺序学习，另一种是使用Transformer做序列内部的Self-Attention。\n对序列直接使用LSTM效果不好，因为用户行为随机性太强，没有NLP里那种词之间的语法关系模式。阿里妈妈的Deep Interest Evolution Network[<sup>2</sup>](#refer-anchor-2)，考虑了兴趣随时间的演变，设计了一个类似LSTM的复杂兴趣提取结构AUGRU，可以解决LSTM的问题，取时间序列最后一个Embeedding作为交互结果。不过这种按顺序的兴趣提取模型，在长序列上性能会是瓶颈，网络结构也比较复杂，在业界不太流行。在我看来，DNN的大部分结构还是比较符合直觉的，比如Attention甚至更复杂Transformer。但我第一次看LSTM结构时，就被那巧妙设计震惊了，一直很想了解这么复杂的玩意背后的设计过程，它不是那么符合直觉，需要细细的去理解各个门的设计以及电路图似的组合。不太符合直觉的东西，就不太想去用，DIEN文章里提出的AUGRU结构就是这种。\n\n<div style=\"text-align: center\">\n<img src=\"/images/dien_model.png\" width=\"900\"/>\n</div>\n\n类似的工作还有Behavior Sequence Transformer[<sup>8</sup>](#refer-anchor-8)，使用Transformer先得到Sequence-aware的Item Embedding，再把Sequence Item Embedding和Target Embedding以及其他特征拼起来过MLP。BST发表时DIN已经提出了，所以这里应该用Sequence Item Embedding与Target Embedding做下Attention交互会更合适，感觉这个工作不是太完整。\n\n<div style=\"text-align: center\">\n<img src=\"/images/bst_model.png\" width=\"900\"/>\n</div>\n\n### SIM\nElement-Wise Interaction的主要问题是，如果序列非常大，每个元素都与Target Item交互的话，性能是个瓶颈，比如DIN只能处理一两百的序列长度。阿里妈妈的Search-based Interest Model[<sup>3</sup>](#refer-anchor-3)提出了两阶段方法，第一阶段根据Target Item信息从超长序列中检索出一个短的子序列，第二阶段使用短序列与Target Item做交互。\n主要创新点在第一阶段，提出了Hard-Search和Soft-Search，前者根据Target Item的类目或其他属性作为Query去检索子序列，后者使用Embedding做ANN近似检索。论文中指出，Soft-Search的Embedding不可以使用短序列训练出来的，因为短序列和长序列的分布不一致，论文是通过长序列训练了一个辅助CTR模型得到的。截止目前SIM仍然是支持序列最长的方案，根据业界交流，Hard-Search使用更广泛，效果已经非常好了。第二阶段使用Multi-head Target Attention，计算Attention时，把序列Item发生时间与当前的时间差也离散化生成Embeeding拼接到了Item Embeeding上，相当于增加了时间bias，业界应用也会增加一些其他类型的bias特征。\n\n<div style=\"text-align: center\">\n<img src=\"/images/sim_model.png\" width=\"900\"/>\n</div>\n\n### ETA\nSIM的问题是第一阶段，检索逻辑和CTR主目标并不一致，会带来效果损失。阿里妈妈提出的Ene-to-End Target Attention[<sup>4</sup>](#refer-anchor-4)使端到端建模长序列成为可能，可以支持一两千的长序列（虽然还不如SIM长）。主要创新点是引入了SimHash，将Item Embedding映射成一个很小的Binary向量，Binary向量通过Hamming距离可以非常快的计算两个Item的相似度，按SimHash相似度取topK得到的子序列用于后续Multi-head Target Attention。SimHash过程相当于SIM第一阶段，不过检索速度更快，可以支持End-to-End训练了。Serving时Item的SimHash可以离线算好，线上直接按Hamming距离检索。虽然端到端训练了，但第一阶段用SimHash算相似度，与第二阶段的Target Attention其实还是不一致。\n\n<div style=\"text-align: center\">\n<img src=\"/images/eta_model.png\" width=\"900\"/>\n</div>\n\n<div style=\"text-align: center\">\n<img src=\"/images/sim_hash.png\" width=\"900\"/>\n</div>\n\n### SDIM\n美团搜索提出的Sampling-based Deep Interest Modeling[<sup>5</sup>](#refer-anchor-5)进一步优化了ETA的性能。ETA检索出topK Item之后，然后做Pooling（不考虑Target Attention的话）。SDIM在离线得到Item SimHash后，把SimHash分成多片，每片把相应的全部Item Embedding做Pooling，然后写到Tair共线上查询，Tair里的key是分片Binary，value是Pooling结果，所以存储大小和Item的数量没关系了。检索时把Target Item的SimHash也分成多片，每片从Tair里检索Pooling后的Embedding，然后把检索结果再Pooling一次即可，检索量也和Item数量没关系了，检索和Pooling耗时基本可以忽略。从近似检索topK的原理上，SDIM其实不太准确，我感觉效果有效的原因可能是因为检索的更多了，稍有有点匹配的就检索回来了，不受topK的数量限制，虽然质量没那么强但数量可以弥补。\n\n<div style=\"text-align: center\">\n<img src=\"/images/sdim_model.png\" width=\"900\"/>\n</div>\n\n### TWIN\nETA虽然做到了端到端训练，但也没解决第一阶段的检索和第二阶段Attention匹配。快手提出的Two-stage Interest Network[<sup>12</sup>](#refer-anchor-12)，巧妙的优化了Multi-Head Target Attention计算效率，提出了Efficient Target Attention。第一阶段和第二阶段使用同一个Target Attention，保证了一二阶段的匹配。优化点是这样：用户行为序列里的Item特征，包含Item自身特征、以及User&Item交互特征，其中Item自身特征这部分在Attention里的中间结果可以缓存起来，放KV存储Serving时直接检索即可，而User&Item交互特征部分，降维到1，实时计算效率也很高。通过这两点优化，第一阶段的MHTA权重也可以在Serving时对全量序列实时计算了，选出topK给第二阶段。既然第一阶段的权重都计算好了，为什么还要选topK，直接在全量序列上Weighted Sum-Pooling不行吗？我认为其实是可以的，论文没这么做的出发点是这样：第一阶段缓存的中间参数更新没有那么及时，所以对全量序列得到的权重不是最新的，而第二阶段参数实时更新，可以算更准确的权重（我理解第二阶段的Sequence Item Embedding也是实时算）。\n\n<div style=\"text-align: center\">\n<img src=\"/images/twin_model.png\" width=\"900\"/>\n</div>\n\n## Group-Wise Interaction\n将Element进一步聚合成Group，Group与Target Item进行交互。\n\n### DSIN\nSession是按时间划分用户会话的方式，比如一般连续行为未超过30分钟算同一个Session。在搜索场景，Session内部行为很近似，Session间差异很大。根据这一特点，阿里妈妈提出的Deep Session Interest Network[<sup>6</sup>](#refer-anchor-6)，将序列划分成多个Session，每个Session内部先聚合形成一个Embedding，多个Session Embedding与Target Item进行交互。论文提取的Session Embeding生成方法比较复杂，据我了解，Session内简单Pooling就效果非常好。好像推荐用DSIN的不太多，可能Session这个特点在推荐不是太符合。\n\n<div style=\"text-align: center\">\n<img src=\"/images/dsin_model.png\" width=\"900\"/>\n</div>\n\n### RACP\n阿里提出的Recurrent Attention over Contextualized Page Sequence[<sup>9</sup>](#refer-anchor-9)，将序列元素按浏览页面Page进行聚合，每个Page内既有点击的也有曝光未点击item。Page先提取Embedding，然后Page Embedding通过类似DIEN的方式与Target Item做交互。Page考虑了元素的上下文信息，对行为刻画更充分。\n\n<div style=\"text-align: center\">\n<img src=\"/images/racp_model.png\" width=\"900\"/>\n</div>\n\n## Interest-Wise Interaction\n先根据用户行为序列提取出Interest Embedding，再使用Interest Embedding与Target Item交互。\n\n### MIMN\n阿里妈妈提出的Multi-channel user Interest Memory Network[<sup>7</sup>](#refer-anchor-7)，也是针对长序列设计的。不过与SIM、ETA、SDIM、TWIN这些两阶段模型不同，MIMN针对长序列是端到端训练，但Serving时把长序列相关的部分单独拿出去部署。一般端到端训练也只能支持一两千的序列长度，所以比两阶段上万的序列还是短一些。MIMN的长序列兴趣提取部分与Target Item无关，根据长序列自身提取用户多个Interest Embedding，Interest Embedding预计算存入Tair供CTR模块使用。论文中长序列独立部署模块是实时接收用户行为，实时更新Tair里的向量。因为CTR主模块和长序列兴趣提取一起训练，但分开部署，所以模型更新时会有不一致，论文说即使长达一天的不一致对效果影响也不大，因为长序列的兴趣比较稳定，不会剧烈变化。美团广告其实也采用的这种长序列建模方法，不过兴趣向量是天级更新，时效性通过实时行为序列建模能弥补回来。对于兴趣怎么提取，Multi-Head Self Attention、Capsule都有广泛应用。MIMN里基于Neural Turing Machine，提出了一个更复杂的兴趣提取结构。另外，这类方法在抽取兴趣时和Target Item无关，所以在召回的双塔模型中用的也很普遍。\n\n<div style=\"text-align: center\">\n<img src=\"/images/mimn_model_2.png\" width=\"900\"/>\n</div>\n\n<div style=\"text-align: center\">\n<img src=\"/images/mimn_model.png\" width=\"900\"/>\n</div>\n\n# 其他改进\n\n## 序列拆分\n拆成多个序列，每个序列可以应用不同的交互方式。\n\n* 按时间拆分序列，比如分成Life-Long长序列和实时行为序列，甚至长序列再按周期拆分成多个子序列。\n* 按场景拆分序列，比如拆分搜索行为和推荐行为序列、拆分图文推荐和视频推荐行为序列。\n\n## 行为类型\n业界对点击、收藏、转化这类行为序列建模比较普遍了，后续可以对更多序列数据进行建模，比如曝光序列、负反馈序列等，相关工作有腾讯的DFN[<sup>10</sup>](#refer-anchor-10)。\n\n## 行为特征\n序列元素的ID、类目等属性信息一般建模都使用了，序列元素发生时刻所处的Context特征用的还不是特别充分，相关工作有美团广告的DCIN[<sup>11</sup>](#refer-anchor-11)。\n\n# 参考\n<div id=\"refer-anchor-1\"></div>\n- [1][Deep Interest Network for Click-Through Rate Prediction](http://arxiv.org/abs/1706.06978)\n<div id=\"refer-anchor-2\"></div>\n- [2][Deep Interest Evolution Network for Click-Through Rate Prediction](http://arxiv.org/abs/1809.03672)\n<div id=\"refer-anchor-3\"></div>\n- [3][Search-based User Interest Modeling with Lifelong Sequential Behavior Data for Click-Through Rate Prediction](http://arxiv.org/abs/2006.05639)\n<div id=\"refer-anchor-4\"></div>\n- [4][End-to-End User Behavior Retrieval in Click-Through RatePrediction Model](http://arxiv.org/abs/2108.04468)\n<div id=\"refer-anchor-5\"></div>\n- [5][Sampling Is All You Need on Modeling Long-Term User Behaviors for CTR Prediction](http://arxiv.org/abs/2205.10249)\n<div id=\"refer-anchor-6\"></div>\n- [6][Deep Session Interest Network for Click-Through Rate Prediction](http://arxiv.org/abs/1905.06482)\n<div id=\"refer-anchor-7\"></div>\n- [7][Practice on Long Sequential User Behavior Modeling for Click-Through Rate Prediction](http://arxiv.org/abs/1905.09248)\n<div id=\"refer-anchor-8\"></div>\n- [8][Behavior Sequence Transformer for E-commerce Recommendation in Alibaba](http://arxiv.org/abs/1905.06874)\n<div id=\"refer-anchor-9\"></div>\n- [9][Modeling Users' Contextualized Page-wise Feedback for Click-Through Rate Prediction in E-commerce Search](http://arxiv.org/abs/2203.15542)\n<div id=\"refer-anchor-10\"></div>\n- [10][Deep Feedback Network for Recommendation](https://www.ijcai.org/proceedings/2020/349)\n<div id=\"refer-anchor-11\"></div>\n- [11][Deep Context Interest Network for Click-Through Rate Prediction](http://arxiv.org/abs/2308.06037)\n<div id=\"refer-anchor-12\"></div>\n- [12][TWo-stage Interest Network for Lifelong User Behavior Modeling in CTR Prediction at Kuaishou](http://arxiv.org/abs/2302.02352)\n","tags":["点击率预估","CTR"],"categories":["tech"]},{"title":"点击率预估中的特征交互方法","url":"/166b866f/","content":"\n<!-- toc -->\n\n# 背景\n我从事广告算法多年了，经历过点击率模型从XGBoost、大规模离散逻辑回归、FFM到后来DNN的演进。2017年从FFM迁移DNN时，主要考察的是FNN、PNN、DeppFM、Wide&Deep这类模型，特点是特征交互基本沿用了浅层模型的方法，区别是后接了MLP。此后，DNN在CTR领域站稳脚跟后，才开始真正的面向DNN，思考怎样进行特征交互建模，比如吸收了Attention等NLP领域的技术，这一阶段主要思考通用的特征交互方法。近几年，又开始面向某类特征，设计专用的特征交互方法，值得一提的是一系列用户行为序列建模的方法在工业界取得了非常大的收益。本文记录一下对前两个阶段的一些工作的理解，第三阶段以后单独开一篇。\n\n# 特征交互方法\n\n## 笛卡尔积\n两个离散特征A、B做拼接成特征C，既`C = A&B`，参数空间为$N_{A} N_{B} D$，其中`D`为embedding size。DNN在搜广推早期应用时，有些言论说Deep侧不用加组合特征，由mlp隐式学习组合就行，其实效果上是站不住脚的。根据我的经验，在Deep侧加一些组合特征，提供一些人工先验知识，可以帮助模型学习的更好。但组合特征本身也存在问题，这才引出后面很多怎么在模型内部进行特征交叉的结构优化工作。\n\n* 优点：记忆性强，能显著提升模型效果。\n* 难点：特征量级大导致模型非常大，需要比较强的工程能力。特征稀疏，存在很多低频组合特征，容易过拟合。一般通过频次准入控制过拟合，但对不同类型的特征组合可能需要不同频次，不太好设定。\n* 缺点：泛化性不好，训练中没有出现的特征组合，在预估时查不到。\n\n## FM\n每个特征有单独的emb，两个特征的emb内积作为交叉。参数空间为$N_{A} D + N_{B} D$。\n\n$$\n\\begin{equation}\n\\label{fm_model}\n\\begin{aligned}\nlogit = \\sum_{i=0}^{N} {w_i \\cdot x_i} + \\sum_{i=0}^{N-1} \\sum_{j=i+1}^{N} {\\left< \\vec{v_i}, \\vec{v_j} \\right> \\cdot x_i \\cdot x_j} + bias\n\\end{aligned}\n\\end{equation}\n$$\n\n* 优点：有一定泛化性，训练中没有出现的特征组合，在预估时也可以得到内积。\n* 缺点：每个特征只有一个emb，与不同特征组合时可能需要不同的emb。比如`A&B = 男性&20岁`对美女视频点击率高，`A&B = 女性&20岁`对帅哥视频点击率高，那`20岁`这个特征就被互相拉扯，最终指向性不明确了。\n\n## FFM\n针对FM缺点的优化方案，每个特征，对每个slot，有单独的emb，两个特征的计算内积时，取对应的slot emb。参数空间为$N_{A} N_{slot} D + N_{B} N_{slot} D$。\n在工业界，会把slot聚合成group，每个特征对每个group学习一个emb，也可以选择哪些group之间可以交叉。在降低参数量的同时还能提效果，参数空间为$N_{A} N_{group} D + N_{B} N_{group} D$。\n根据我的经验，效果真的强，同等特征条件下，我们最初上DNN时用的Wide&Deep也就稍微好一点点，也有大厂用FFM&Deep。\n\n$$\n\\begin{equation}\n\\label{ffm_model}\n\\begin{aligned}\nlogit = \\sum_{i=0}^{N} {w_i \\cdot x_i} + \\sum_{i=0}^{N-1} \\sum_{j=i+1}^{N} {\\left< \\vec{v_{i,j}}, \\vec{v_{j,i}} \\right> \\cdot x_i \\cdot x_j} + bias\n\\end{aligned}\n\\end{equation}\n$$\n\n* 优点：解决了FM的缺点，效果更好。\n* 缺点：group聚合设计、group间交叉关系，需要一些经验。\n\n## FNN\n论文[<sup>1</sup>](#refer-anchor-1)出发点是DNN学习大规模离散特征的emb太复杂，所以基于预训练FM对每个特征学出一个w和一个v向量，FNN将所有特征的w和v拼接起来过dnn。论文的出发点在当时是存在的，当时并没有能够支持大规模离散特征的开源深度学习框架，只有某些大厂有自研能力，包括百度凤巢第一版DNN时，也是先学习的逻辑回归，然后把逻辑回归的w拼起来过dnn，后面才有的MIO-Framework、Abacus、AI-Box深度学习框架。从方案上看，FM训练时，w是用于reduce sum，v是用于内积，FNN把它们拼起来过mlp，虽然理论上mlp是万能逼近器，能逼近出reduce sum和内积的效果，期待在有个保底的基础上，然后找到一个比reduce sum和内积更好的方程。根据我的经验，w、v和后面的mlp，驴唇不对马嘴，mlp连学习到保底水平都达不到，最终效果还不如FM。\n\n## PNN\n论文[<sup>2</sup>](#refer-anchor-2)算是对FM的一个扩展，两个向量内积后，后面接mlp，而不是直接reduce sum。除了内积，也可以使用外积或内外积同时使用。内积PNN相比于FNN，不同的是向量端到端训练，可以适应mlp，而不像FNN那样驴唇不对马嘴。内积和外积算是比较基础的交互方法了，在其他模型比如NLP里应用也比较广泛，可以作为最基础的特征交互方法。\n\n## NFM\n论文[<sup>12</sup>](#refer-anchor-12)和PNN有点像，不同的是两个向量做哈达玛积，后面接mlp。有个AFM类似的工作，两个向量哈达玛积的基础上，再乘个attention，感觉有点多余。\n\n## DeepFM\n论文[<sup>3</sup>](#refer-anchor-3)和Wide&Deep差不多，叫FM&Deep更合适，论文把Wide侧换成FM模型，并且FM侧的emb和Deep侧的emb是共享的。这个结构在工业界用的挺多，不过和论文有一些不同点，比如FM侧和Deep侧的emb不共享、FM侧用FFM。\n\n## Deep Crossing\n[<sup>7</sup>](#refer-anchor-7)加了个Residual，没啥好说的。之前在ffm迁dnn时试过，没用。\n\n## DCN\n[<sup>4</sup>](#refer-anchor-4)把所有特征的emb起来后，每层通过简单乘法进行特征交叉，层数越多交叉越高阶，后面又出了个v2版[<sup>5</sup>](#refer-anchor-5)。DCN是bit-wise交互，并且只用最高阶的输出作为MLP输入。我之前在ffm迁dnn时试过v1版，效果不如wide&deep。\n\n$$\n\\begin{equation}\n\\label{dcn_model}\n\\begin{aligned}\nx_{l+1} = x_0 x_{l}^{T} w_{l} + b_{l} + x_{l}\n\\end{aligned}\n\\end{equation}\n$$\n\n<div style=\"text-align: center\">\n<img src=\"/images/dcn_model.png\" width=\"640\"/>\n</div>\n\n## xDeepFM\n[<sup>6</sup>](#refer-anchor-6)每个slot emb间做哈达玛积，再过一个全连接网络，然后累加起来，得到下一层某个slot的emb。每次计算下一层一个slot emb，相当于遍历了所有高阶组合，多个slot有一点multi-head的作用，每个slot关注不同层面的组合。最后把所有阶的emb pooling起来过mlp。与DCN不同，xDeepFM关注slot-wise的交互，并且每阶的交互都保留下来过MLP。\n\n$$\n\\begin{equation}\n\\label{xdeepfm_model}\n\\begin{aligned}\nx_{h}^{k} = \\Sigma_{i=1}^{H_{k-1}} \\Sigma_{j=1}^{H_{0}} W_{i,j}^{k,h} x_{i}^{k-1} x_{j}^{0}\n\\end{aligned}\n\\end{equation}\n$$\n\n<div style=\"text-align: center\">\n<img src=\"/images/xdeep_fm_model.png\" width=\"640\"/>\n</div>\n\n难点：nn参数和计算量有点大，不太清楚有哪些工业界落地。\n\n## FiBiNET\n[<sup>14</sup>](#refer-anchor-14)内积和哈达玛积太简单，FiNiNET里它俩结合了一下，叫做Bi-Linear Interaction，如下图。\n\n<div style=\"text-align: center\">\n<img src=\"/images/fibinet_interaction.png\" width=\"640\"/>\n</div>\n\n论文另一个主要贡献是引入了SENet，学习特征的权重，权重作用到原始emb后得到新emb。然后，在原始emb和新emb上分别使用Bi-Linear Interaction进行二阶交互，然后全拼起来过mlp。\n\n<div style=\"text-align: center\">\n<img src=\"/images/fibinet_model.png\" width=\"720\"/>\n</div>\n\nSENet后来在粗排用的挺多的，用于淘汰一些没用的特征，提升模型速度。Bi-Linear Interaction的话不太了解哪些厂有落地。\nFiNiNET的特征交互后太宽，FiNiNET++[<sup>15</sup>](#refer-anchor-15)对它进行了优化。\n\n\n## AutoInt\n[<sup>8</sup>](#refer-anchor-8)每个slot得到emb后，对所有slot做multi-head attention，得到融合了交互信息的slot emb。和xDeepFM类似，都是slot-wise交互。xDeepFM像RNN一样，下一阶的交互要依赖更浅一阶的交互，如果AutoInt做多层multi-head attention，也能获得高阶交互。\n\n<div style=\"text-align: center\">\n<img src=\"/images/autoint_model.png\" width=\"640\"/>\n</div>\n\n* 难点：计算量有点大。学术界作品，不清楚工业界有没有落地。\n\n## Co-Action Unit\n[<sup>9</sup>](#refer-anchor-9)阿里妈妈提出的，在两个离散特征A、B做交叉时，把A的emb拆分成mlp每层的参数，B的emb作为mlp输入，mlp的输出向量作为交叉结果。\n\n<div style=\"text-align: center\">\n<img src=\"/images/alimama_can_model.png\" width=\"720\"/>\n</div>\n\n参数空间为$N_{A} T + N_{B} D$，其中$T > D ,\\quad T << N_{B}$，T为作为mlp参数的emb大小。\n这个结构对两个特征不是对称的，需要一个做mlp参数，一个做mlp输入。论文中做User Action Sequence和Target Item交叉时，把Target Item作为mlp，User Action Sequence中的项做mlp输入，文中没有解释这种选择的原因以及反过来的效果。文章[<sup>11</sup>](#refer-anchor-11)推测，是由于Target Item被更多地共享了，所以要拓宽其参数空间，更好的包容来交叉的信息。\n论文和作者解读[<sup>10</sup>](#refer-anchor-10)的出发点比较有说服力，但最终这个结构是否能够解决论文的出发点我还是比较疑惑。和FM对比的话，缺点是一样的，会不会是mlp比内积效果好导致的效果提升。为了解决这个缺点，当然也可以像FFM那样设计，参数空间变为$N_{A} N_{group} T + N_{B} N_{group} D$，比FFM参数稍多一点。\n[<sup>13</sup>](#refer-anchor-13)介绍了CAN落地的一些工程细节，可以参考。\n\n\n# 参考\n<div id=\"refer-anchor-1\"></div>\n- [1][Deep Learning over Multi-field Categorical Data: A Case Study on User Response Prediction](http://arxiv.org/abs/1601.02376)\n<div id=\"refer-anchor-2\"></div>\n- [2][Product-based Neural Networks for User Response Prediction](http://arxiv.org/abs/1611.00144)\n<div id=\"refer-anchor-3\"></div>\n- [3][DeepFM: A Factorization-Machine based Neural Network for CTR Prediction](http://arxiv.org/abs/1703.04247)\n<div id=\"refer-anchor-4\"></div>\n- [4][Deep & Cross Network for Ad Click Predictions](https://arxiv.org/abs/1708.05123)\n<div id=\"refer-anchor-5\"></div>\n- [5][DCN V2: Improved Deep & Cross Network and Practical Lessons for Web-scale Learning to Rank Systems](http://arxiv.org/abs/2008.13535)\n<div id=\"refer-anchor-6\"></div>\n- [6][xDeepFM: Combining Explicit and Implicit Feature Interactions for Recommender Systems](http://arxiv.org/abs/1803.05170)\n<div id=\"refer-anchor-7\"></div>\n- [7][Deep Crossing: Web-Scale Modeling without Manually Crafted Combinatorial Features](http://www.kdd.org/kdd2016/papers/files/adf0975-shanA.pdf)\n<div id=\"refer-anchor-8\"></div>\n- [8][AutoInt: Automatic Feature Interaction Learning via Self-Attentive Neural Networks](http://arxiv.org/abs/1810.11921)\n<div id=\"refer-anchor-9\"></div>\n- [9][CAN: Feature Co-Action for Click-Through Rate Prediction](https://arxiv.org/abs/2011.05625)\n<div id=\"refer-anchor-10\"></div>\n- [10][想为特征交互走一条新的路](https://zhuanlan.zhihu.com/p/287898562)\n<div id=\"refer-anchor-11\"></div>\n- [11][特征交互新思路|阿里 Co-action Network论文解读](http://xtf615.com/2021/01/10/can/)\n<div id=\"refer-anchor-12\"></div>\n- [12][Neural Factorization Machines for Sparse Predictive Analytics](http://arxiv.org/abs/1708.05027)\n<div id=\"refer-anchor-13\"></div>\n- [13][如何在工业界优化点击率预估:（五）特征交叉建模](https://zhuanlan.zhihu.com/p/489284765)\n<div id=\"refer-anchor-14\"></div>\n- [14][FiBiNET: Combining Feature Importance and Bilinear feature Interaction for Click-Through Rate Prediction](http://arxiv.org/abs/1905.09433)\n<div id=\"refer-anchor-15\"></div>\n- [15][FiBiNet++: Reducing Model Size by Low Rank Feature Interaction Layer for CTR Prediction](http://arxiv.org/abs/2209.05016)\n","tags":["点击率预估","CTR","特征工程","Feature-Engineering"],"categories":["tech"]},{"title":"同步模式模型训练中存在的一个简单但普遍的问题","url":"/a6a552a7/","content":"\n<!-- toc -->\n\n# 背景\n点击率预估模型训练，在早期阶段由于模型结构比较简单，稀疏Embedding占比非常大而稠密参数较少，因此异步训练存在的参数更新冲突和延迟问题并不严重，异步训练是普遍采用的方式。随着Attention等复杂结构在稠密网络部分的应用，稠密参数的影响力变大，异步训练带来的参数更新问题越来越严重，制约着模型训练效果，另外随着GPU的应用，同步训练的性能问题也有缓解，所以同步训练渐渐成为主流。\n同步训练有两种方式，一种是基于Parameter Server的同步训练，一种是基于AllReduce方式的训练。以目前推荐系统领域依然重度使用的TensorFlow为例，第一种经常采用TensorFlow SyncReplicasOptimizer，第二种经常采用Horovod TensorFlow。但这两种方式都存在一个简单却多年无人去解决的问题，对于用户群体这么大的框架来说，有点匪夷所思。\n\n## 问题描述\n同步训练的逻辑如下：\n```\nepoch = 0\nwhile epoch < max_epoch:\n  try read batch_data from data_iterator:\n    1）前向计算；\n    2）后向计算获得梯度；\n    3）所有worker同步梯度；\n    4）参数更新；\n  except OutOfRange:\n    save_checkpoint\n    epoch++\n```\n第3步需要收集所有worker的梯度，这里会存在一个问题：\n数据并行并不能保证每个worker的数据量一模一样，导致worker的`batch_num = all_data_num / worker_num / batch_size`可能稍有不同。在收集梯度时，可能有些worker上的数据已经消耗完，这样就永远无法集齐`work_num`个梯度，导致训练任务卡住。\n这个问题在PS模式或AllReduce都存在：\n\n* [TensorFlow SyncReplicasOptimizer问题](https://stackoverflow.com/questions/43747200/on-a-distributed-tensorflow-1-0-1-chief-worker-hangs-at-end-of-training-when-u\n)\n* [Horovod TensorFlow AllReduce问题](https://github.com/horovod/horovod/discussions/3887)\n\n# 不完美的解决方案\n由于每个worker的`batch_num`都不一样，那就不用它了，用一个全局的`max_step`做停止条件。\n具体的方式为：\n```\nstep = 0\nwhile step < max_step:\n  1）前向计算；\n  2）后向计算获得梯度；\n  3）所有worker同步梯度；\n  4）参数更新；\n  5）step++;\n  6）每n_step保存checkpoint；\n```\n这种方式是业界比较常见的方式，但还是有几个问题：\n\n* 由于每个worker都运行相同的步数，但每个worker的数据量不一致，就会有少量的数据多训练或少训练了一定次数。\n* `max_step`要提前算好，对于大数据集可能比较浪费时间，人工拍一个值的话又不准。\n* 对于稀疏特征的频次准入，只需要在第一轮进行累计和判断，第二轮起就不能再累计了，否则会严重过拟合，所以按轮次训练更优。\n\n这里还有需要注意的几个点：\n\n1）SyncReplicaOptimizer奇葩的同步机制设计问题：\n\n> https://github.com/tensorflow/tensorflow/issues/11753\n> https://stackoverflow.com/questions/36762872/distributed-tensorflow-tf-train-syncreplicasoptimizer-seems-not-synchronized\n> \n> SyncReplicaOptimizer does not really care if the `replicas_to_aggregate` gradients come from the different workers or not. Even if other workers are waiting or not initialized, the chief starts training immediately. And if you print the global_steps you will see same global_steps for `replicas_to_aggregate` times. This means that the chief pushes enough gradients for `tf.train.SyncReplicaOptimizer` to average and apply the gradients. So, start the chief worker process only after starting all other workers.\n\n原因是：\n\n> https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/train/SyncReplicasOptimizer\n> \n> Only after all variables have been updated, increment the global step.\n> Only after step 4, pushes global_step in the token_queue, once for each worker replica. The workers can now fetch the global step, use it to update its local_step variable and start the next batch. Please note that some workers can consume multiple minibatches, while some may not consume even one. This is because each worker fetches minibatches as long as a token exists. If one worker is stuck for some reason and does not consume a token, another worker can use it.\n\n为了解决这个问题，不要用：\n```\nhook = optimizer.make_session_run_hook(is_chief)\n```\n要用：\n```\nhook = optimizer.make_session_run_hook(is_chief, num_tokens=0)\n```\n\n2）Exception in thread QueueRunnerThread-dummy_queue-sync_token_q_EnqueueMany\n\n> https://github.com/tensorflow/tensorflow/issues/20833\n\n因为每个worker的数据量不一致，在最后一个step的时候，有些worker已经结束了，但梯度收集器还在等待梯度入队列，等不到就会超时报异常。\n为了解决这个问题，dataset不要只repeat N轮，需要设成无限，通过`max_step`进行停止。\n\n不要用：\n```\ntf.data.SomeDataset(...).repeat(epoch=N).batch(batch_size)\n```\n要用：\n```\ntf.data.SomeDataset(...).repeat(epoch=-1).batch(batch_size)\n```\n\n3）根据样本量计算出来的`max_step`，实际运行的step数比这个少就退出了。\n\n原因是丢弃了一些过时梯度。\n\n> https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/train/SyncReplicasOptimizer\n> \n> In a typical asynchronous training environment, it's common to have some stale gradients. For example, with a N-replica asynchronous training, gradients will be applied to the variables N times independently. Depending on each replica's training speed, some gradients might be calculated from copies of the variable from several steps back (N-1 steps on average). This optimizer avoids stale gradients by collecting gradients from all replicas, averaging them, then applying them to the variables in one shot, after which replicas can fetch the new variables and continue.\n\n解决方案：同2，给数据多留一些buffer。\n\n\n# 完美的解决方案\n在TensorFlow/DNN时代之前，为了训练大规模离散LR、FFM类模型，我在公司内部开发了一个基于PS的分布式训练框架，其支持BSP、ASP、SSP三种同步方式，其中的BSP训练方式当时也面临着同样问题。我的解法是：\n当有worker已经读完一轮数据后，它向ps节点发送一个`worker_done`信号，ps侧每轮会从0开始累加这个信号，记为`worker_done_num`，每次进行梯度收集时，只收集`worker_num - worker_done_num`个梯度就进行参数更新，这样就能完美的保证每条数据都训练`max_epoch`次并且不会卡住。\n","tags":["Deep-Learning","TensorFlow"],"categories":["tech"]},{"title":"DeepSpeed-Chat强化学习策略","url":"/be7f3b4f/","content":"\n<!-- toc -->\n\n# 背景\nChatGPT出现后，已经有许多开源项目尝试复现其效果，包括LLaMa、DeepSpeed-Chat、ColossalChat、ChatGLM等。其中DeepSpeed-Chat是微软Deep Speed团队的开源项目，其完整的提供了Supervised Fine-tuning、Reward Model Training、RLHF PPO Traing三阶段的代码，逻辑简单，模块划分清晰，另外也由于Deep Speed在大模型训练中的使用非常普遍，所以笔者近期正在研究DeepSpeed-Chat的代码。之前博客中已经介绍了全部三阶段的训练实战情况：\n\n  * [DeepSpeed-Chat全流程训练实战](https://mathmach.com/ff7e0546/)\n  \n本文以DeepSpeed-Chat的实现为例，详细介绍下RLHF——基于人类反馈的强化学习策略，并与经典Off-Policy Actor-Critic策略做对比。\n\n# 符号定义\n$$\n\\begin{equation}\n\\label{formulation}\n\\begin{aligned}\n& N：样本量。& \\\\\n& T：token序列最大长度。& \\\\\n& t：动作顺序，既预测第t个token时刻，属于[1,\\,T]。& \\\\\n& s_{t}：t时刻的状态，既当前已知的token序列。& \\\\\n& a_{t}：t时刻做出的动作，既预测下一个token。& \\\\\n& rw_{t}：做出a_{t}动作的实际reward。& \\\\\n& rn_{t}：当前状态s_{t}下未来能获得的期望平均收益，用于回归State \\,Value \\,Function。& \\\\\n& adv_{t}：优势函数Advantage \\,Function，表示在s_{t}状态下选择特定的a_{t}相比其他a的优势值。& \\\\\n& P(a_{t}|s_{t};\\,\\theta)：Actor \\,Model预估值，根据状态选择动作。& \\\\\n& V(s_{t};\\,\\theta)：State \\,Value \\,Function，模型预估值，表示当前状态下未来能获得的期望平均收益。& \\\\\n& Q(s_{t},a_{t};\\,\\theta)：State-Action \\,Value \\,Function，模型预估值，表示当前状态下采取某行动后未来能获得的期望平均收益。& \\\\\n& R(s_{t},a_{t})：Reward \\,Model预估值，表示在s_{t}状态下选择a_{t}时获得的即时收益。& \\\\ \n& \\theta：最新模型参数。& \\\\\n& {\\theta}'：老的的模型参数。&\n\\end{aligned}\n\\end{equation}\n$$\n\n# Off-Policy Advantage Actor-Critic标准范式\nOff-Policy策略，在老版本参数${\\theta}'$的模型下做出动作选择和环境交互，放入样本池，在后面训练过程中可以重复利用。样本格式可以为：\n$$\n\\begin{equation}\n\\label{off_policy_sample}\n\\begin{aligned}\n(s_{t}, \\, a_{t}, \\, P(a_{t}|s_{t};\\,{\\theta}'), \\, rw_{t}, \\, adv_{t}, \\, rn_{t})\n\\end{aligned}\n\\end{equation}\n$$\n\n标准Advantage Actor-Critic强化学习中，$Actor的P(a_{t}|s_{t};\\,\\theta)$和Critic的$V(s_{t};\\,\\theta)$是可训练模型。\n\n其中Actor模型的Loss为：\n$$\n\\begin{equation}\n\\label{standrd_actor_critic_loss}\n\\begin{aligned}\nActorLoss &= - \\mathbb{E}_{(s_t, a_t) \\sim P(a_{t}|s_{t};\\,{\\theta}')} [ \\frac{P(a_{t}|s_{t};\\,\\theta)}{P(a_{t}|s_{t};\\,{\\theta}')} adv_{t} ]\n\\\\ &= - \\frac{1}{NT} \\sum_{i=1}^{N} \\sum_{t=1}^{T} \\frac{P(a_{i,t}|s_{i,t};\\,\\theta)}{P(a_{i,t}|s_{i,t};\\,{\\theta}')} adv_{i,t}\n\\\\ &= - \\frac{1}{NT} \\sum_{i=1}^{N} \\sum_{t=1}^{T} \\frac{P(a_{i,t}|s_{i,t};\\,\\theta)}{P(a_{i,t}|s_{i,t};\\,{\\theta}')} (Q(s_{i,t},a_{i,t};\\,{\\theta}') - V(s_{i,t};\\,{\\theta}'))\n\\\\ &= - \\frac{1}{NT} \\sum_{i=1}^{N} \\sum_{t=1}^{T} \\frac{P(a_{i,t}|s_{i,t};\\,\\theta)}{P(a_{i,t}|s_{i,t};\\,{\\theta}')} (rw_{i,t} + V(s_{i,t+1};\\,{\\theta}') - V(s_{i,t};\\,{\\theta}'))\n\\end{aligned}\n\\end{equation}\n$$\n\n# DeepSpeed-Chat强化学习策略\nDeepSpeed-Chat和ColossalChat强化学习部分的策略借鉴了TRLX开源项目。从InstructGPT论文和一些开源复现中，可以推测出ChatGPT对于step和episode的定义。每次预估下一个token是一个step，完成一个完整response是一个episode。\n\n## Reward设计\n每个episode获得一个收益$R$，由Reward Model预估得到，Reward Model相当于强化学习中的环境。并且，所有step共享episode的reward。\nReward除了Reward Model预估值外，增加了当前Actor模型与SFT模型的KL散度，保证Actor模型不要改变的太远。因为Off-Policy理论中，采样模型和最新模型接近时才有效果保障，否则需要非常多的采样样本，因此这里增加KL保障是符合理论要求的。不过这里的KL计算逻辑和严格数学定义也不太一样。\n\n$$\n\\begin{equation}\n\\label{reward_function}\n\\begin{aligned}\nrw_{t} = \\max(\\min(R, rw^{upper}), -rw^{upper}) - \\lambda_{kl} \\log \\frac{P(a_{t}|s_{t};\\,\\theta)}{P(a_{t}|s_{t};\\,\\theta^{SFT})}\n\\end{aligned}\n\\end{equation}\n$$\n```python\ndef compute_rewards(self, prompts, log_probs, ref_log_probs, reward_score, action_mask):\n    kl_divergence_estimate = -self.kl_ctl * (log_probs - ref_log_probs)\n    rewards = kl_divergence_estimate\n    start = prompts.shape[1] - 1 # 状态s_1在prompt最后一个token，动作a_1表示预测response的第一个token\n    ends = start + action_mask[:, start:].sum(1)\n    reward_clip = torch.clamp(reward_score, -self.clip_reward_value, self.clip_reward_value)\n    batch_size = log_probs.shape[0]\n    for j in range(batch_size):\n        rewards[j, start:ends[j]][-1] += reward_clip[j] # 在最后一个token加reward_score\n    return rewards\n```\n\n## Advantage设计\n在标准的Advantage Actor-Critic策略中，$adv_{t}=rw_{t} + V(s_{t+1};\\,{\\theta}') - V(s_{t};\\,{\\theta}')$。与此不同，ChatGPT的reward加在了最后一个token，因此每一步$adv_{t}$依赖下一步$adv_{t+1}$，可以看到计算adv时是从后向前遍历，reward从后向前传。\n$$\n\\begin{equation}\n\\label{advantage_function}\n\\begin{aligned}\nadv_{t} = rw_{t} + \\gamma V(s_{t+1};\\,{\\theta}') - V(s_{t};\\,{\\theta}') + \\gamma \\lambda_{adv} adv_{t+1} \n\\end{aligned}\n\\end{equation}\n$$\n```python\ndef get_advantages_and_returns(self, values, rewards, start):\n    lastgaelam = 0\n    advantages_reversed = []\n    length = rewards.size()[-1]\n    for t in reversed(range(start, length)): # 反向计算\n        nextvalues = values[:, t + 1] if t < length - 1 else 0.0\n        delta = rewards[:, t] + self.gamma * nextvalues - values[:, t]\n        lastgaelam = delta + self.gamma * self.lam * lastgaelam\n        advantages_reversed.append(lastgaelam)\n    advantages = torch.stack(advantages_reversed[::-1], dim=1) # 再反转\n    returns = advantages + values[:, start:] # adv(t) + value(t+1)更合理些\n    return advantages.detach(), returns\n```\n\n## Actor Model\nActor模型以SFT模型初始化，其损失函数设计与标准Actor-Critic有个不同点，$loss^{clip}$是PPO2策略，整体loss对原始PPO和PPO2进行了结合。\n另外，代码中没有直接计算两个概率值相除，而是使用对数指数变换，应该是数值稳定性考虑。\n\n$$\n\\begin{equation}\n\\label{actor_loss}\n\\begin{aligned}\nloss^{noClip}_{t} &= - \\frac{P(a_{t}|s_{t};\\,\\theta)}{P(a_{t}|s_{t};\\,{\\theta}')}) adv_{t} \n\\\\ &= - \\exp(\\log{P(a_{t}|s_{t};\\,\\theta)} - \\log{P(a_{t}|s_{t};\\,{\\theta}')}) adv_{t} \n\\\\\nloss^{clip}_{t} &= -\\min(\\max(\\frac{P(a_{t}|s_{t};\\,\\theta)}{P(a_{t}|s_{t};\\,{\\theta}')}, \\, 1 - pg^{upper}), \\, 1 + pg^{upper}) adv_{t}\n\\\\ &= -\\min(\\max(\\exp{\\log{P(a_{t}|s_{t};\\,\\theta)} - \\log{P(a_{t}|s_{t};\\,{\\theta}')}}, \\, 1 - pg_{upper}), \\, 1 + pg_{upper}) adv_{t}\n\\\\\nActorLoss &= \\mathbb{E}_{(s_t, a_t) \\sim P(a_{t}|s_{t};\\,{\\theta}')} [ {\\max(loss^{noClip}_{t}, \\, loss^{clip}_{t})} ] \n\\\\ &= \\frac{1}{NT} \\sum_{i=1}^{N} \\sum_{t=1}^{T} {\\max(loss^{noClip}_{i,t}, \\, loss^{clip}_{i,t})}\n\\end{aligned}\n\\end{equation}\n$$\n```python\ndef actor_loss_fn(self, logprobs, old_logprobs, advantages, mask):\n    log_ratio = (logprobs - old_logprobs) * mask\n    ratio = torch.exp(log_ratio)\n    pg_loss1 = -advantages * ratio\n    pg_loss2 = -advantages * torch.clamp(ratio, 1.0 - self.cliprange, 1.0 + self.cliprange)\n    pg_loss = torch.sum(torch.max(pg_loss1, pg_loss2) * mask) / mask.sum()\n    return pg_loss\n```\n\n## Critic Model\nCritic模型以Reward Model初始化，其损失函数为模型预估值$V(s_{t};\\,\\theta)$与回报$rn_{t}$的差别，为平方损失的回归任务。这里比较奇怪的一点是$rn_{t}$为何如此设计，感觉$adv_{t} + V(s_{t+1};\\,{\\theta}')$更合理一些。\n\n$$\n\\begin{equation}\n\\label{critic_loss}\n\\begin{aligned}\nrn_{t} &= adv_{t} + V(s_{t};\\,{\\theta}')\n\\\\\nloss^{noClip}_{t} &= \\frac{1}{2} (V(s_{t};\\,\\theta) - rn_{t})^2\n\\\\\nloss^{clip}_{t} &= \\frac{1}{2} (\\min(\\max(V(s_{t};\\,\\theta), \\, V(s_{t};\\,{\\theta}')-v^{upper}), \\, V(s_{t};\\,{\\theta}')+v^{upper}) - rn_{t})^2\n\\\\\nCriticLoss &= \\mathbb{E}_{(s_t, a_t) \\sim P(a_{t}|s_{t};\\,{\\theta}')} [ {\\max(loss^{noClip}_{t}, \\, loss^{clip}_{t})} ] \n\\\\ &= \\frac{1}{NT} \\sum_{i=1}^{N} \\sum_{t=1}^{T} {\\max(loss^{noClip}_{i,t}, \\, loss^{clip}_{i,t})}\n\\end{aligned}\n\\end{equation}\n$$\n```python\ndef critic_loss_fn(self, values, old_values, returns, mask):\n    values_clipped = torch.clamp(values, old_values - self.cliprange_value, old_values + self.cliprange_value)\n    vf_loss1 = (values - returns)**2\n    vf_loss2 = (values_clipped - returns)**2\n    vf_loss = 0.5 * torch.sum(torch.max(vf_loss1, vf_loss2) * mask) / mask.sum()\n    return vf_loss\n```\n\n# DeepSpeed-Chat强化学习训练逻辑\n训练逻辑是Off-Policy策略，外层循环读取prompt数据生成prompt+response数据放入样本池，内层循环从样本池中读取prompt+response数据进行Actor Model和Critic Model的训练。性能上，SFT模型放到CPU上，Actor模型通过DeepSpeed Hybrid Engine支持训练和推理两种模式的高效切换。\n另外，Instruct论文中在Actor Loss中增加了一个SFT Loss和一个Unsupervised Loss，两个Loss也加到之前的Actor Loss上。\n\n最终的Actor Loss为：\n\n$$\n\\begin{equation}\n\\label{actor_loss_2}\n\\begin{aligned}\nActorCombinedLoss &= \\mathbb{E}_{(s_t, a_t) \\in D_{RL}} [ \\max(loss^{noClip}_{t}, \\, loss^{clip}_{t}) + \\beta_1 \\log \\frac{P(a_{t}|s_{t};\\,\\theta)}{P(a_{t}|s_{t};\\,\\theta^{SFT})} ]\n\\\\ & \\quad - \\beta_2 \\mathbb{E}_{(s_t, a_t) \\in D_{Unsupervised}} \\log P(a_{t}|s_{t};\\,\\theta)\n\\end{aligned}\n\\end{equation}\n$$\n其中，SFT Loss部分保证和Actor模型和SFT模型偏离不远，Unsupervised Loss部分增加了一个自回归任务，整体Loss计算梯度做模型更新，而DeepSpeed-Chat只使用了Unsupervised，没有增加SFT部分（在reward计算时使用了，间接引入），并且先用Actor Loss更新，再用Unsupervised Loss更新。Actor模型参数都采用了Exponential Moving Averages策略。\n\n```python\ndef generate_experience(self, prompts, mask):\n    self.eval()\n    seq = self._generate_sequence(prompts, mask) # [batch_size, prompt_response_max_len]\n    self.train()\n\n    pad_token_id = self.tokenizer.pad_token_id\n    attention_mask = seq.not_equal(pad_token_id).long()\n\n    with torch.no_grad():\n        output = self.actor_model(seq, attention_mask=attention_mask)\n        output_ref = self.ref_model(seq, attention_mask=attention_mask)\n        reward_score = self.reward_model.forward_value(seq, attention_mask, prompt_length=self.prompt_length)['chosen_end_scores'].detach()\n        values = self.critic_model.forward_value(seq, attention_mask, return_value_only=True).detach()[:, :-1]\n\n    logits = output.logits\n    logits_ref = output_ref.logits\n\n    return {\n        'prompts': prompts, # [batch_size, prompt_max_len]\n        'logprobs': gather_log_probs(logits[:, :-1, :], seq[:, 1:]), # [batch_size, prompt_response_max_len]\n        'ref_logprobs': gather_log_probs(logits_ref[:, :-1, :], seq[:, 1:]), # [batch_size, prompt_response_max_len]\n        'value': values, # [batch_size, prompt_response_max_len]\n        'rewards': reward_score, # [batch_size, 1]\n        'input_ids': seq, # [batch_size, prompt_response_max_len]\n        \"attention_mask\": attention_mask # [batch_size, prompt_response_max_len]\n    }\n\ndef train_rlhf(self, inputs):\n    ### process the old outputs\n    prompts = inputs['prompts']\n    log_probs = inputs['logprobs']\n    ref_log_probs = inputs['ref_logprobs']\n    reward_score = inputs['rewards']\n    values = inputs['value']\n    attention_mask = inputs['attention_mask']\n    seq = inputs['input_ids']\n\n    start = prompts.size()[-1] - 1\n    action_mask = attention_mask[:, 1:]\n\n    old_values = values\n    with torch.no_grad():\n        old_rewards = self.compute_rewards(prompts, log_probs, ref_log_probs, reward_score, action_mask)\n        advantages, returns = self.get_advantages_and_returns(old_values, old_rewards, start)\n\n    ### process the new outputs\n    batch = {'input_ids': seq, \"attention_mask\": attention_mask}\n    actor_prob = self.actor_model(**batch, use_cache=False).logits\n    actor_log_prob = gather_log_probs(actor_prob[:, :-1, :], seq[:, 1:])\n    actor_loss = self.actor_loss_fn(actor_log_prob[:, start:], log_probs[:, start:], advantages, action_mask[:, start:])\n    self.actor_model.backward(actor_loss)\n    self.actor_model.step()\n    value = self.critic_model.forward_value(**batch, return_value_only=True, use_cache=False)[:, :-1]\n    critic_loss = self.critic_loss_fn(value[:, start:], old_values[:, start:], returns, action_mask[:, start:])\n    self.critic_model.backward(critic_loss)\n    self.critic_model.step()\n\n    return actor_loss, critic_loss\n```\n\n# 参考\n* [台大李宏毅强化学习课程](http://speech.ee.ntu.edu.tw/~tlkagk/courses_MLDS18.html)\n* [DeepSpeed-Chat](https://github.com/microsoft/DeepSpeedExamples/tree/master/applications/DeepSpeed-Chat)\n* [ColossalChat](https://github.com/hpcaitech/ColossalAI/tree/main/applications/Chat)\n* [TRLX](https://github.com/CarperAI/trlx/blob/main/trlx/models/modeling_ppo.py#L174)\n* [Training language models to follow instructions with human feedback](http://arxiv.org/abs/2203.02155)\n* [DeepSpeed-Chat全流程训练实战](https://mathmach.com/ff7e0546/)\n","tags":["ChatGPT","DeepSpeed","DeepSpeed-Chat","RLHF","PPO","Actor-Critic","强化学习"],"categories":["tech"]},{"title":"DeepSpeed-Chat全流程训练实战","url":"/ff7e0546/","content":"\n<!-- toc -->\n\n# 背景\nChatGPT出现后，已经有许多开源项目尝试复现其效果，包括LLaMa、DeepSpeed-Chat、ColossalChat、ChatGLM等。其中DeepSpeed-Chat是微软Deep Speed团队的开源项目，其完整的提供了Supervised Fine-tuning、Reward Model Training、RLHF PPO Traing三阶段的代码，逻辑简单，模块划分清晰，另外也由于Deep Speed在大模型训练中的使用非常普遍，所以笔者近期正在研究DeepSpeed-Chat的代码。本文介绍下在13b模型上运行SFT、RW、RLHF全部三阶段的实战情况。\n\n# 运行条件准备\n\n## 运行环境\n\n```\nOS: CentOS 7\nGPU: A100 80G\nCuda: 11.0\nPython: 3.9.6\nConda: 4.10.3\nGCC: 7.3.1\n```\n\n安装依赖：\n```bash\npip install datasets\npip install sentencepiece\npip install protobuf\npip install accelerate\npip install torch\n\n# pip最新版0.9.5有bug，所以用源码安装\ngit clone https://github.com/microsoft/DeepSpeed.git\ncd DeepSpeed\npip install -e .\n\n# pip最新包可能不支持某些模型，所以用源码安装\ngit clone https://github.com/huggingface/transformers.git\ncd transformers\npip install -e .\n```\n\n## 数据和模型\n由于公司内GPU集群的机器不允许连接外网，因此先在本地将模型和数据准备好后再传到GPU机器。\n参考文档：[transformers offline mode](https://huggingface.co/docs/transformers/installation?highlight=transformers_cache#offline-mode)。\n开启VPN，按下面代码从Huggingface下载对应模型、词典和数据，完成后将`model_output_dir`和`dataset_output_dir`目录传到GPU机器。\n\n### 模型下载\n如果`AutoModel.from_pretrained(model_name)`内存不足，可以直接从下载缓存`cache_dir/model_name/snapshot`目录拉取模型。`cache_dir`的具体配置可以参考[transformers cache](https://huggingface.co/docs/transformers/installation?highlight=transformers_cache#cache-setup)。\n\n下载大模型，用于训练SFT Model。\n```python\nfrom transformers import AutoModel, AutoTokenizer\n\nmodel_name = 'facebook/opt-13b' # change this to the model you need\nmodel_output_dir = 'your_dir/facebook_opt_13b'\nmodel = AutoModel.from_pretrained(model_name)\nmodel.save_pretrained(model_output_dir)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\ntokenizer.save_pretrained(model_output_dir)\n```\n\n下载完成后，目录下包含了model和tokenizer的具体数据。\n<div style=\"text-align: center\">\n<img src=\"/images/facebook_opt_13b_files.png\" alt=\"图片替换文本\" width=\"600\" height=\"280\" align=\"middle\" />\n</div>\n\n其中`config.json`为模型配置，内容为：\n```\n{\n  \"_name_or_path\": \"facebook/opt-13b\",\n  \"_remove_final_layer_norm\": false,\n  \"activation_dropout\": 0.0,\n  \"activation_function\": \"relu\",\n  \"architectures\": [\n    \"OPTForCausalLM\"\n  ],\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 2,\n  \"do_layer_norm_before\": true,\n  \"dropout\": 0.1,\n  \"eos_token_id\": 2,\n  \"ffn_dim\": 20480,\n  \"hidden_size\": 5120,\n  \"init_std\": 0.02,\n  \"layerdrop\": 0.0,\n  \"max_position_embeddings\": 2048,\n  \"model_type\": \"opt\",\n  \"num_attention_heads\": 40,\n  \"num_hidden_layers\": 40,\n  \"output_projection\": true,\n  \"pad_token_id\": 1,\n  \"prefix\": \"</s>\",\n  \"torch_dtype\": \"float16\",\n  \"transformers_version\": \"4.21.0.dev0\",\n  \"use_cache\": true,\n  \"vocab_size\": 50272,\n  \"word_embed_proj_dim\": 5120\n}\n```\n\n下载小规模模型，用于训练Reward Model。\n```python\nfrom transformers import AutoModel, AutoTokenizer\n\nmodel_name = 'facebook/opt-350m' # change this to the model you need\nmodel_output_dir = 'your_dir/facebook_opt_350m'\nmodel = AutoModel.from_pretrained(model_name)\nmodel.save_pretrained(model_output_dir)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\ntokenizer.save_pretrained(model_output_dir)\n```\n\n### 数据下载\n以`Dahoas/rm-static`数据为例，可自行下载其他数据。\n```python\nimport datasets\n\ndataset_name = 'Dahoas/rm-static' # change this to the dataset you need\ndataset_output_dir = 'your_dir/dahoas_rm_static'\ndataset = datasets.load_dataset(dataset_name)\ndataset.save_to_disk(dataset_output_dir)\n```\n\n# Step 1: SFT训练\n\n## tokenizer\n因为不能联网，所以改成强制使用本地文件，修改以下代码。\n\n`DeepSpeedExamples/applications/DeepSpeed-Chat/training/utils/utils.py`：\n```python\n def load_hf_tokenizer(model_name_or_path, fast_tokenizer=True):\n    #if os.path.exists(model_name_or_path):\n    #    # Locally tokenizer loading has some issue, so we need to force download\n    #    model_json = os.path.join(model_name_or_path, \"config.json\")\n    #    if os.path.exists(model_json):\n    #        model_json_file = json.load(open(model_json))\n    #        model_name = model_json_file[\"_name_or_path\"]\n    #        tokenizer = AutoTokenizer.from_pretrained(model_name,\n    #                                                  fast_tokenizer=True)\n    #else:\n    tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, fast_tokenizer=True)\n    return tokenizer\n```\n\n`DeepSpeedExamples/applications/DeepSpeed-Chat/training/step1_supervised_finetuning/main.py`：\n```python\n#tokenizer = load_hf_tokenizer(args.model_name_or_path, fast_tokenizer=True)\ntokenizer = load_hf_tokenizer('your_dir/facebook_opt_13b', fast_tokenizer=True)\n```\n\n## 训练数据读取\n训练数据也使用缓存，修改文件`DeepSpeedExamples/applications/DeepSpeed-Chat/training/utils/data/raw_datasets.py`：\n```python\nclass PromptRawDataset(object):\n    def __init__(self, output_path, seed, local_rank, dataset_name):\n        self.output_path = output_path\n        self.seed = seed\n        self.local_rank = local_rank\n        if not dataset_name == 'local/jsonfile':\n            #self.raw_datasets = load_dataset(dataset_name) # 即使dataset_name是本地目录，也会先联网，可以设置export HF_DATASETS_OFFLINE=1或换用load_from_disk\n            self.raw_datasets = datasets.load_from_disk(dataset_name)\n```\n\n## 任务启动脚本\n修改文件`DeepSpeedExamples/applications/DeepSpeed-Chat/training/step1_supervised_finetuning/training_scripts/single_node/run_13b.sh`：\n```bash\ndeepspeed main.py \\\n   --data_path your_dir/dahoas_rm_static \\\n   --data_split 2,4,4 \\\n   --model_name_or_path your_dir/facebook_opt_13b \\\n   --per_device_train_batch_size 4 \\\n   --per_device_eval_batch_size 4 \\\n   --max_seq_len 512 \\\n   --learning_rate 1e-4 \\\n   --weight_decay 0. \\\n   --num_train_epochs 16  \\\n   --gradient_accumulation_steps 1 \\\n   --lr_scheduler_type cosine \\\n   --num_warmup_steps 0 \\\n   --seed 1234 \\\n   --gradient_checkpointing \\\n   --zero_stage $ZERO_STAGE \\\n   --lora_dim 128 \\\n   --lora_module_name decoder.layers. \\\n   --deepspeed \\\n   --output_dir $OUTPUT \\\n   &> $OUTPUT/training.log\n```\n\n## 启动训练\n```bash\ncd DeepSpeedExamples/applications/DeepSpeed-Chat\npython train.py --step 1 --actor-model 13b --deployment-type single_node\n```\n没问题的话就可以在`DeepSpeed-Chat/output/actor-models/13b/training.log`看到训练情况了：\n```\n229 ***** Running training *****\n230 ***** Evaluating perplexity, Epoch 0/16 *****\n231 ppl: 2771.550537109375\n232 Beginning of Epoch 1/16, Total Micro Batches 3813\n233 [2023-07-07 15:34:07,295] [INFO] [loss_scaler.py:188:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis     to 1\n234 Invalidate trace cache @ step 0: expected module 13, but got module 0\n235 [2023-07-07 15:34:08,960] [INFO] [loss_scaler.py:181:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n236 [2023-07-07 15:34:10,421] [INFO] [loss_scaler.py:181:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n237 [2023-07-07 15:34:12,200] [INFO] [loss_scaler.py:181:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n238 [2023-07-07 15:34:13,468] [INFO] [loss_scaler.py:181:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, reducing to 4096\n239 [2023-07-07 15:34:14,738] [INFO] [loss_scaler.py:181:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 4096, reducing to 2048\n240 [2023-07-07 15:34:17,337] [INFO] [loss_scaler.py:181:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 2048, reducing to 1024\n241 [2023-07-07 15:34:19,964] [INFO] [logging.py:96:log_dist] [Rank 0] step=10, skipped=7, lr=[9.999999940336507e-05, 9.999999940336507e-05], mom=[(0.9, 0.95), (0.9, 0.95)]\n242 [2023-07-07 15:34:19,964] [INFO] [timer.py:199:stop] epoch=0/micro_step=10/global_step=10, RunningAvgSamplesPerSec=2.9106454245373556, CurrSamplesPerSec=3.07555254100577, MemAllocat    ed=36.12GB, MaxMemAllocated=40.26GB\n243 [2023-07-07 15:34:32,952] [INFO] [logging.py:96:log_dist] [Rank 0] step=20, skipped=7, lr=[9.999998879652199e-05, 9.999998879652199e-05], mom=[(0.9, 0.95), (0.9, 0.95)]\n244 [2023-07-07 15:34:32,952] [INFO] [timer.py:199:stop] epoch=0/micro_step=20/global_step=20, RunningAvgSamplesPerSec=3.003776798900565, CurrSamplesPerSec=3.0893153637186064, MemAlloca    ted=36.12GB, MaxMemAllocated=40.26GB\n245 [2023-07-07 15:34:45,914] [INFO] [logging.py:96:log_dist] [Rank 0] step=30, skipped=7, lr=[9.99999649311278e-05, 9.99999649311278e-05], mom=[(0.9, 0.95), (0.9, 0.95)]\n246 [2023-07-07 15:34:45,914] [INFO] [timer.py:199:stop] epoch=0/micro_step=30/global_step=30, RunningAvgSamplesPerSec=3.0336418997022405, CurrSamplesPerSec=3.0865395529882864, MemAlloc    ated=36.12GB, MaxMemAllocated=40.26GB\n247 [2023-07-07 15:34:58,905] [INFO] [logging.py:96:log_dist] [Rank 0] step=40, skipped=7, lr=[9.999992780718883e-05, 9.999992780718883e-05], mom=[(0.9, 0.95), (0.9, 0.95)]\n248 [2023-07-07 15:34:58,906] [INFO] [timer.py:199:stop] epoch=0/micro_step=40/global_step=40, RunningAvgSamplesPerSec=3.0462048161750825, CurrSamplesPerSec=3.0772742271467597, MemAlloc    ated=36.12GB, MaxMemAllocated=40.26GB\n249 [2023-07-07 15:35:11,897] [INFO] [logging.py:96:log_dist] [Rank 0] step=50, skipped=7, lr=[9.999987742471495e-05, 9.999987742471495e-05], mom=[(0.9, 0.95), (0.9, 0.95)]\n250 [2023-07-07 15:35:11,898] [INFO] [timer.py:199:stop] epoch=0/micro_step=50/global_step=50, RunningAvgSamplesPerSec=3.0535265510887886, CurrSamplesPerSec=3.0858537581286103, MemAlloc    ated=36.12GB, MaxMemAllocated=40.26GB\n251 [2023-07-07 15:35:24,885] [INFO] [logging.py:96:log_dist] [Rank 0] step=60, skipped=7, lr=[9.999981378371948e-05, 9.999981378371948e-05], mom=[(0.9, 0.95), (0.9, 0.95)]\n252 [2023-07-07 15:35:24,886] [INFO] [timer.py:199:stop] epoch=0/micro_step=60/global_step=60, RunningAvgSamplesPerSec=3.058528008594015, CurrSamplesPerSec=3.089157229026472, MemAllocat    ed=36.12GB, MaxMemAllocated=40.26GB\n253 [2023-07-07 15:35:37,859] [INFO] [logging.py:96:log_dist] [Rank 0] step=70, skipped=7, lr=[9.999973688421931e-05, 9.999973688421931e-05], mom=[(0.9, 0.95), (0.9, 0.95)]\n254 [2023-07-07 15:35:37,860] [INFO] [timer.py:199:stop] epoch=0/micro_step=70/global_step=70, RunningAvgSamplesPerSec=3.062547383419453, CurrSamplesPerSec=3.0862085394366274, MemAlloca    ted=36.12GB, MaxMemAllocated=40.26GB\n255 [2023-07-07 15:35:50,853] [INFO] [logging.py:96:log_dist] [Rank 0] step=80, skipped=7, lr=[9.999964672623485e-05, 9.999964672623485e-05], mom=[(0.9, 0.95), (0.9, 0.95)]\n256 [2023-07-07 15:35:50,854] [INFO] [timer.py:199:stop] epoch=0/micro_step=80/global_step=80, RunningAvgSamplesPerSec=3.0649199847585775, CurrSamplesPerSec=3.081825198818058, MemAlloca    ted=36.12GB, MaxMemAllocated=40.26GB\n```\n\n每轮评估指标的变化情况如下，可以看到开始几轮效果反而下降，后面几轮又开始提升。\n```\nppl: 2771.550537109375\nppl: 2.4410853385925293\nppl: 2.680394172668457\nppl: 2.779381036758423\nppl: 2.80298113822937\nppl: 2.813119888305664\nppl: 2.8290116786956787\nppl: 2.833710193634033\nppl: 2.832332134246826\nppl: 2.8273046016693115\nppl: 2.8215107917785645\nppl: 2.8162872791290283\nppl: 2.808527946472168\nppl: 2.792924165725708\nppl: 2.775484561920166\nppl: 2.755317449569702\nppl: 2.74511981010437\n```\n\n大概1小时24分钟训练一轮，GPU存储占用47G，其中模型参数占用24G。\n根据[ChatGPT量化分析（二） - 存储占用分析](https://mathmach.com/4e7f3af5/)，中间激活存储占用：\n$$\n\\begin{equation}\n\\label{transformer_blocks_memory}\n\\begin{aligned}\nn_{transformer \\, multi \\, block \\, memory} = n_{batch} n_{sequence} ( 8 n_{heads} d_{head} n_{layers} + 2 n_{heads} n_{sequence} n_{layers} + 10 d_{model} n_{layers} + 6 d_{ffn} n_{layers})\n\\end{aligned}\n\\end{equation}\n$$\n代入参数$n_{batch}=4,\\,n_{sequence}=512,\\,n_{heads}=40,\\,n_{layers}=40,\\,d_{head}=\\frac{d_{model}}{n_{heads}},\\,d_{model}=5120,\\,d_{ffn}=20480$，可得中间激活占用19G。如果全量微调中间激活的梯度也占用19G，但这里使用了Lora，可训练参数较少，并且开启了ZeRO-Offload，将优化器状态存储放到了CPU。忽略掉这些参数的话，模型参数24G加上中间激活19G共43G，与实际的47G差距不大，符合预期。\n<div style=\"text-align: center\">\n<img src=\"/images/deep_speed_chat_step1_gpu_usage.png\" alt=\"图片替换文本\" width=\"600\" height=\"350\" align=\"middle\" />\n</div>\n\n# Step 2: Reward Model训练\n\n## tokenizer\n`DeepSpeedExamples/applications/DeepSpeed-Chat/training/step2_reward_model_finetuning/main.py`：\n```python\n#tokenizer = load_hf_tokenizer(args.model_name_or_path, fast_tokenizer=True)\ntokenizer = load_hf_tokenizer('your_dir/facebook_opt_350m', fast_tokenizer=True)\n```\n\n## 训练数据读取\n训练数据与SFT阶段一样，所以这里没有修改。\n\n## 任务启动脚本\n修改文件`DeepSpeedExamples/applications/DeepSpeed-Chat/training/step2_reward_model_finetuning/training_scripts/single_node/run_350m.sh`：\n```bash\ndeepspeed main.py \\\n   --data_path your_dir/dahoas_rm_static \\\n   --data_split 2,4,4 \\\n   --model_name_or_path your_dir/facebook_opt_350m \\\n   --num_padding_at_beginning 1 \\\n   --per_device_train_batch_size 4 \\\n   --per_device_eval_batch_size 4 \\\n   --max_seq_len 512 \\\n   --learning_rate 5e-5 \\\n   --weight_decay 0.1 \\\n   --num_train_epochs 1 \\\n   --disable_dropout \\\n   --gradient_accumulation_steps 1 \\\n   --lr_scheduler_type cosine \\\n   --num_warmup_steps 0 \\\n   --seed 1234 \\\n   --zero_stage $ZERO_STAGE \\\n   --deepspeed \\\n   --output_dir $OUTPUT \\\n   &> $OUTPUT/training.log\n```\n\n## 启动训练\n```bash\ncd DeepSpeedExamples/applications/DeepSpeed-Chat\npython train.py --step 2 --reward-model 350m --deployment-type single_node\n```\n没问题的话就可以在`DeepSpeed-Chat/output/reward-models/350m/training.log`看到训练情况了。每轮评估指标的变化情况：\n```\nchosen_last_scores (higher is better) : 2.7135448455810547, acc (higher is better) : 0.4949999749660492\nchosen_last_scores (higher is better) : -8.86074161529541, acc (higher is better) : 0.5600000023841858\n```\nGPU占用1.2G。\n\n# Step 3: RLHF训练\n\n## tokenizer\nDeepSpeed-Chat代码里actor和critic模型用的是同一个tokenizer，因为opt-13b和opt-350m的词典一样所以只加载一个tokenizer不会报错，如果不一致的话需要修改代码，让它们用各自的tokenizer。\n\n## 训练数据读取\n训练数据与SFT、RW阶段一样，所以这里没有修改。\n\n## 任务启动脚本\n目前DeepSpeed Hybrid Engine会报以下错，所以先关掉`--enable_hybrid_engine`。关掉后则会CUDA out of memory，所以再加上`--offload_reference_model`。\n```\n232   File \"/home/formath/DeepSpeedExamples/applications/DeepSpeed-Chat/training/step3_rlhf_finetuning/rlhf_engine.py\", line 119, in _init_actor\n233     actor_engine, *_ = deepspeed.initialize(model=actor_model,\n234   File \"/conda/envs/py39/lib/python3.9/site-packages/deepspeed/__init__.py\", line 153, in initialize\n235     engine = DeepSpeedHybridEngine(args=args,\n236   File \"/conda/envs/py39/lib/python3.9/site-packages/deepspeed/runtime/hybrid_engine.py\", line 52, in __init__\n237     self.create_inference_module()\n238   File \"/conda/envs/py39/lib/python3.9/site-packages/deepspeed/runtime/hybrid_engine.py\", line 359, in create_inference_module\n239     self.create_inference_containers(self.module)\n240   File \"/conda/envs/py39/lib/python3.9/site-packages/deepspeed/runtime/hybrid_engine.py\", line 308, in create_inference_containers\n241     self.create_inference_containers(child, layer_id=layer_id)\n242   File \"/conda/envs/py39/lib/python3.9/site-packages/deepspeed/runtime/hybrid_engine.py\", line 308, in create_inference_containers\n243     self.create_inference_containers(child, layer_id=layer_id)\n244   File \"/conda/envs/py39/lib/python3.9/site-packages/deepspeed/runtime/hybrid_engine.py\", line 308, in create_inference_containers\n245     self.create_inference_containers(child, layer_id=layer_id)\n246   File \"/conda/envs/py39/lib/python3.9/site-packages/deepspeed/runtime/hybrid_engine.py\", line 288, in create_inference_containers\n247     self._inference_containers.append(self.inference_policies[child.__class__][0](\n248   File \"/conda/envs/py39/lib/python3.9/site-packages/deepspeed/runtime/hybrid_engine.py\", line 107, in new_inference_container\n249     _container.set_tensor_parallel_config(self._config.hybrid_engine.inference_tp_size, self.mp_group)\n250   File \"/conda/envs/py39/lib/python3.9/site-packages/deepspeed/runtime/engine.py\", line 460, in __getattr__\n251     raise AttributeError(f\"'{type(self).__name__}' object has no attribute '{name}'\")\n252 AttributeError: 'DeepSpeedHybridEngine' object has no attribute 'mp_group'\n```\n\n修改文件`DeepSpeedExamples/applications/DeepSpeed-Chat/training/step3_rlhf_finetuning/training_scripts/single_node/run_13b.sh`：\n```bash\ndeepspeed --master_port 12346 main.py \\\n   --data_path your_dir/dahoas_rm_static \\\n   --data_split 2,4,4 \\\n   --actor_model_name_or_path $ACTOR_MODEL_PATH \\\n   --critic_model_name_or_path $CRITIC_MODEL_PATH \\\n   --num_padding_at_beginning 1 \\\n   --per_device_train_batch_size 16 \\\n   --per_device_mini_train_batch_size 16 \\\n   --generation_batch_numbers 1 \\\n   --ppo_epochs 1 \\\n   --max_answer_seq_len 256 \\\n   --max_prompt_seq_len 256 \\\n   --actor_learning_rate ${Actor_Lr} \\\n   --critic_learning_rate ${Critic_Lr} \\\n   --num_train_epochs 1 \\\n   --lr_scheduler_type cosine \\\n   --gradient_accumulation_steps 1 \\\n   --num_warmup_steps 100 \\\n   --deepspeed --seed 1234 \\\n   --offload_reference_model \\\n   --inference_tp_size 2 \\\n   --actor_zero_stage $ACTOR_ZERO_STAGE \\\n   --critic_zero_stage $CRITIC_ZERO_STAGE \\\n   --actor_gradient_checkpointing \\\n   --disable_actor_dropout \\\n   --actor_lora_dim 128 \\\n   --actor_lora_module_name decoder.layers. \\\n   --output_dir $OUTPUT \\\n    &> $OUTPUT/training.log\n```\n\n## 启动训练\n```bash\ncd DeepSpeedExamples/applications/DeepSpeed-Chat\npython train.py --step 3 --actor-model 13b --reward-model 350m --deployment-type single_node\n```\n没问题的话就可以在`DeepSpeed-Chat/step3-models/13b/training.log`看到训练情况了。\n```\n723 ***** Running training *****\n724 Beginning of Epoch 1/1, Total Generation Batches 1907\n725 [2023-07-12 14:56:26,391] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis     to 1\n726 [2023-07-12 14:56:26,872] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis     to 1\n727 epoch: 0|step: 0|ppo_ep: 1|act_loss: -0.341552734375|cri_loss: 0.1712646484375|unsuper_loss: 0.0\n728 average reward score: -7.6640625\n729 -------------------------------------------------------------------------------------\n730 [2023-07-12 14:57:59,326] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n731 [2023-07-12 14:58:00,227] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n732 epoch: 0|step: 1|ppo_ep: 1|act_loss: -0.386474609375|cri_loss: 0.19775390625|unsuper_loss: 0.0\n733 average reward score: -7.78125\n734 -------------------------------------------------------------------------------------\n735 [2023-07-12 14:59:19,445] [WARNING] [stage3.py:1898:step] 5 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to pe    rformance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelera    tor().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n736 [2023-07-12 14:59:19,709] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n737 epoch: 0|step: 2|ppo_ep: 1|act_loss: -0.35400390625|cri_loss: 0.1790771484375|unsuper_loss: 0.0\n738 average reward score: -7.640625\n739 -------------------------------------------------------------------------------------\n740 [2023-07-12 15:00:39,293] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n741 epoch: 0|step: 3|ppo_ep: 1|act_loss: -0.31884765625|cri_loss: 0.1573486328125|unsuper_loss: 0.0\n742 average reward score: -7.61328125\n743 -------------------------------------------------------------------------------------\n744 [2023-07-12 15:01:58,839] [WARNING] [stage3.py:1898:step] 5 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to pe    rformance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelera    tor().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n745 epoch: 0|step: 4|ppo_ep: 1|act_loss: -0.36962890625|cri_loss: 0.18896484375|unsuper_loss: 0.0\n746 average reward score: -7.78125\n747 -------------------------------------------------------------------------------------\n748 epoch: 0|step: 5|ppo_ep: 1|act_loss: -0.32373046875|cri_loss: 0.1724853515625|unsuper_loss: 0.0\n749 average reward score: -7.7109375\n750 -------------------------------------------------------------------------------------\n751 epoch: 0|step: 6|ppo_ep: 1|act_loss: -0.327392578125|cri_loss: 0.415283203125|unsuper_loss: 0.0\n752 average reward score: -8.3125\n753 -------------------------------------------------------------------------------------\n754 epoch: 0|step: 7|ppo_ep: 1|act_loss: -0.368408203125|cri_loss: 0.208740234375|unsuper_loss: 0.0\n755 average reward score: -7.84765625\n756 -------------------------------------------------------------------------------------\n757 epoch: 0|step: 8|ppo_ep: 1|act_loss: -0.340087890625|cri_loss: 0.1898193359375|unsuper_loss: 0.0\n758 average reward score: -7.76953125\n759 -------------------------------------------------------------------------------------\n```\n\nGPU内存占用77G，已经接近单卡极限，但利用率只有18%。\n<div style=\"text-align: center\">\n<img src=\"/images/deep_speed_chat_step3_gpu_usage.png\" alt=\"图片替换文本\" width=\"600\" height=\"350\" align=\"middle\" />\n</div>\n\n# 对话测试\n以上训练结束后，就可以导入各阶段的模型进行对话了，比如用step 3保存的模型进行对话。\n```bash\ncd DeepSpeedExamples/applications/DeepSpeed-Chat\npython chat.py --path /home/formath/DeepSpeedExamples/applications/DeepSpeed-Chat/output/step3-models/13b/actor --max_new_tokens 512\n```\n\n# 参考\n* [DeepSpeed-Chat](https://github.com/microsoft/DeepSpeedExamples/tree/master/applications/DeepSpeed-Chat)\n* [ColossalChat](https://github.com/hpcaitech/ColossalAI/tree/main/applications/Chat)\n* [ChatGPT量化分析（二） - 存储占用分析](https://mathmach.com/4e7f3af5/)\n* [ZeRO-Offload](https://www.deepspeed.ai/tutorials/zero-offload/)\n","tags":["ChatGPT","DeepSpeed","DeepSpeed-Chat"],"categories":["tech"]},{"title":"模型召回如何打压热门内容","url":"/a4afa06d/","content":"\n<!-- toc -->\n\n# 背景\n理论上的召回模型，是根据概率分$p(user,\\,item)$，从全部候选item中选出分数最高的top K个item作为召回结果。\n$$\n\\begin{equation}\n\\label{softmax}\n\\begin{aligned}\np(user,\\,item) = \\frac{exp^{u(user,\\,item;\\,\\theta)}}{Z(\\theta)}\n\\end{aligned}\n\\end{equation}\n$$\n其中，$\\theta$是模型参数，$Z(\\theta)=\\sum_{j=0}^{N}{u(user,\\,item_j;\\,\\theta)}$是为了概率归一化。\n\n这里的$u(user,\\,item;\\,\\theta)$根据业务目标不同可以调整，搜索场景可以是query和item的相关性，推荐场景可以是用户历史或实时偏好和item的相关性，广告场景可以是用户和item的eCPM，当然也可以建模多目标，比如同时考虑相关性和eCPM。\n\n概率分$p(user,\\,item)$和$u(user,\\,item;\\,\\theta)$是正相关关系，所以只用根据$u(user,\\,item;\\,\\theta)$打分选出top K即可。线上召回时由于耗时原因不可能遍历全部候选item计算$u(user,\\,item;\\,\\theta)$，因此$u(user,\\,item;\\,\\theta)$往往是双塔模型，既$u(user,\\,item;\\,\\theta)=score(emb(user;\\theta),\\,emb(item;\\theta))$，$score$是$cos$或内积等度量，候选item的向量可以离线计算好，线上user的向量实时生成，借助Faiss等向量检索工具进行高效的top K近似检索。\n\n离线训练时，正样本往往是用户历史正反馈数据，比如点击、转化、收藏、转发。而$Z(\\theta)$难以计算，因此采用Noise Contrastive Estimation（NCE）、Negative Sampling（NEG）、Sampled Softmax（SSM）等方法近似估计，这些方法都使用了负采样。大规模Softmax近似估计方法在之前的几篇博客中已经介绍，可以参考：\n\n* [Softmax近似方法（一） - Noise Contrastive Estimation理论详解](https://mathmach.com/b5ec3256/)\n* [Softmax近似方法（二） - Sampled Softmax理论详解](https://mathmach.com/b5ec3256/)\n* [Softmax近似方法（三） - NCE、NEG、Sampled Softmax对比](https://mathmach.com/a817e3bf/)\n\n为了离线训练时的样本空间尽量和线上对齐，负采样要保证覆盖到全部item。另外，正样本中热门item的占比往往非常大，为了降低整体训练loss，最终得到的用户向量和热门item的向量往往分数更高，这样就导致线上召回时top K也被热门item占领，和业务目标偏离。\n\n# 热门item的定义\n\n<div style=\"text-align: center\">\n<img src=\"/images/hot_item_formulation.png\" alt=\"图片替换文本\" width=\"600\" height=\"500\" align=\"middle\" />\n</div>\n用户喜欢的item作为可召回空间，又可分为热门的大众偏好和非热门的个性化偏好，所以热门和个性化是互斥的两个空间。在召回建模中，建模目标是喜欢与否。如背景介绍中描述，如果不进行热门item打压，可召回空间往往被大众偏好所占领，导致个性化空间不足，因此热门item打压的目标是在大众偏好和个性化偏好中寻找平衡。\n<div style=\"text-align: center\">\n<img src=\"/images/hot_item_suppress.png\" alt=\"图片替换文本\" width=\"450\" height=\"300\" align=\"middle\" />\n</div>\n那么，平衡又怎么定义好坏呢？在不打压热门item的时候，热门item占比80%，个性化item只占20%。打压之后，让热门item占比20%，个性化item占比80%算好吗，甚至个性化占100%可行吗？笔者认为可以。原因是召回往往是多通路召回，模型召回只是其中一路。热门item通过简单的统计即可实现召回，无需借助模型，并且辅助用户标签就可从热门item中过滤出用户喜欢的。因此，大众偏好这部分通过简单的召回策略即可实现，而复杂的模型召回可以更关注个性化偏好这部分。\n当然，业务场景间差别非常大，笔者的观点在一些场景可能并不适用，仅供参考。\n\n\n# 热门item打压\n\n## 数据角度\n\n### 通过正样本打压\n正样本往往是用户历史正反馈数据，比如点击、转化、收藏、转发，这种样本生成方法自带偏向热门属性，因此直觉的办法就是降低正样本中的热门item样本的比例。但是，正样本又是稀缺的，降低热门item样本量的方法一定程度上损害了数据多样性，需要谨慎使用。\n\n### 通过负样本打压\n增加热门item在负样本的比例，可以使用In-Batch负采样。\n也可以根据热度做全局负采样，热度越高被采中概率越大，这样就可以一定程度上对冲掉正样本中的热门item，在一些情况下热门item可能负采样太多导致打压过度（如背景所述笔者认为在多通路召回下问题不大）。比如根据item的点击次数做负采样。\n$$\n\\begin{equation}\n\\label{q}\n\\begin{aligned}\nq(item_i) = \\frac{ClickCount_{item_i}^{\\alpha}}{\\sum_{j=0}^{N}{ClickCount_{item_j}}^{\\alpha}}\n\\end{aligned}\n\\end{equation}\n$$\n其中$q(item)$为负采样概率，$\\alpha \\in \\left[ 0, \\, 1 \\right]$为平衡负采样集中度和打压力度的配置。当$\\alpha=0$时为均匀采样，覆盖广度最高但无负样本打压作用，当$\\alpha=1$时覆盖广度降低同时负样本打压力度最强。\n\n\n## 模型角度\n如果采用NCE或SSM方法，在打分公式中引入了负样本概率消偏，新的打分公式为：\n$$\n\\begin{equation}\n\\label{u_new}\n\\begin{aligned}\n\\widehat{u}(user,\\,item;\\,\\theta)=u(user,\\,item;\\,\\theta) - \\log q(item)\n\\end{aligned}\n\\end{equation}\n$$\n在NCE数学原理的文章[<sup>1</sup>](#refer-anchor-1)中解释过，$\\log q(item)$就是item被负采样到的概率。考虑$q(item)$概率值和热门程度正相关的情况下，由于$\\log q(item) \\in \\left( -\\infty ,\\,0 \\right]$，item越冷门，采样概率越小，分数补偿的越多，相当于变相打压了热门item。\n\n注意这里只是在训练时打压了热门item，理论上$\\widehat{u}(user,\\,item;\\,\\theta)$只在离线训练时采用，线上检索时仍然使用未修正的打分$u(user,\\,item;\\,\\theta)$，而$u(user,\\,item;\\,\\theta)$丢掉了打压项。笔者认为在线上检索时也可以使用$\\widehat{u}(user,\\,item;\\,\\theta)$，它可以在检索时继续通过$\\log q(item)$打压热门item，并且$q(item)$也不一定要与离线时使用的一致，可以在线实时调控。\n怎样在线上检索时将$\\log q(item)$融入$score(emb(user;\\theta),\\,emb(item;\\theta))$中呢？如果$score$是内积，可以用$score(concat(emb(user;\\theta), \\, [-1]),\\, concat(emb(item;\\theta),\\, [\\log q(item)]))$，如果是$cos$，emb先归一化再concat，都只需要在向量中增加一列即可。\n如果使用NEG方法，它忽略了负采样概率修正项$\\log q(item)$，因此在训练和检索阶段都没有改变热门item问题，需要在其他层面解决。\n\n## 特征角度\n可以观察高分热门item是由哪些特征导致分数高，这样就可以将这些特征替换成非热门item的特征，实现分数打压。不过模型很黑盒，并且特征是异构的、离散的、数量又非常多，定位和替换特征有难度，不一定有可行性。另外强制替换特征会改变打分分布，导致模型评估指标和离线不一致，所以这种方法不太建议使用。\n\n## 业务角度\n从召回本身来看，往往是多通路召回，有些通路关注热门，有些通路关注个性化，这里要避免的就是所有通路都偏向热门。通路间有quota分配，可以人工干预热门通路的占比，甚至根据大盘业务指标实时自动调配。\n\n从整个业务层面来看，召回只是最上游，下游还有粗排精排甚至重排，在排序环节也需要考虑多样性问题，另外也需要有item粒度实时提权或打压的运营干预手段。\n\n# 参考资料\n<div id=\"refer-anchor-1\"></div>\n- [1][Softmax近似方法（一） - Noise Contrastive Estimation理论详解](https://mathmach.com/b5ec3256/)\n<div id=\"refer-anchor-2\"></div>\n- [2][Softmax近似方法（二） - Sampled Softmax理论详解](https://mathmach.com/b5ec3256/)\n<div id=\"refer-anchor-3\"></div>\n- [3][Softmax近似方法（三） - NCE、NEG、Sampled Softmax对比](https://mathmach.com/a817e3bf/)\n<div id=\"refer-anchor-4\"></div>\n- [4][推荐系统传统召回是怎么实现热门item的打压?](https://www.zhihu.com/question/426543628/)\n","tags":["搜索引擎","推荐系统","计算广告"],"categories":["tech"]},{"title":"ChatGPT量化分析（三） - 计算量分析","url":"/b3deb998/","content":"\n<!-- toc -->\n\n# 背景\nChatGPT出现后，惊人的效果完全颠覆了业界人员包括笔者的认知，抛开其模型细节层面的因素，已公开的训练方法，需要巨量的数据和计算资源，门槛非常高。本文基于公开资料，希望以量化方式分多篇介绍ChatGPT的分析结论，具体内容包含以下三篇，本文为计算量分析篇。\n\n* [ChatGPT量化分析（一） - 模型参数分析](https://mathmach.com/2a29fbbd/)\n* [ChatGPT量化分析（二） - 存储占用分析](https://mathmach.com/4e7f3af5/)\n* [ChatGPT量化分析（三） - 计算量分析](https://mathmach.com/b3deb998/)\n\nChatGPT模型结构为Transformer，下面对计算细节拆解分析。\n\n# 计算量分析\n## 变量定义\n$$\n\\begin{equation}\n\\begin{aligned}\n& n_{params}: \\quad 模型参数数量\n\\\\ & n_{flops}: \\quad 浮点运算次数，用来表示计算量\n\\end{aligned}\n\\end{equation}\n$$\n\n## Embedding\nembedding层是lookup操作，输入是词序列，输出是词序列embedding，浮点操作可忽略。\n\n## Transformer Blocks\ntransformer block的计算图如下，每个transformer block主要包含四部分，既multi-head attention和mlp，以及两个add&norm。\n<div style=\"text-align: center\">\n<img src=\"/images/transformer_block.png\" alt=\"图片替换文本\" width=\"180\" height=\"260\" align=\"middle\" />\n</div>\n\n### Multi-head Attention\nmulti-head attention结构如图：\n<div style=\"text-align: center\">\n<img src=\"/images/multi_head_attention.png\" alt=\"图片替换文本\" width=\"600\" height=\"300\" align=\"middle\" />\n</div>\n变量定义：\n$$\n\\begin{equation}\n\\begin{aligned}\n& n_{batch}: \\quad 样本batch \\, size\n\\\\ & n_{sequence}: \\quad 样本的序列长度\n\\\\ & n_{tokens}: \\quad token数量\n\\\\ & d_{k}: \\quad head \\, Q、K矩阵的维度\n\\\\& d_{v}: \\quad head \\, V矩阵的维度\n\\\\& d_{head}: \\quad d_{k}和d_{v}往往一样，都表示为d_{head}\n\\\\& n_{heads}: \\quad head数量\n\\\\& n_{layers}: transformer \\, block数量\n\\end{aligned}\n\\end{equation}\n$$\nmulti-head attention的计算逻辑为：\n$$\n\\begin{equation}\n\\label{multi_head_attention_op}\n\\begin{aligned}\nMultiHead(Q,\\, K,\\, V ) &= Concat(head_1,\\, head_2,\\, ...,\\, head_{n_{heads}}) W^O\n\\\\ head_i &= Attention(EW^Q_i,\\, EW^K_i,\\,EW^V_i) \n\\\\ &= softmax(\\frac{EW^Q_i (EW^K_i)^T}{\\sqrt{d_{head}}}) EW^V_i\n\\end{aligned}\n\\end{equation}\n$$\n其中，$W^Q_i \\in \\mathbb{R}^{d_{model}\\times d_{head}}, \\quad W^K_i \\in \\mathbb{R}^{d_{model}\\times d_{head}}, \\quad W^V_i \\in \\mathbb{R}^{d_{model}\\times d_{head}}, \\quad W^O \\in \\mathbb{R}^{n_{heads}d_{head} \\times d_{model}}$。\n\n* 矩阵乘法算子中包括浮点乘法和加法，$EW^Q_i$的输入激活Tensor是$E$，$E$在第一个block是词本身embedding和position embedding之和，在其他block是上游block的输出，$E$的形状为$[n_{batch},\\,n_{sequence},\\,d_{model}]$，$W^Q_i$的形状为$[d_{model}, \\, d_{head}]$，因此计算量为$2 n_{batch} n_{sequence} d_{model} d_{head}$，$EW^K_i$和$EW^V_i$同$EW^Q_i$；\n* Softmax算子的输入Tensor为$EW^Q_i$和$EW^K_i$，形状为$[n_{batch},\\,n_{sequence},\\,d_{head}]$，$EW^Q_i$和$EW^K_i$矩阵乘法的计算量为$2 n_{batch} n_{sequence}^2 d_{head}$，$exp$操作的计算量为$n_{batch} n_{sequence} n_{sequence}$，$reduceSum$操作计算量为$n_{batch} n_{sequence} n_{sequence}$，归一化操作计算量为$n_{batch} n_{sequence} n_{sequence}$，共$2 n_{batch} n_{sequence}^2 d_{head} + 3 n_{batch} n_{sequence}^2$；\n* $softmax EW^V_i$的输入Tensor为$[n_{batch},\\,n_{sequence},\\,n_{sequence}]$和$[n_{batch},\\,n_{sequence},\\,d_{head}]$，计算量为$2 n_{batch} n_{sequence}^2 d_{head}$；\n* 每个head的计算量为$6 n_{batch} n_{sequence} d_{model} d_{head} + 4 n_{batch} n_{sequence} d_{head} + n_{batch} n_{sequence}^2 d_{head}$，共$n_{heads}$个head，计算量为：$n_{heads} ( 6 n_{batch} n_{sequence} d_{model} d_{head} + 4 n_{batch} n_{sequence}^2 d_{head} + 3 n_{batch} n_{sequence}^2 )$；\n* $concat W^O$输入为激活$[n_{batch},\\,n_{sequence},\\,d_{head} n_{heads}]$和参数$[d_{head} n_{heads},\\,d_{model}]$，计算量为$2 n_{batch} n_{sequence} d_{model} d_{head} n_{heads}$。\n\n综上，multi-head attention的总计算量为：\n$$\n\\begin{equation}\n\\label{multi_head_attention_flops}\n\\begin{aligned}\nn_{multi-head \\, attention \\, flops} &= n_{batch} n_{sequence} ( 8 n_{heads} d_{model} d_{head} + 4 n_{sequece} n_{heads} d_{head} + 3 n_{heads} n_{sequece})\n\\end{aligned}\n\\end{equation}\n$$\n\n### MLP\nmulti-head attention后面，接两层的全连接网络，计算逻辑为：\n$$\n\\begin{equation}\n\\label{mlp_op}\n\\begin{aligned}\nFFN(x) = gelu(xW_1 + b_1) W_2 + b_2\n\\end{aligned}\n\\end{equation}\n$$\n其中，$W_1 \\in \\mathbb{R}^{d_{ffn}\\times d_{model}}, \\quad b_1 \\in \\mathbb{R}^{d_{ffn}}, \\quad W_2 \\in \\mathbb{R}^{d_{ffn}\\times d_{model}}, \\quad b_2 \\in \\mathbb{R}^{d_{model}}$。\n\n* 矩阵乘法算子$xW_1$的输入激活Tensor $x$是multi-head attention输出，形状为$[n_{batch},\\,n_{sequence},\\,d_{model}]$，$W_1$形状为$[d_{model},\\,d_{ffn}]$，计算量为$2 n_{batch} n_{sequence} d_{ffn} d_{model}$；\n* $xW_1 + b_1$加法部分输入是激活$[n_{batch},\\,n_{sequence},\\,d_{ffn}]$和参数$[d_{ffn},\\,1]$，计算量为$n_{batch} n_{sequence} d_{ffn}$；\n* $gelu$输入和输出都是$[n_{batch},\\,n_{sequence},\\,d_{ffn}]$，计算量简单计为$n_{batch} n_{sequence} d_{ffn}$；\n* $gelu W_2$的输入是激活$[n_{batch},\\,n_{sequence},\\,d_{ffn}]$和参数$[d_{ffn},\\,d_{model}]$，计算量为$2 n_{batch} n_{sequence} d_{model} d_{ffn}$；\n* $gelu W_2 + b_2$加法部分输入是激活$[n_{batch},\\,n_{sequence},\\,d_{model}]$和参数$[d_{model},\\,1]$，计算量为$n_{batch} n_{sequence} d_{model}$；\n\n综上，mlp部分的总计算量为：\n$$\n\\begin{equation}\n\\label{mlp_flops}\n\\begin{aligned}\nn_{mlp \\, flops} &= n_{batch} n_{sequence} ( 4 d_{ffn} d_{model} + 2 d_{ffn} + d_{model})\n\\end{aligned}\n\\end{equation}\n$$\n\n### Add&Norm\n两个add部分的计算量为$2 n_{batch} n_{sequence} d_{model}$。每个norm中均值计算量为$2 n_{batch} n_{sequence} d_{model}$，标准差计算量为$3 n_{batch} n_{sequence} d_{model}$，归一化计算量为$3 n_{batch} n_{sequence} d_{model}$\n所以，add&norm部分的计算量为：\n$$\n\\begin{equation}\n\\label{add_norm_flops}\n\\begin{aligned}\nn_{add-norm \\, flops} = 18 n_{batch} n_{sequence} d_{model}\n\\end{aligned}\n\\end{equation}\n$$\n\n## Transformer Blocks计算量\n综上，每个transformer block的计算量为：\n$$\n\\begin{equation}\n\\label{transformer_block_flops}\n\\begin{aligned}\nn_{transformer \\, block \\, flops} &= n_{multi-head \\, attention \\, flops} + n_{mlp \\, flops} + n_{add-norm \\, flops}\n\\\\ &= n_{batch} n_{sequence} ( 8 n_{heads} d_{model} d_{head} + 4 n_{sequece} n_{heads} d_{head} + 3 n_{heads} n_{sequece} + 4 d_{ffn} d_{model} + 2 d_{ffn} + 19 d_{model})\n\\end{aligned}\n\\end{equation}\n$$\n共$n_{layers}$个transformer block，总计算量为：\n$$\n\\begin{equation}\n\\label{transformer_multi_block_flops}\n\\begin{aligned}\nn_{transformer \\, multi \\, block \\, flops} &= n_{layers} n_{transformer \\, block \\, flops}\n\\\\ &= n_{batch} n_{sequence} n_{layers} ( 8 n_{heads} d_{model} d_{head} + 4 n_{sequece} n_{heads} d_{head} + 3 n_{heads} n_{sequece} + 4 d_{ffn} d_{model} + 2 d_{ffn} + 19 d_{model})\n\\end{aligned}\n\\end{equation}\n$$\n\n# 推理阶段\n在推理阶段，计费量为：\n$$\n\\begin{equation}\n\\label{pred_flops}\n\\begin{aligned}\nn_{predict \\, flops} = n_{transformer \\, multi \\, block \\, flops} \n\\end{aligned}\n\\end{equation}\n$$\n按GPT3 175B配置计算：\n$$\n\\begin{equation}\n\\label{gpt3_175b_pred_flops}\n\\begin{aligned}\nn_{gpt3 \\, 175B \\, predict \\, flops} &= n_{batch} n_{sequence} ( 347924201472 + 4746240 n_{sequence})\n\\\\ &= n_{tokens} ( 347928920064 + 1179648 n_{sequence})\n\\end{aligned}\n\\end{equation}\n$$\n按$n_{batch}=1,\\, n_{sequence}=2048$的话，约为$7.32e14$。每个token每个参数的计算量为：\n$$\n\\begin{equation}\n\\label{gpt3_175b_pred_flops_per_token_param}\n\\begin{aligned}\nn_{gpt3 \\, 175B \\, predict \\, flops \\, per \\, token \\, per \\, param} &= n_{tokens} ( 347924201472 + 4746240 n_{sequence}) / n_{tokens} / n_{params}\n\\\\ & \\approx 1.99\n\\end{aligned}\n\\end{equation}\n$$\n每个token的计算量约为$2 n_{params}$。\n\n# 训练阶段\n在训练阶段，需要反向传播，反向传播计算量和前向一致。另外还有梯度更新，Adam需要更新两个累积值和参数，计算量简单计为$3 n_{params}$，因此训练的计算量为：\n$$\n\\begin{equation}\n\\label{train_flops}\n\\begin{aligned}\nn_{train \\, flops} &= n_{batch} n_{sequence} ( 16 n_{heads} d_{model} d_{head} + 8 n_{sequece} n_{heads} d_{head} + 6 n_{heads} n_{sequece} + 8 d_{ffn} d_{model} + 4 d_{ffn} + 38 d_{model} + 3 n_{params})\n\\end{aligned}\n\\end{equation}\n$$\n按GPT3 175B配置计算：\n$$\n\\begin{equation}\n\\label{gpt3_175b_train_flops}\n\\begin{aligned}\nn_{gpt3 \\, 175B \\, train \\, flops} &= n_{batch} n_{sequence} ( 1219547824128 + 9492480 n_{sequence})\n\\\\ &= n_{tokens} ( 1219547824128 + 9492480 n_{sequence})\n\\end{aligned}\n\\end{equation}\n$$\n按GPT3 175B配置$n_{sequence}=2048$和$n_{tokens}=300B$计算，训练的总计算量为$3.72e23$，与GPT3论文接近。在300张A100 80G卡集群训练，GPU利用率40%情况下，需要耗时$3.72e23/300/(312*10^{12})/0.4 \\approx 9927791 seconds \\approx 115days$。每个token每个参数的计算量为：\n$$\n\\begin{equation}\n\\label{gpt3_175b_train_flops_per_token_param}\n\\begin{aligned}\nn_{gpt3 \\, 175B \\, train \\, flops \\, per \\, token \\, per \\, param} &= n_{tokens} ( 1219547824128 + 9492480 n_{sequence}) / n_{tokens} / n_{params}\n\\\\ & \\approx 7.1\n\\end{aligned}\n\\end{equation}\n$$\n每个token的计算量约为$7n_{params}$。\n\n<div style=\"text-align: center\">\n<img src=\"/images/gpt3_flops.png\" alt=\"图片替换文本\" width=\"900\" height=\"400\" align=\"middle\" />\n</div>\n\n# 参考资料\n* [Attention Is All You Need](http://arxiv.org/abs/1706.03762)\n* [Language Models are Few-Shot Learners](http://arxiv.org/abs/2005.14165)\n* [分析transformer模型的参数量、计算量、中间激活、KV cache](https://zhuanlan.zhihu.com/p/624740065)","tags":["Machine-Learning","Deep-Learning"],"categories":["tech"]},{"title":"ChatGPT量化分析（二） - 存储占用分析","url":"/4e7f3af5/","content":"\n<!-- toc -->\n\n# 背景\nChatGPT出现后，惊人的效果完全颠覆了业界人员包括笔者的认知，抛开其模型细节层面的因素，已公开的训练方法，需要巨量的数据和计算资源，门槛非常高。本文基于公开资料，希望以量化方式分多篇介绍ChatGPT的分析结论，具体内容包含以下三篇，本文为存储占用分析篇。\n\n* [ChatGPT量化分析（一） - 模型参数分析](https://mathmach.com/2a29fbbd/)\n* [ChatGPT量化分析（二） - 存储占用分析](https://mathmach.com/4e7f3af5/)\n* [ChatGPT量化分析（三） - 计算量分析](https://mathmach.com/b3deb998/)\n\nChatGPT模型结构为Transformer，Transformer模型运行时存储可以分成两部分，其一是模型参数，这部分规模是固定的，其二是中间激活，这部分和batch size、sequence length有线性关系，下面对两者分别分析。\n\n# 模型参数\n\n变量定义：\n$$\n\\begin{equation}\n\\begin{aligned}\n& n_{params}: \\quad 模型参数数量\n\\\\ & n_{memory}: \\quad 存储占用，以byte为单位\n\\end{aligned}\n\\end{equation}\n$$\n\n## 训练阶段\n在训练阶段，采用Adam优化器，先看下Adam的公式[SGD优化算法的各种变体](https://mathmach.com/f4f9dedd/)。\n\nAdam超参数：\n\n* 学习率 $\\eta \\in (0, +\\infty) \\quad [default=0.001]$\n* 惯性项 $\\rho_1 \\in [0, 1) \\quad [default=0.9]$\n* 惯性项 $\\rho_2 \\in [0, 1) \\quad [default=0.999]$\n* 衰减率 $\\lambda \\in [0, 1] \\quad [default=1-10^{-8}]$\n\nAdam更新公式：\n$$ init: \\quad m_0 = 0, \\; v_0 = 0, \\; t = 0 $$\n$$ t = t + 1 $$\n$$ \\rho_{1,t} = \\rho_1 \\cdot \\lambda^{t-1} $$\n$$ g_t = \\nabla_w f_t(w_{t-1}) $$\n$$ m_t = \\rho_{1,t} \\cdot m_{t-1} + (1 - \\rho_{1,t}) \\cdot g_t $$\n$$ v_t = \\rho_2 \\cdot v_{t-1} + (1 - \\rho_2) \\cdot g_t^2 $$\n$$ \\widehat{m_t} = \\frac{m_t}{(1 - \\rho_1^t)} $$\n$$ \\widehat{v_t} = \\frac{v_t}{(1 - \\rho_2^t)} $$\n$$ w_t = w_{t-1} - \\eta \\cdot \\frac{\\widehat{m_t}}{\\sqrt{\\widehat{v_t}} + \\epsilon} $$\n\nAdam中有$m$和$v$两个动量累积值，因此每个模型训练参数会对应4个浮点值，包括参数本身、梯度值、Adam两个动量累积值。这部分参数对精度要求较高，需要用到float32（4 bytes）存储，每个参数占用存储16 bytes。\n因此，训练阶段全部模型参数共占用存储为：\n$$\n\\begin{equation}\n\\label{params_memory}\n\\begin{aligned}\nn_{params \\, memory} = 16 n_{params}\n\\end{aligned}\n\\end{equation}\n$$\nGPT3 175B模型，训练阶段参数存储占用为2595GB。\n\n## 推理阶段\n推理阶段只有前向过程，用float16存储的话，存储占用为：\n$$\n\\begin{equation}\n\\label{embedding_param_num}\n\\begin{aligned}\nn_{params \\, memory} = 2 n_{params}\n\\end{aligned}\n\\end{equation}\n$$\nGPT3 175B模型，推理阶段参数存储占用为324GB。可以看到，A100单卡80G也无法放下，需要更低的存储精度或者多级存储方案，或者单机多卡。\n\n# 中间激活\n\n变量定义：\n$$\n\\begin{equation}\n\\begin{aligned}\n& n_{batch}: \\quad 样本batch \\, size\n\\\\ & n_{sequence}: \\quad 样本的序列长度\n\\end{aligned}\n\\end{equation}\n$$\n\nTensorFlow、PyTorch、MXNet等深度学习框架以逻辑计算图描述模型，以运行时计算图启动计算，计算图以Tensor（数据）和Operatopn（算子）组织，Operation依赖输入Tensor，通过内部计算得到输出Tensor。更具体一点，前向计算过程中，Operation接收上一Operation产出的激活Tensor和本Operation模型参数Tensor，计算得到激活Tensor给下游Operation，既：\n$$\n\\begin{equation}\n\\label{forward_op}\n\\begin{aligned}\ntensor_{output \\, activation} = forwardOperation(tensor_{input \\, activation},\\, tensor_{param})\n\\end{aligned}\n\\end{equation}\n$$\n后向计算过程，除上述Tensor外，还有下游Operation后向传回来的梯度Tensor，既：\n$$\n\\begin{equation}\n\\label{backward_op}\n\\begin{aligned}\ntensor_{input \\, activation \\, grad}, \\, tensor_{param \\, grad} = backwardOperation(& tensor_{input \\, activation},\\, \\\\& tensor_{output \\, activation},\\, \\\\& tensor_{output \\, activation \\, grad} \\, \\\\& tensor_{param})\n\\end{aligned}\n\\end{equation}\n$$\n其中，$tensor_{input \\, activation \\, grad}$需要传给上游Operation用于后向传播的梯度计算，$tensor_{param}$和$tensor_{param \\, grad}$传给优化器用于梯度更新。模型参数param和其梯度param grad在上一节内容已经包括在内，所以本节只考虑activation相关的Tensor存储占用即可。Operation内部也有中间变量，这些临时变量生命中期较短，因此本文讨论存储占用时，不考虑这部分，只考虑计算图中Operation之间的Tensor。一个Operation的输出Tensor是下游多个Operation的输入Tensor，此Tensor只占用一份存储即可，因此下文统计存储时只考虑Operation的输出Tensor。中间激活也和Operation的数量或粒度有关，细粒度Operation接口的激活值更多，粗粒度Operation将部分激活转化成了内部临时变量，因此接口的激活值更少。\n\n下面按Operation进行分析。训练阶段经常使用混合精度训练，推理阶段也会采用量化，因此下面统一以半精度float16（2 bytes）进行浮点存储和计算。\n\n## Embedding\nembedding层是lookup操作，输入是词序列，输出形状是$[n_{batch},\\,n_{sequence},\\,d_{model}]$，存储占用为：\n$$\n\\begin{equation}\n\\label{embedding_memory}\n\\begin{aligned}\nn_{embedding \\, memory} = 2 n_{batch} n_{sequence} d_{model}\n\\end{aligned}\n\\end{equation}\n$$\n\n## Transformer Blocks\ntransformer block的计算图如下，每个transformer block主要包含四部分，既multi-head attention和mlp，以及两个add&norm。\n<div style=\"text-align: center\">\n<img src=\"/images/transformer_block.png\" alt=\"图片替换文本\" width=\"180\" height=\"260\" align=\"middle\" />\n</div>\n\n### Multi-head Attention\nmulti-head attention结构如图：\n<div style=\"text-align: center\">\n<img src=\"/images/multi_head_attention.png\" alt=\"图片替换文本\" width=\"600\" height=\"300\" align=\"middle\" />\n</div>\n变量定义：\n$$\n\\begin{equation}\n\\begin{aligned}\n& d_{k}: \\quad head \\, Q、K矩阵的维度\n\\\\& d_{v}: \\quad head \\, V矩阵的维度\n\\\\& d_{head}: \\quad d_{k}和d_{v}往往一样，都表示为d_{head}\n\\\\& n_{heads}: \\quad head数量\n\\\\& n_{layers}: transformer \\, block数量\n\\end{aligned}\n\\end{equation}\n$$\nmulti-head attention的计算逻辑为：\n$$\n\\begin{equation}\n\\label{multi_head_attention_op}\n\\begin{aligned}\nMultiHead(Q,\\, K,\\, V ) &= Concat(head_1,\\, head_2,\\, ...,\\, head_{n_{heads}}) W^O\n\\\\ head_i &= Attention(EW^Q_i,\\, EW^K_i,\\,EW^V_i) \n\\\\ &= softmax(\\frac{EW^Q_i (EW^K_i)^T}{\\sqrt{d_{head}}}) EW^V_i\n\\end{aligned}\n\\end{equation}\n$$\n其中，$W^Q_i \\in \\mathbb{R}^{d_{model}\\times d_{head}}, \\quad W^K_i \\in \\mathbb{R}^{d_{model}\\times d_{head}}, \\quad W^V_i \\in \\mathbb{R}^{d_{model}\\times d_{head}}, \\quad W^O \\in \\mathbb{R}^{n_{heads}d_{head} \\times d_{model}}$。\n\n* 矩阵乘法算子$EW^Q_i$的输入激活Tensor是E，E在第一个block是词本身embedding和position embedding之和，在其他block是上游block的输出，E的形状为$[n_{batch},\\,n_{sequence},\\,d_{model}]$，乘法输出Tensor形状为$[n_{batch},\\,n_{sequence},\\,d_{head}]$，因此存储占用为$2 n_{batch} n_{sequence} d_{head}$，$EW^K_i$和$EW^V_i$同$EW^Q_i$；\n* Softmax算子的输入Tensor为$EW^Q_i$和$EW^K_i$，输出形状为$[n_{batch},\\,n_{sequence},\\,n_{sequence}]$，存储占用为$2 n_{batch} n_{sequence} n_{sequence}$；\n* $softmax EW^V_i$的输入Tensor为$[n_{batch},\\,n_{sequence},\\,n_{sequence}]$和$[n_{batch},\\,n_{sequence},\\,d_{head}]$，输出为$[n_{batch},\\,n_{sequence},\\,d_{head}]$，存储为$2 n_{batch} n_{sequence} d_{head}$；\n* 每个head的存储为$6 n_{batch} n_{sequence} d_{head} + 2 n_{batch} n_{sequence} n_{sequence} + 2 n_{batch} n_{sequence} d_{head}$，共$n_{heads}$个head，存储占用为：$n_{heads} ( 6 n_{batch} n_{sequence} d_{head} + 2 n_{batch} n_{sequence} n_{sequence} + 2 n_{batch} n_{sequence} d_{head} )$；\n* concat的输入为$n_{heads}$个$[n_{batch},\\,n_{sequence},\\,d_{head}]$，输出为$[n_{batch},\\,n_{sequence},\\,d_{head} n_{heads}]$，存储为$2 n_{batch} n_{sequence} d_{head} n_{heads}$；\n* $concat W^O$输入为激活$[n_{batch},\\,n_{sequence},\\,d_{head} n_{heads}]$和参数$[d_{head} n_{heads},\\,d_{model}]$，输出为$[n_{batch},\\,n_{sequence},\\,d_{model}]$，因此存储为$2 n_{batch} n_{sequence} d_{model}$。\n\n实际实现时，往往不会使用concat，而是将多头的Q、K、V合并成大矩阵，因此将concat存储忽略。综上，multi-head attention的输入存储总占用为：\n$$\n\\begin{equation}\n\\label{multi_head_attention_memory}\n\\begin{aligned}\nn_{multi-head \\, attention \\, memory} &= n_{batch} n_{sequence} ( 8 n_{heads} d_{head} + 2 n_{heads} n_{sequence} + 2 d_{model} )\n\\end{aligned}\n\\end{equation}\n$$\n\n### MLP\nmulti-head attention后面，接两层的全连接网络，计算逻辑为：\n$$\n\\begin{equation}\n\\label{mlp_op}\n\\begin{aligned}\nFFN(x) = g(xW_1 + b_1) W_2 + b_2\n\\end{aligned}\n\\end{equation}\n$$\n其中，$W_1 \\in \\mathbb{R}^{d_{ffn}\\times d_{model}}, \\quad b_1 \\in \\mathbb{R}^{d_{ffn}}, \\quad W_2 \\in \\mathbb{R}^{d_{ffn}\\times d_{model}}, \\quad b_2 \\in \\mathbb{R}^{d_{model}}$。\n\n* 矩阵乘法算子$xW_1$的输入激活Tensor x是multi-head attention输出，形状为$[n_{batch},\\,n_{sequence},\\,d_{model}]$，输出Tensor形状为$[n_{batch},\\,n_{sequence},\\,d_{ffn}]$，因此存储占用为$2 n_{batch} n_{sequence} d_{ffn}$；\n* $xW_1 + b_1$加法部分输入是激活$[n_{batch},\\,n_{sequence},\\,d_{ffn}]$和参数$[d_{ffn},\\,1]$，输出是$[n_{batch},\\,n_{sequence},\\,d_{ffn}]$，存储占用为$2 n_{batch} n_{sequence} d_{ffn}$；\n* $gelu$输入和输出都是$[n_{batch},\\,n_{sequence},\\,d_{ffn}]$，输入存储为$2 n_{batch} n_{sequence} d_{ffn}$；\n* $gelu W_2$的输入是激活$[n_{batch},\\,n_{sequence},\\,d_{ffn}]$和参数$[d_{ffn},\\,d_{model}]$，输出是$[n_{batch},\\,n_{sequence},\\,d_{model}]$，输入存储为$2 n_{batch} n_{sequence} d_{model}$；\n* $gelu W_2 + b_2$加法部分输入是激活$[n_{batch},\\,n_{sequence},\\,d_{model}]$和参数$[d_{model},\\,1]$，输出是$[n_{batch},\\,n_{sequence},\\,d_{model}]$，输入存储占用为$2 n_{batch} n_{sequence} d_{model}$；\n\n综上，mlp部分的存储占用为：\n$$\n\\begin{equation}\n\\label{mlp_memory}\n\\begin{aligned}\nn_{mlp \\, memory} &= n_{batch} n_{sequence} ( 6 d_{ffn} + 4 d_{model} )\n\\end{aligned}\n\\end{equation}\n$$\n\n### Add&Norm\nadd部分的输入激活可以与multi-head attention和mlp里的输入共享存储，因此不用计算。norm的输入和输出都是$[n_{batch},\\,n_{sequence},\\,d_{model}]$，两个norm的存储占用为$4 n_{batch} n_{sequence} d_{model}$。\n所以，add&norm部分的存储占用为：\n$$\n\\begin{equation}\n\\label{add_norm_memory}\n\\begin{aligned}\nn_{add-norm \\, memory} = 4 n_{batch} n_{sequence} d_{model}\n\\end{aligned}\n\\end{equation}\n$$\n\n## 中间激活存储占用\n综上，每个transformer block的存储占用为：\n$$\n\\begin{equation}\n\\label{transformer_block_memory}\n\\begin{aligned}\nn_{transformer \\, block \\, memory} &= n_{multi-head \\, attention \\, memory} + n_{mlp \\, memory} + n_{add-norm \\, memory}\n\\\\ &= n_{batch} n_{sequence} ( 8 n_{heads} d_{head} + 2 n_{heads} n_{sequence} + 10 d_{model} + 6 d_{ffn})\n\\end{aligned}\n\\end{equation}\n$$\n共$n_{layers}$个block，因此总存储占用为：\n$$\n\\begin{equation}\n\\label{transformer_blocks_memory}\n\\begin{aligned}\nn_{transformer \\, multi \\, block \\, memory} &= n_{layers} n_{transformer \\, block \\, memory}\n\\\\ &= n_{batch} n_{sequence} ( 8 n_{heads} d_{head} n_{layers} + 2 n_{heads} n_{sequence} n_{layers} + 10 d_{model} n_{layers} + 6 d_{ffn} n_{layers})\n\\end{aligned}\n\\end{equation}\n$$\n\n### 训练阶段\n在训练阶段，中间激活需要保留用于反向传播的梯度计算，并且每个激活tensor都对应一个梯度tensor，因此存储占用翻倍，既$2 n_{transformer \\, multi \\, block \\, memory}$。\n按GPT3 175B配置计算：\n$$\n\\begin{equation}\n\\label{gpt3_175b_train_memory}\n\\begin{aligned}\nn_{gpt3 \\, 175B \\, train \\, memory} &= 2 n_{transformer \\, multi \\, block \\, memory} \n\\\\ &= n_{batch} n_{sequence} ( 99090432 + 36864 n_{sequence})\n\\end{aligned}\n\\end{equation}\n$$\n按$n_{sequence}=2048$的话，每种$n_{batch}$的存储占用为：\n\n<div style=\"text-align: center\">\n|  $n_{batch}$   | 中间激活存储(GB)  |\n|  ----  | ----    |\n| 128  |  42624 |\n| 512  |  170496 |\n| 1024   |  340992 |\n| 4096   |  1363968 |\n</div>\n\n这里将较小的embedding存储忽略，可以看到，相比模型参数部分占用的2595GB，中间激活存储占比更大，需要借助混合并行（数据并行、流水并行、模型并行）、recompute、ZeRO等技术来解决。在batch为128时，存储消耗2595+42624G，需要近600张A100 80G卡。\n\n### 推理阶段\n在推理阶段，中间激活使用完成后可以立即释放。多个block串行计算，block间可以共享内存。block内部有些tensor也串行依赖，tensor使用完成后即可释放。因此，推理阶段最大存储占用比单block存储大小$\\eqref{transformer_block_memory}$更小一点。\n按GPT3 175B配置计算：\n$$\n\\begin{equation}\n\\label{gpt3_175b_pred_memory}\n\\begin{aligned}\nn_{gpt3 \\, 175B \\, predict \\, memory} & \\lt n_{transformer \\, block \\, memory} \n\\\\ &= n_{batch} n_{sequence} ( 516096 + 192 n_{sequence})\n\\end{aligned}\n\\end{equation}\n$$\n按$n_{batch}=1,\\, n_{sequence}=2048$的话，约为1.7G，相比于模型参数占用324G很小，因此推理阶段存储主要消耗在模型参数上。\n\n# 参考资料\n* [Attention Is All You Need](http://arxiv.org/abs/1706.03762)\n* [Language Models are Few-Shot Learners](http://arxiv.org/abs/2005.14165)\n* [分析transformer模型的参数量、计算量、中间激活、KV cache](https://zhuanlan.zhihu.com/p/624740065)\n* [DeepSpeed之ZeRO系列：将显存优化进行到底](https://basicv8vc.github.io/posts/zero/)\n* [巨型AI模型背后的分布式训练技术](https://zhuanlan.zhihu.com/p/430383324)\n* [巨型AI模型背后的分布式训练技术（二）](https://zhuanlan.zhihu.com/p/629443563)\n* [OneFlow —— 让每一位算法工程师都有能力训练GPT](https://zhuanlan.zhihu.com/p/371499074)\n* [千亿参数开源大模型BLOOM背后的技术](https://zhuanlan.zhihu.com/p/615839149)","tags":["Machine-Learning","Deep-Learning"],"categories":["tech"]},{"title":"ChatGPT量化分析（一） - 模型参数分析","url":"/2a29fbbd/","content":"\n<!-- toc -->\n\n# 背景\nChatGPT出现后，惊人的效果完全颠覆了业界人员包括笔者的认知，抛开其模型细节层面的因素，已公开的训练方法，需要巨量的数据和计算资源，门槛非常高。本文基于公开资料，希望以量化方式分多篇介绍ChatGPT的分析结论，具体内容包含以下三篇，本文为模型参数分析篇。\n\n* [ChatGPT量化分析（一） - 模型参数分析](https://mathmach.com/2a29fbbd/)\n* [ChatGPT量化分析（二） - 存储占用分析](https://mathmach.com/4e7f3af5/)\n* [ChatGPT量化分析（三） - 计算量分析](https://mathmach.com/b3deb998/)\n\n# 模型结构\n\n基于Transformer结构的encoder-decoder模型如图：\n<div style=\"text-align: center\">\n<img src=\"/images/transformer_model.png\" alt=\"图片替换文本\" width=\"420\" height=\"600\" align=\"middle\" />\n</div>\nencoder编码器将输入词序列$(x_1,\\,x_2,\\,...,\\,x_n)$转换成一个向量序列$(z_1,\\,z_2,\\,...,\\,z_n)$，decoder解码器生成输出词序列$(y_1,\\,y_2,\\,...,\\,y_m)$。解码时一个个词按顺序解码，在解码$y_i$时，根据$(z_1,\\,z_2,\\,...,\\,z_n)$和$(y_1,\\,y_2,\\,...,\\,y_{i-1})$，生成$y_i$。\nChatGPT的基础模型为Transformer，采用的是decoder-only结构。\n\n# 参数量分析\n\n输入词序列$(x_1,\\,x_2,\\,...,\\,x_n)$首先lookup词向量，得到$(e_1,\\,e_2,\\,...,\\,e_n)$，然后经过多个transformer block，参数分布在embedding层和transformer block中。\n\n## Embedding层\n变量定义：\n$$\n\\begin{equation}\n\\begin{aligned}\n& d_{model}: \\quad 词的embedding\\,size\n\\\\ & n_{vocab}: \\quad 词表大小\n\\\\ & n_{windows}: \\quad 上文窗口或者序列最大长度\n\\end{aligned}\n\\end{equation}\n$$\n对于可训练的position embedding，其参数量为$n_{windows} d_{model}$，一般在整体中占比不大，并且很多模型都为固定参数，因此将它忽略。\n所以，embedding层的参数量为：\n$$\n\\begin{equation}\n\\label{embedding_param_num}\n\\begin{aligned}\nn_{embedding \\, params} = n_{vocab} d_{model}\n\\end{aligned}\n\\end{equation}\n$$\n\n## Transformer Blocks层\ntransformer block计算图如下，每个transformer block的参数分布在两部分中，既multi-head attention和mlp。\n<div style=\"text-align: center\">\n<img src=\"/images/transformer_block.png\" alt=\"图片替换文本\" width=\"180\" height=\"260\" align=\"middle\" />\n</div>\n\n### Multi-head Attention\nmulti-head attention结构如图：\n<div style=\"text-align: center\">\n<img src=\"/images/multi_head_attention.png\" alt=\"图片替换文本\" width=\"600\" height=\"300\" align=\"middle\" />\n</div>\n变量定义：\n$$\n\\begin{equation}\n\\begin{aligned}\n& d_{k}: \\quad head \\, Q、\\, K矩阵的维度\n\\\\ & d_{v}: \\quad head \\, V矩阵的维度\n\\\\ & d_{head}: \\quad d_{k}和d_{v}往往一样，都表示为d_{head}\n\\\\ & n_{heads}: \\quad head数量\n\\\\ & n_{layers}: transformer \\, block数量\n\\end{aligned}\n\\end{equation}\n$$\nmulti-head attention的计算逻辑为：\n$$\n\\begin{equation}\n\\label{multi_head_attention_op}\n\\begin{aligned}\nMultiHead(Q,\\, K,\\, V ) &= Concat(head_1,\\, head_2,\\, ...,\\, head_{n_{heads}}) W^O\n\\\\ head_i &= Attention(EW^Q_i,\\, EW^K_i,\\,EW^V_i) \n\\\\ &= softmax(\\frac{EW^Q_i (EW^K_i)^T}{\\sqrt{d_{head}}}) EW^V_i\n\\end{aligned}\n\\end{equation}\n$$\n其中，$W^Q_i \\in \\mathbb{R}^{d_{model}\\times d_{head}}, \\quad W^K_i \\in \\mathbb{R}^{d_{model}\\times d_{head}}, \\quad W^V_i \\in \\mathbb{R}^{d_{model}\\times d_{head}}, \\quad W^O \\in \\mathbb{R}^{n_{heads}d_{head} \\times d_{model}}$。\n所以，multi-head attention的参数量为：\n$$\n\\begin{equation}\n\\label{multi_head_attention_param_num}\n\\begin{aligned}\nn_{multi-head \\, attention \\, params} = 4 n_{heads} d_{model} d_{head}\n\\end{aligned}\n\\end{equation}\n$$\n\n### MLP\nmulti-head attention后面，接两层的全连接网络，计算逻辑为：\n$$\n\\begin{equation}\n\\label{mlp_op}\n\\begin{aligned}\nFFN(x) = gelu(xW_1 + b_1) W_2 + b_2\n\\end{aligned}\n\\end{equation}\n$$\n其中，$W_1 \\in \\mathbb{R}^{d_{ffn}\\times d_{model}}, \\quad b_1 \\in \\mathbb{R}^{d_{ffn}}, \\quad W_2 \\in \\mathbb{R}^{d_{ffn}\\times d_{model}}, \\quad b_2 \\in \\mathbb{R}^{d_{model}}$。\n\n所以，mlp的参数量为：\n$$\n\\begin{equation}\n\\label{mlp_param_num}\n\\begin{aligned}\nn_{mlp \\, params} = 2 d_{ffn} d_{model} + d_{ffn} + d_{model}\n\\end{aligned}\n\\end{equation}\n$$\n\n### Transformer Block参数量\n根据multi-head attention和mlp，得到每个transformer block的参数量为：\n$$\n\\begin{equation}\n\\label{transformer_block_param_num}\n\\begin{aligned}\nn_{transformer \\, block \\, params} = 4 n_{heads} d_{model} d_{head} + 2 d_{ffn} d_{model} + d_{ffn} + d_{model}\n\\end{aligned}\n\\end{equation}\n$$\n共$n_{layers}$个transformer block，参数量为：\n$$\n\\begin{equation}\n\\label{transformer_blocks_param_num}\n\\begin{aligned}\nn_{transformer \\, multi \\, block \\, params} = n_{layers} (4 n_{heads} d_{model} d_{head} + 2 d_{ffn} d_{model} + d_{ffn} + d_{model})\n\\end{aligned}\n\\end{equation}\n$$\n\n# 模型总参数量\n最终，Transformer的总参数量为：\n$$\n\\begin{equation}\n\\label{whole_param_num}\n\\begin{aligned}\nn_{params} = n_{vocab} d_{model} + n_{layers} (4 n_{heads} d_{model} d_{head} + 2 d_{ffn} d_{model} + d_{ffn} + d_{model})\n\\end{aligned}\n\\end{equation}\n$$\n\n在GPT3中，$d_{head} = \\frac{d_{model}}{n_{heads}}，\\, d_{ffn} = 4 d_{model}$，都与Transformer原始论文一致，上式也可写为：\n$$\n\\begin{equation}\n\\label{whole_param_num_simple}\n\\begin{aligned}\nn_{params} = n_{vocab} d_{model} + n_{layers} (12 d_{model}^2 + 5 d_{model})\n\\end{aligned}\n\\end{equation}\n$$\n\n根据GPT3 175B的模型参数配置，并假设$n_{vocab}=50000$，代入公式$\\eqref{whole_param_num}$计算得到的参数量为174566473728，与175B非常接近。\n<div style=\"text-align: center\">\n<img src=\"/images/gpt3_param_num.png\" alt=\"图片替换文本\" width=\"700\" height=\"200\" align=\"middle\" />\n</div>\n各个参数对模型大小的影响力度不同，这里需要特别注意$d_{ffn}$的配置，如果$d_{ffn} = d_{model}$，GPT3 175B大小的模型将会降低至87B，可见$d_{ffn}$对模型参数量的影响力度非常大，但它对模型效果影响在论文中并没有提及。\n\n# 各配置对总参数量的影响\n对参数配置进行求导，得到：\n$$\n\\begin{equation}\n\\label{whole_param_num_grad}\n\\begin{aligned}\n& \\frac{\\partial n_{params}}{\\partial d_{model}} = n_{vocab} + n_{layers} (4 n_{heads} d_{head} + 2 d_{ffn} + 1)\n\\\\ & \\frac{\\partial n_{params}}{\\partial d_{ffn}} = n_{layers} (2 d_{model} + 1)\n\\\\ & \\frac{\\partial n_{params}}{\\partial d_{head}} = n_{layers} (4 n_{heads} d_{model})\n\\\\ & \\frac{\\partial n_{params}}{\\partial n_{layers}} = 4 n_{heads} d_{model} d_{head} + 2 d_{ffn} d_{model} + d_{ffn} + d_{model}\n\\\\ & \\frac{\\partial n_{params}}{\\partial n_{heads}} = 4 n_{layers} d_{model} d_{head}\n\\end{aligned}\n\\end{equation}\n$$\n我们使用GPT3 175B配置代入$\\eqref{whole_param_num_grad}$得到：\n\n|  参数配置   | 梯度值  |\n|  ----  | ----    |\n| $\\frac{\\partial n_{params}}{\\partial d_{model}}$   |  14175872 |\n| $\\frac{\\partial n_{params}}{\\partial d_{ffn}}$   |  2359392 |\n| $\\frac{\\partial n_{params}}{\\partial d_{head}}$   |  452984832 |\n| $\\frac{\\partial n_{params}}{\\partial n_{layers}}$   |  1812000768 |\n| $\\frac{\\partial n_{params}}{\\partial n_{heads}}$   |  603979776 |\n\n可以看到，$n_{layers}$、$n_{heads}$、$d_{head}$对参数量的影响最大，在GPT3中训练了不同规模的模型，主要通过调整这几个参数。\n\n# 参考资料\n* [Attention Is All You Need](http://arxiv.org/abs/1706.03762)\n* [Language Models are Few-Shot Learners](http://arxiv.org/abs/2005.14165)","tags":["Machine-Learning","Deep-Learning"],"categories":["tech"]},{"title":"Softmax近似方法（三） - NCE、NEG、Sampled Softmax对比","url":"/a817e3bf/","content":"\n<!-- toc -->\n\n# 背景\n在[Noise Contrastive Estimation理论详解](https://mathmach.com/b5ec3256/)和[Sampled Softmax理论详解](https://mathmach.com/b5ec3256/)中，针对大规模多分类概率模型$p(x)=\\frac{exp^{u(x;\\,\\theta)}}{Z(\\theta)}$的学习问题进行了NCE和Sampled Softmax的理论介绍。考虑到实际场景中，有一类问题是条件概率预估。假设给定了一个上下文$c$，预估$x$的条件概率，既$p(x|c)$。为了与前面两篇文章中的公式尽量兼容，下面将$p(x|c)$简写为$p(x^c)$。\n\n$$\n\\begin{equation}\n\\label{softmax}\n\\tag{1}\n\\begin{aligned}\np(x^c) = \\frac{exp^{u(x^c;\\,\\theta)}}{Z(\\theta;\\,c)}\n\\end{aligned}\n\\end{equation}\n$$\n其中，$Z(\\theta;\\,c)=\\int exp^{u(x^c;\\,\\theta)}dx$，是归一化因子。\n\n在文章[TensorFlow Candidate Sampling](https://www.tensorflow.org/extras/candidate_sampling.pdf)中，很多地方直接使用了$u(x^c;\\,\\theta) = \\log \\frac{p(x^c)}{q(x^c)}$，缺乏理论支持，笔者认为不太正确，所以在本文记录下个人理解。\n\n\n# NCE\n\nNCE的学习目标是：\n$$\n\\begin{equation}\n\\label{nce_u}\n\\tag{2}\n\\begin{aligned}\n\\widehat{u}(x^c;\\,\\widehat\\theta) = u(x^c;\\,\\theta) - \\log q(x^c) + \\gamma\n\\end{aligned}\n\\end{equation}\n$$\n其中，$\\widehat\\theta = \\left \\{ \\theta, \\, \\gamma \\right \\}$是可学习参数。\n\n最终学到的概率模型是：\n$$\n\\begin{equation}\n\\label{nce_p_x_detail}\n\\tag{3}\n\\begin{aligned}\np(x) &= \\frac{exp^{u(x^c;\\,\\theta)}}{Z(\\theta;\\,c)}\n\\\\ &= exp^{u(x^c;\\,\\theta)+\\gamma+\\log k}\n\\end{aligned}\n\\end{equation}\n$$\n\nNCE将原始Softmax多分类问题转化为多个二分类问题，既$label \\in \\left \\{ 0,\\,1 \\right \\}$。二分类的odds计算如下：\n$$\n\\begin{equation}\n\\label{logodds_nce}\n\\tag{4}\n\\begin{aligned}\nodds(label|x^c) &= \\frac{p(1|x^c)}{p(0|x^c)}\n\\\\ &= \\frac{1}{1+exp^{-\\widehat{u}}}\\frac{1+exp^{-\\widehat{u}}}{exp^{-\\widehat{u}}}\n\\\\ &= exp(\\widehat{u})\n\\end{aligned}\n\\end{equation}\n$$\n那么，\n$$\n\\begin{equation}\n\\label{logodds}\n\\tag{5}\n\\begin{aligned}\n\\widehat{u} = logodds(label|x^c)\n\\end{aligned}\n\\end{equation}\n$$\n所以，NCE的$\\widehat{u}(x^c,\\,\\widehat{\\theta})$拟合的是$logodds(label|x^c)$。\n\n二分类问题的损失函数为交叉熵损失，既：\n$$\n\\begin{equation}\n\\label{nce_loss}\n\\tag{6}\n\\begin{aligned}\nL_{nce} &= -\\sum_{i=1}^{T} [ \\log p(1|x_i^c) + \\sum_{j=1}^{k} \\log p(0|x_{i,j}^c)]\n\\\\&= \\sum_{i=1}^{T} [ \\log (1 + exp^{-\\widehat{u}(x_i^c;\\,\\widehat\\theta)}) + \\sum_{j=1}^{k} \\log (1 + exp^{\\widehat{u}(x_{i,j}^c;\\,\\widehat\\theta)})]\n\\end{aligned}\n\\end{equation}\n$$\n其中，$T$是正样本集合大小，$k$是每个正样本的负采样数目。\n\n# NEG\nNEG将NCE学习目标进行简化，忽略了$\\log q(x)$，NEG的学习目标为：\n$$\n\\begin{equation}\n\\label{neg_u}\n\\tag{7}\n\\begin{aligned}\n\\widehat{u}(x^c;\\,\\widehat\\theta) = u(x;\\,\\theta) + \\gamma\n\\end{aligned}\n\\end{equation}\n$$\n其中，$\\widehat\\theta = \\left \\{ \\theta, \\, \\gamma \\right \\}$是可学习参数。\n\n最终学到的概率模型与NCE稍有不同，既：\n$$\n\\begin{equation}\n\\label{neg_p_x_detail}\n\\tag{8}\n\\begin{aligned}\np(x) &= \\frac{exp^{u(x^c;\\,\\theta)}}{Z(\\theta;\\,c)}\n\\\\ &= exp^{u(x^c;\\,\\theta)+\\gamma+\\log k+\\log q(x^c)}\n\\end{aligned}\n\\end{equation}\n$$\n\nNEG的学习目标$\\widehat{u}(x^c;\\,\\widehat\\theta)$拟合$\\log p(x^c) - log k$，既：\n$$\n\\begin{equation}\n\\label{neg_logodds}\n\\tag{9}\n\\begin{aligned}\n\\widehat{u} &= \\log\\frac{p(1|x^c)}{p(0|x^c)} + \\log q(x^c)\n\\\\ &= \\log\\frac{p(x^c)}{kq(x^c)} + \\log q(x^c)\n\\\\ &= \\log p(x^c) - log k\n\\end{aligned}\n\\end{equation}\n$$\n\n损失函数同NCE一致，既：\n$$\n\\begin{equation}\n\\label{neg_loss}\n\\tag{10}\n\\begin{aligned}\nL_{nce} &= -\\sum_{i=1}^{T} [ \\log p(1|x_i^c) + \\sum_{j=1}^{k} \\log p(0|x_{i,j}^c)]\n\\\\&= \\sum_{i=1}^{T} [ \\log (1 + exp^{-\\widehat{u}(x_i^c;\\,\\widehat\\theta)}) + \\sum_{j=1}^{k} \\log (1 + exp^{\\widehat{u}(x_{i,j}^c;\\,\\widehat\\theta)})]\n\\end{aligned}\n\\end{equation}\n$$\n\n# Sampled Softmax\nSampled Softmax的学习目标同NCE一致，既：\n$$\n\\begin{equation}\n\\label{sampled_softmax_u}\n\\tag{11}\n\\begin{aligned}\n\\widehat{u}(x^c;\\,\\widehat\\theta) = u(x^c;\\,\\theta) - \\log q(x^c) + \\gamma\n\\end{aligned}\n\\end{equation}\n$$\n其中，$\\widehat\\theta = \\left \\{ \\theta, \\, \\gamma \\right \\}$是可学习参数。\n\n最终学到的概率模型为：\n$$\n\\begin{equation}\n\\label{ssm_p_x_detail}\n\\tag{12}\n\\begin{aligned}\np(x^c) &= exp^{u(x;\\,\\theta) + \\gamma - \\log K(C)}\n\\end{aligned}\n\\end{equation}\n$$\n其中，$K(C)$是一个与当前$x$无关的常数项。\n\n多分类问题的损失函数为：\n$$\n\\begin{equation}\n\\label{ssm_loss}\n\\tag{13}\n\\begin{aligned}\nL_{ssm} &= -\\sum_{i=1}^{T} [ \\log \\frac{exp^{\\widehat{u}(x_i^c;\\,\\widehat\\theta)}}{Z(\\widehat\\theta;\\,C_i)}]\n\\\\ &= -\\sum_{i=1}^{T} [ \\widehat{u}(x_i^c;\\,\\widehat\\theta) - \\log \\sum_{x \\in C_i^c} exp^{\\widehat{u}(x;\\,\\widehat\\theta)}]\n\\end{aligned}\n\\end{equation}\n$$\n\n# 总结\n|  方法   | $\\widehat{u}$  | $p(x^c)$  | 损失函数  |\n|  ----  | ----           | ----      | ----           |\n| NCE    | $u(x^c;\\,\\theta) - \\log q(x^c) + \\gamma$ | $exp^{u(x^c;\\,\\theta)+\\gamma+\\log k}$ | $\\sum_{i=1}^{T} [ \\log (1 + exp^{-\\widehat{u}(x_i^c;\\,\\widehat\\theta)}) + \\sum_{j=1}^{k} \\log (1 + exp^{\\widehat{u}(x_{i,j}^c;\\,\\widehat\\theta)})]$ |\n| NEG    | $u(x^c;\\,\\theta) + \\gamma$ | $exp^{u(x^c;\\,\\theta)+\\gamma+\\log k+\\log q(x^c)}$ |  同NCE |\n| SSM    | 同NCE | $exp^{u(x^c;\\,\\theta) + \\gamma - \\log K(C)}$ | $-\\sum_{i=1}^{T} [ \\widehat{u}(x_i^c;\\,\\widehat\\theta) - \\log \\sum_{x \\in C_i^c} exp^{\\widehat{u}(x;\\,\\widehat\\theta)}]$ |\n\n# 参考资料\n* [TensorFlow Candidate Sampling](https://www.tensorflow.org/extras/candidate_sampling.pdf)\n* [一文搞懂Approximate Softmax：从公式到代码](https://zhuanlan.zhihu.com/p/528862933)\n* [Softmax近似方法（一） - Noise Contrastive Estimation理论详解](https://mathmach.com/b5ec3256/)\n* [Softmax近似方法（二） - Sampled Softmax理论详解](https://mathmach.com/b5ec3256/)","tags":["Numerical-Optimization","Machine-Learning"],"categories":["tech"]},{"title":"Softmax近似方法（二） - Sampled Softmax理论详解","url":"/fa54b17/","content":"\n<!-- toc -->\n\n# 背景\n我们有一个样本集合$T$，它的概率分布为$\\widetilde{p}(x)$，我们希望学习一个分布$p(x;\\,\\theta)$来逼近$\\widetilde{p}(x)$。\n$$\n\\begin{equation}\n\\label{softmax}\n\\tag{1}\n\\begin{aligned}\np(x) = \\frac{exp^{u(x;\\,\\theta)}}{Z(\\theta)}\n\\end{aligned}\n\\end{equation}\n$$\n其中，$Z(\\theta)=\\int exp^{u(x;\\,\\theta)}dx$。\n\n通过最大似然得到的log-likelihood损失函数：\n$$\n\\begin{equation}\n\\label{mle}\n\\tag{2}\n\\begin{aligned}\nL_{mle}&=-\\mathbb{E}_{x\\sim\\widetilde{p}(x)}[\\log{p(x)}]\n\\end{aligned}\n\\end{equation}\n$$\n\n接下来需要对公式$\\eqref{mle}$进行求导，这里导数计算依赖$Z(\\theta)$。但是，$Z(\\theta)$在很多情况下并不容易求得。比如，在推荐系统领域，预测用户下一次点击某个item的概率，需要在全部item上计算$Z(\\theta)$，而item数量可能为千万甚至亿级。那么，怎么解决这个问题呢？\n\n# Sampled Softmax的优化目标\nSampled Softmax的核心思想是将上述问题改造为一个类别少很多的多分类问题（注意体会这里与[Noise Contrastive Estimation理论详解](https://mathmach.com/b5ec3256/)的区别）。\n除了样本集合$T$，我们再根据一个人为设定的概率$q(x)$采样若干样本集合$S$，$S$的样本量是$T$的$k$倍。即对每个正样本，采样$k$个负样本。其中，负样本集合$S$称为噪声样本。\n这样，对于每个正样本$x_i$和它对应的$k$个负样本，共$k+1$个样本，记为集合$C_i$，从$C_i$中选出一个样本$x$，选中正样本的概率为（以下忽略$i$）：\n$$\n\\begin{equation}\n\\label{data_p_x_cond_C}\n\\tag{3}\n\\begin{aligned}\np(x|C) &= \\frac{p(x,C)}{p(C)}\n\\\\ &= \\frac{p(C|x)p(x)}{p(C)}\n\\\\ &= \\frac{p(x) \\prod_{x_i \\in C-x} q(x_i) \\prod_{x_i \\in L-C} (1-q(x_i))}{p(C)}\n\\\\ &= \\frac{p(x)}{q(x)}\\frac{\\prod_{x_i \\in C} q(x_i) \\prod_{x_i \\in L-C} (1-q(x_i))}{p(C)}\n\\\\ &= \\frac{p(x)}{q(x)}K(C)\n\\end{aligned}\n\\end{equation}\n$$\n其中，$L$是全部样本集合，$p(C|x)是从$C-x$中选出全部剩余样本，并且$L-C$中选出0个样本的概率乘积。$K(C)$与当前$x$无关，是常数项。其实，就是用小样本集合下的$p(x|C)$去近似拟合全局的$p(x)$。\n\n将公式$\\eqref{softmax}$代入，进一步可得：\n$$\n\\begin{equation}\n\\label{data_p_x_cond_C_detail}\n\\tag{4}\n\\begin{aligned}\np(x|C) &= \\frac{exp^{u(x;\\,\\theta)}}{Z(\\theta)}\\frac{K(C)}{q(x)}\n\\\\ &= exp^{u(x;\\,\\theta) - \\log q(x) + \\log K(C) - \\log Z(\\theta)}\n\\\\ &\\propto exp^{u(x;\\,\\theta) - \\log q(x) + \\gamma}\n\\\\ &= exp^{\\widehat{u}(x;\\,\\widehat\\theta)}\n\\end{aligned}\n\\end{equation}\n$$\n其中，\n$$\n\\begin{equation}\n\\label{u_new}\n\\tag{5}\n\\begin{aligned}\n\\widehat{u}(x;\\,\\widehat\\theta)=u(x;\\,\\theta) - \\log q(x) + \\gamma\n\\\\\n\\gamma = \\log K(C) - \\log Z(\\theta)\n\\end{aligned}\n\\end{equation}\n$$\n这里把$Z(\\theta)、\\,K(C)$将它变为一个可学习参数$\\gamma$，总的学习参数为$\\widehat\\theta = \\left \\{ \\theta, \\, \\gamma \\right \\}$。可以看到，$\\widehat{u}$的形式与NCE一样。变为可学习参数后，不能保证概率之和为1，既概率没有归一化，所以公式第三行变成近似，归一化后的概率变为：\n$$\n\\begin{equation}\n\\label{data_p_x_cond_C_detail_normal}\n\\tag{6}\n\\begin{aligned}\np(x|C) &= \\frac{exp^{\\widehat{u}(x;\\,\\widehat\\theta)}}{\\sum_{x_i \\in C} exp^{\\widehat{u}(x_i;\\,\\widehat\\theta)}}\n\\\\ &= \\frac{exp^{\\widehat{u}(x;\\,\\widehat\\theta)}}{Z(\\widehat\\theta;\\,C)}\n\\end{aligned}\n\\end{equation}\n$$\n\n最后，我们的优化目标为最大化选中正样本的概率，损失函数既：\n$$\n\\begin{equation}\n\\label{sampled_softmax_loss}\n\\tag{7}\n\\begin{aligned}\nL_{ssm}&=-\\mathbb{E}_{x\\sim\\widetilde{p}(x)}[\\log{p(x|C)}]\n\\end{aligned}\n\\end{equation}\n$$\n\n损失函数进一步可以写成以下形式：\n$$\n\\begin{equation}\n\\label{sampled_softmax_loss_implemant}\n\\tag{8}\n\\begin{aligned}\nL_{ssm\\,implementation} &= -\\sum_{i=1}^{T} [ \\log \\frac{exp^{\\widehat{u}(x_i;\\,\\widehat\\theta)}}{Z(\\widehat\\theta;\\,C_i)}]\n\\\\ &= -\\sum_{i=1}^{T} [ \\widehat{u}(x_i;\\,\\widehat\\theta) - \\log \\sum_{x \\in C_i} exp^{\\widehat{u}(x;\\,\\widehat\\theta)}]\n\\end{aligned}\n\\end{equation}\n$$\n其中，$N$是正样本集合$T$的样本数量，$C_i$是正样本$x_i$对应的正样本加负采样集合。\n\n# Sampled Softmax优化目标下的概率模型\n\n上文我们只定义了$p(x|C)$，那么$p(x)$是什么样呢？\n\n可得：\n$$\n\\begin{equation}\n\\label{p_x}\n\\tag{9}\n\\begin{aligned}\np(x) &= \\frac{exp^{u(x;\\,\\theta)}}{Z(\\theta)}\n\\\\ &= exp^{u(x;\\,\\theta) + \\gamma - \\log K(C)}\n\\end{aligned}\n\\end{equation}\n$$\n与公式$\\eqref{softmax}$相比，难以计算的$Z(\\theta)$没有了，代替以可学习偏差项$\\gamma$和$\\log K(C)$。\n\n# Sampled Softmax效果的理论分析\nSampled Softmax通过优化目标和概率模型的调整，达到了计算可行性。但是，能够保证学习出的概率模型没有跑偏吗，既$p(x)$能近似$\\widetilde{p}(x)$吗？\n\n## 概率模型收敛性分析\n$$\n\\begin{equation}\n\\tag{10}\n\\begin{aligned}\nL_{ssm} &= -\\mathbb{E}_{x\\sim\\widetilde{p}(x)}[\\log{p(x|C)}]\n\\\\ &= -\\int{\\widetilde{p}(x) \\log p(x|C)} dx\n\\\\ &= -\\int {\\widetilde{p}(x) (\\log p(x) + \\log K(C) - \\log q(x))} dx\n\\\\ &= -\\int {\\widetilde{p}(x) \\log p(x)} dx + \\int {\\log q(x) - \\log K(C)} dx\n\\\\ &= -\\mathbb{E}_{x\\sim\\widetilde{p}(x)}[\\log{p(x)}] + \\int {\\log q(x) - \\log K(C)} dx\n\\\\ &= L_{mle} + \\int {\\log q(x) - \\log K(C)} dx\n\\end{aligned}\n\\end{equation}\n$$\n其中，$K(C)$与$\\theta$有关，所以最小化$L_{ssm}$与最小化$L_{mle}$有偏差，既：\n$$\n\\begin{equation}\n\\tag{11}\n\\begin{aligned}\np(x)_{min\\,L_{ssm}} \\ne p(x)_{min\\,L_{mle}} = \\widetilde{p}(x)\n\\end{aligned}\n\\end{equation}\n$$\n\n## 梯度值分析\n[Sampled Softmax with Random Fourier Features](https://arxiv.org/pdf/1907.10747.pdf)给出了梯度偏差的上下界，既$L_{ssm}$的梯度是有偏的。\n$$\n\\begin{equation}\n\\tag{12}\n\\begin{aligned}\n\\mathbb{E}_{x\\sim\\widetilde{p}(x)}[\\frac{\\partial}{\\partial \\theta} L_{ssm}] \\ne \\frac{\\partial}{\\partial \\theta} L_{mle}\n\\end{aligned}\n\\end{equation}\n$$\n\n# 参考资料\n* [一文搞懂Approximate Softmax：从公式到代码](https://zhuanlan.zhihu.com/p/528862933)\n* [TensorFlow Candidate Sampling](https://www.tensorflow.org/extras/candidate_sampling.pdf)\n* [Softmax近似方法（一） - Noise Contrastive Estimation理论详解](https://mathmach.com/b5ec3256/)\n* [Sampled Softmax with Random Fourier Features](http://arxiv.org/abs/1907.10747)\n* [Adaptive Importance Sampling to Accelerate Training of a Neural Probabilistic Language Model](http://ieeexplore.ieee.org/document/4443871/)","tags":["Numerical-Optimization","Machine-Learning","Deep-Learning"],"categories":["tech"]},{"title":"Softmax近似方法（一） - Noise Contrastive Estimation理论详解","url":"/b5ec3256/","content":"\n<!-- toc -->\n\n# 背景\n我们有一个样本集合$T$，它的概率分布为$\\widetilde{p}(x)$，我们希望学习一个分布$p(x;\\,\\theta)$来逼近$\\widetilde{p}(x)$。\n$$\n\\begin{equation}\n\\label{softmax}\n\\tag{1}\n\\begin{aligned}\np(x) = \\frac{exp^{u(x;\\,\\theta)}}{Z(\\theta)}\n\\end{aligned}\n\\end{equation}\n$$\n其中，$Z(\\theta)=\\int exp^{u(x;\\,\\theta)}dx$。\n\n通过最大似然得到的log-likelihood损失函数：\n$$\n\\begin{equation}\n\\label{mle}\n\\tag{2}\n\\begin{aligned}\nL_{mle}&=-\\mathbb{E}_{x\\sim\\widetilde{p}(x)}[\\log{p(x)}]\n\\end{aligned}\n\\end{equation}\n$$\n\n接下来需要对公式$\\eqref{mle}$进行求导，这里导数计算依赖$Z(\\theta)$。但是，$Z(\\theta)$在很多情况下并不容易求得。比如，在推荐系统领域，预测用户下一次点击某个item的概率，需要在全部item上计算$Z(\\theta)$，而item数量可能为千万甚至亿级。那么，怎么解决这个问题呢？\n\n# NCE的优化目标\nNCE的核心思想是将上述问题改造为多个二分类问题。\n除了样本集合$T$，我们再根据一个人为设定的概率$q(x)$采样若干样本集合$S$，$S$的样本量是$T$的$k$倍。即对每个正样本，采样$k$个负样本。其中，负样本集合$S$称为噪声样本。\n这样，对于每个正样本和它对应的$k$个负样本，可以生成$k+1$个二分类任务。$k+1$个样本中的每个样本$x$，如果它属于$T$则为正类，$label=1$，如果属于$S$则为负类，$label=0$。\n得到以下概率：\n$$\n\\begin{equation}\n\\label{data_p_1_x}\n\\tag{3}\n\\begin{aligned}\n\\widetilde{p}(label=1,x)&=p(x|label=1)p(label=1)\n\\\\&=\\widetilde{p}(x)p(label=1)\n\\\\&=\\frac{1}{k+1}\\widetilde{p}(x)\n\\\\\n\\\\\n\\widetilde{p}(label=0,x)&=p(x|label=0)p(label=0)\n\\\\&=q(x)p(label=0)\n\\\\&=\\frac{k}{k+1}q(x)\n\\end{aligned}\n\\end{equation}\n$$\n那么，条件概率分布为：\n$$\n\\begin{equation}\n\\label{data_p_1_cond_x}\n\\tag{4}\n\\begin{aligned}\n\\widetilde{p}(label=1|x)&=\\frac{\\widetilde{p}(label=1,x)}{\\widetilde{p}(label=1,x)+\\widetilde{p}(label=0,x)}\n\\\\&=\\frac{\\widetilde{p}(x)}{\\widetilde{p}(x)+kq(x)}\n\\\\\n\\\\\n\\widetilde{p}(label=0|x)&=1-p(label=1|x)\n\\\\&=\\frac{kq(x)}{\\widetilde{p}(x)+kq(x)}\n\\end{aligned}\n\\end{equation}\n$$\n\n我们希望学习一个分布$p(x)$来逼近$\\widetilde{p}(x)$，既：\n$$\n\\begin{equation}\n\\label{p_1_cond_x}\n\\tag{5}\n\\begin{aligned}\np(label=1|x;\\,\\theta)=\\frac{p(x)}{p(x)+kq(x)}\n\\\\\n\\\\\np(label=0|x;\\,\\theta)=\\frac{kq(x)}{p(x)+kq(x)}\n\\end{aligned}\n\\end{equation}\n$$\n下文简写为$p(1|x)$和$p(0|x)$。\n\n$p(1|x)$是二分类问题，我们将它继续推导为如下形式：\n$$\n\\begin{equation}\n\\label{p_1_cond_x_detail}\n\\tag{6}\n\\begin{aligned}\np(1|x)&=\\frac{p(x)}{p(x)+kq(x)}\n\\\\&=\\frac{1}{1+\\frac{kq(x)}{p(x)}}\n\\\\&=\\frac{1}{1+exp^{\\log kq(x) - \\log p(x)}}\n\\\\&=\\frac{1}{1+exp^{\\log kq(x) - u(x;\\,\\theta) + \\log Z(\\theta)}}\n\\\\&=\\frac{1}{1+exp^{-\\widehat{u}(x;\\,\\widehat\\theta)}}\n\\end{aligned}\n\\end{equation}\n$$\n其中，\n$$\n\\begin{equation}\n\\label{u_new}\n\\tag{7}\n\\begin{aligned}\n\\widehat{u}(x;\\,\\widehat\\theta)=u(x;\\,\\theta) - \\log q(x) + \\gamma\n\\\\\n\\gamma = - \\log Z(\\theta) - \\log k\n\\end{aligned}\n\\end{equation}\n$$\n这里我们把$Z(\\theta)$变成可学习参数，所以总的参数变为$\\widehat\\theta = \\left \\{ \\theta, \\, \\gamma \\right \\}$。\n\n最后，我们的优化目标为交叉熵损失：\n$$\n\\begin{equation}\n\\label{nce}\n\\tag{8}\n\\begin{aligned}\nL_{nce}&=-\\mathbb{E}_{x\\sim\\widetilde{p}(x)}[\\log{p(1|x)}]-k\\mathbb{E}_{x\\sim q(x)}[\\log(p(0|x))]\n\\end{aligned}\n\\end{equation}\n$$\n\n损失函数进一步可以写成以下形式：\n$$\n\\begin{equation}\n\\label{nce_loss_implement}\n\\tag{9}\n\\begin{aligned}\nL_{nce\\,implementation} &= -\\frac{1}{N} \\sum_{i=1}^{N} [ \\log p(1|x_i) + \\sum_{j=1}^{k} \\log p(0|x_{i,j})]\n\\\\&= \\frac{1}{N} \\sum_{i=1}^{N} [ \\log (1 + exp^{-\\widehat{u}(x_i;\\,\\widehat\\theta)}) + \\sum_{j=1}^{k} \\log (1 + exp^{\\widehat{u}(x_{i,j};\\,\\widehat\\theta)})]\n\\end{aligned}\n\\end{equation}\n$$\n其中，$N$是正样本集合$T$的样本数量。\n\n# NCE优化目标下的概率模型\n上文我们只定义了$p(label|x)$，那么$p(x)$是什么样呢？\n可得：\n$$\n\\begin{equation}\n\\label{p_x_detail}\n\\tag{10}\n\\begin{aligned}\np(x) &= \\frac{exp^{u(x;\\,\\theta)}}{Z(\\theta)}\n\\\\ &= exp^{u(x;\\,\\theta)+\\gamma+\\log k}\n\\end{aligned}\n\\end{equation}\n$$\n与公式$\\eqref{softmax}$相比，难以计算的$Z(\\theta)$没有了，代替以可学习偏差项$\\gamma$。在实际实现中$k$是个固定值，$q(x)$的选取往往也比较简单，$\\log q(x)$也可以变成常数项。另外，为了保证$\\int p(x)dx=1$，需要通过模型来学习达到这一目的，通过梯度下降迭代求解无法完全保证这一点。在实际使用中，一般得到logit的值$u(x;\\,\\theta)$或更前面$x$的向量表示即可，$p(x)$是否能归一化并不影响使用，所以问题不大。\n\n\n# NCE效果的理论分析\nNCE通过优化目标和概率模型的调整，达到了计算可行性。但是，能够保证学习出的概率模型没有跑偏吗，既$p(x)$能近似$\\widetilde{p}(x)$吗？\n\n## 概率模型收敛性分析\n首先分析$p(x)$在$L_{nce}$下收敛到哪里？\n$$\n\\begin{equation}\n\\tag{11}\n\\begin{aligned}\nL_{nce}&=-\\mathbb{E}_{x\\sim\\widetilde{p}(x)}[\\log{p(1|x)}]-k\\mathbb{E}_{x\\sim q(x)}[\\log(p(0|x))]\n\\\\&=-\\int{\\widetilde{p}(x)\\log{p(1|x)}}dx-\\int{kq(x)\\log{p(0|x)}}dx\n\\\\&=\\int{-\\widetilde{p}(x)\\log{p(1|x)}-kq(x)\\log{p(0|x)}}dx\n\\end{aligned}\n\\end{equation}\n$$\n因为$\\widetilde{p}(x)、q(x)$与参数$\\theta$无关，所以以上优化目标中增加两项后不影响最终$\\theta$，可以写成：\n$$\n\\begin{equation}\n\\label{kl_theory}\n\\tag{12}\n\\begin{aligned}\nL_{\\widehat{nce}}&=\\int{-\\widetilde{p}(x)\\log{p(1|x)}-kq(x)\\log{p(0|x)}+\\widetilde{p}(x)\\log{\\widetilde{p}(1|x)}+kq(x)\\log{\\widetilde{p}(0|x)}}dx\n\\\\&=\\int{\\widetilde{p}(x)\\log{\\frac{\\widetilde{p}(1|x)}{p(1|x)}}+kq(x)\\log{\\frac{\\widetilde{p}(0|x)}{p(0|x)}}}dx\n\\\\&=\\int{(\\widetilde{p}(x)+kq(x))(\\frac{\\widetilde{p}(x)}{\\widetilde{p}(x)+kq(x)}\\log{\\frac{\\widetilde{p}(1|x)}{p(1|x)}}+\\frac{kq(x)}{\\widetilde{p}(x)+kq(x)}\\log{\\frac{\\widetilde{p}(0|x)}{p(0|x)}})}dx\n\\\\&=\\int{(\\widetilde{p}(x)+kq(x))(\\widetilde{p}(1|x)\\log{\\frac{\\widetilde{p}(1|x)}{p(1|x)}}+\\widetilde{p}(0|x)\\log{\\frac{\\widetilde{p}(0|x)}{p(0|x)}})}dx\n\\\\&=\\int{(\\widetilde{p}(x)+kq(x))KL(\\widetilde{p}(label|x)||p(label|x))}dx\n\\end{aligned}\n\\end{equation}\n$$\n\n已知KL散度非负，所以当$p(label|x)=\\widetilde{p}(label|x)$时，$L_{\\widehat{nce}}$达到最小值。此时，\n$$\n\\begin{equation}\n\\tag{13}\n\\begin{aligned}\n\\frac{p(x)}{p(x)+kq(x)}=\\frac{\\widetilde{p}(x)}{\\widetilde{p}(x)+kq(x)}\n\\end{aligned}\n\\end{equation}\n$$\n既：\n$$\n\\begin{equation}\n\\tag{14}\n\\begin{aligned}\np(x)=\\widetilde{p}(x)\n\\end{aligned}\n\\end{equation}\n$$\n这样可以理论上保证$p(x)$收敛到$\\widetilde{p}(x)$，$L_{nce}$优化目标能够保证最优解情况下的$p(x)$没有跑偏。\n\n## 梯度值分析\n概率模型收敛性分析只能保证在全局函数空间，可以找到最优$p(x)$。但是，工程实现时，$p(x)=exp^{u(x;\\,\\theta)+\\gamma+\\log k}$将函数空间做了限制，能保证效果吗？\n\n因为需要通过梯度下降进行迭代求解，所以这里通过两者的梯度进行对比。\n首先对$L_{mle}$进行求导：\n$$\n\\begin{equation}\n\\tag{15}\n\\begin{aligned}\n\\frac{\\partial}{\\partial \\theta}L_{mle}&=-\\mathbb{E}_{x\\sim\\widetilde{p}(x)}[\\frac{\\partial}{\\partial \\theta} \\log{p(x)}]\n\\\\&=-\\mathbb{E}_{x\\sim \\widetilde{p}(x)}[\\frac{\\partial u(x;\\,\\theta)}{\\partial \\theta}] + \\frac{\\partial}{\\partial \\theta}\\log{Z(\\theta)}\n\\\\&=-\\mathbb{E}_{x\\sim \\widetilde{p}(x)}[\\frac{\\partial u(x;\\,\\theta)}{\\partial \\theta}] + \\frac{1}{Z(\\theta)}\\frac{\\partial Z(\\theta)}{\\partial \\theta}\n\\\\&=-\\mathbb{E}_{x\\sim \\widetilde{p}(x)}[\\frac{\\partial u(x;\\,\\theta)}{\\partial \\theta}] + \\frac{1}{Z(\\theta)}\\frac{\\partial}{\\partial \\theta} \\int exp(u(x;\\,\\theta))dx\n\\\\&=-\\mathbb{E}_{x\\sim \\widetilde{p}(x)}[\\frac{\\partial u(x;\\,\\theta)}{\\partial \\theta}] + \\frac{1}{Z(\\theta)} \\int \\frac{\\partial}{\\partial \\theta} exp(u(x;\\,\\theta))dx\n\\\\&=-\\mathbb{E}_{x\\sim \\widetilde{p}(x)}[\\frac{\\partial u(x;\\,\\theta)}{\\partial \\theta}] + \\frac{1}{Z(\\theta)} \\int exp(u(x;\\,\\theta))\\frac{\\partial u(x;\\,\\theta)}{\\partial \\theta} dx\n\\\\&=-\\mathbb{E}_{x\\sim \\widetilde{p}(x)}[\\frac{\\partial u(x;\\,\\theta)}{\\partial \\theta}] + \\int p(x)\\frac{\\partial u(x;\\,\\theta)}{\\partial \\theta} dx\n\\\\&=-\\int \\widetilde{p}(x)\\frac{\\partial u(x;\\,\\theta)}{\\partial \\theta}dx + \\int p(x)\\frac{\\partial u(x;\\,\\theta)}{\\partial \\theta} dx\n\\\\&=\\int (p(x)-\\widetilde{p}(x))\\frac{\\partial u(x;\\,\\theta)}{\\partial \\theta} dx\n\\end{aligned}\n\\end{equation}\n$$\n\n然后对$L_{nce}$进行求导：\n$$\n\\begin{equation}\n\\label{l_nce_gradient}\n\\tag{16}\n\\begin{aligned}\n\\frac{\\partial}{\\partial \\theta}L_{nce}&=-\\mathbb{E}_{x\\sim\\widetilde{p}(x)}[\\frac{\\partial}{\\partial \\theta} \\log{p(1|x)}]-k\\mathbb{E}_{x\\sim q(x)}[\\frac{\\partial}{\\partial \\theta} \\log{p(0|x)}]\n\\\\&=-\\mathbb{E}_{x\\sim\\widetilde{p}(x)}[\\frac{kq(x)}{p(x)(p(x)+kq(x))}\\frac{\\partial p(x)}{\\partial \\theta}] + k\\mathbb{E}_{x\\sim q(x)}[\\frac{1}{p(x)+kq(x)}\\frac{\\partial p(x)}{\\partial \\theta}]\n\\\\&=\\int \\frac{kq(x)}{p(x)+kq(x)}\\frac{\\partial p(x)}{\\partial \\theta}-\\frac{\\widetilde{p}(x)kq(x)}{p(x)(p(x)+kq(x))}\\frac{\\partial p(x)}{\\partial \\theta}dx\n\\\\&=\\int \\frac{kq(x)}{p(x)+kq(x)}(p(x)-\\widetilde{p}(x))\\frac{\\partial \\log p(x)}{\\partial \\theta}dx\n\\\\&=\\int \\frac{kq(x)}{p(x)+kq(x)}(p(x)-\\widetilde{p}(x))\\frac{\\partial (u(x;\\,\\theta) + \\gamma + \\log k)}{\\partial \\theta}dx\n\\\\&=\\int \\frac{kq(x)}{p(x)+kq(x)}(p(x)-\\widetilde{p}(x))\\frac{\\partial u(x;\\,\\theta)}{\\partial \\theta}dx\n\\end{aligned}\n\\end{equation}\n$$\n$$\n\\begin{equation}\n\\tag{17}\n\\begin{aligned}\n\\frac{\\partial}{\\partial \\gamma}L_{nce}&=\\int \\frac{kq(x)}{p(x)+kq(x)}(p(x)-\\widetilde{p}(x))\\frac{\\partial (u(x;\\,\\theta) + \\gamma + \\log k)}{\\partial \\gamma}dx\n\\\\&=\\int \\frac{kq(x)}{p(x)+kq(x)}(p(x)-\\widetilde{p}(x))dx\n\\end{aligned}\n\\end{equation}\n$$\n\n可以看出，$\\frac{\\partial}{\\partial \\theta} L_{nce}$比$\\frac{\\partial}{\\partial \\theta} L_{mle}$仅多了一项$\\frac{kq(x)}{p(x)+kq(x)}$。当$k \\rightarrow \\infty$时，$\\frac{\\partial}{\\partial \\theta} L_{nce}$等于$\\frac{\\partial}{\\partial \\theta} L_{mle}$，既：\n$$\n\\begin{equation}\n\\tag{18}\n\\begin{aligned}\n\\lim_{k \\to \\infty} \\frac{\\partial}{\\partial \\theta}L_{nce} = \\frac{\\partial}{\\partial \\theta}L_{mle}\n\\end{aligned}\n\\end{equation}\n$$\n\n到这里可以看到，虽然NCE学习出的概率模型形式$\\eqref{p_x_detail}$与原始Softmax形式$\\eqref{softmax}$有区别，但通过优化目标的调整，在极限条件下能够在梯度下降过程中近似MLE的效果。并且，$\\frac{kq(x)}{p(x)+kq(x)}$在实际使用时往往接近1，导致$\\frac{\\partial}{\\partial \\theta} L_{nce}$和$\\frac{\\partial}{\\partial \\theta} L_{mle}$的差别并不大，在非极限条件下也能够有很好的效果。\n\n# NCE的变体\n\n## NEG\n$\\widehat{u}(x;\\,\\widehat\\theta)=u(x;\\,\\theta) - \\log q(x) + \\gamma$是标准NCE下的logit，$\\log q(x)$在此起了修正作用。实际的工作中，比如推荐算法中常用的一个变体Negative Sampling（NEG）方法，忽略掉了$\\log q(x)$，只使用$\\widehat{u}(x;\\,\\widehat\\theta)=u(x;\\,\\theta) + \\gamma$，并且采用相同的损失函数$L_{neg}=L_{nce}$，获得了不错的效果，那么NEG是否有理论保证呢？\n从上文的NCE概率模型收敛性分析$\\eqref{kl_theory}$中可以看出，不管$\\widehat{u}(x;\\,\\widehat\\theta)$任何形式，$L_{nce}$都可以保证$p(x)$收敛到$\\widetilde{p}(x)$。从NCE梯度值分析$\\eqref{l_nce_gradient}$中亦可得出，只要$\\widehat{u}(x;\\,\\widehat\\theta)=u(x;\\,\\theta)+anyValue$，就能保证梯度与原问题近似。这里，$anyValue$可以是可学习的$\\gamma$或者人工设定的固定值，并且固定值不能太大或太小，否则会把$u(x;\\,\\theta)$推到一个很大或很小的值域，造成训练中的数值问题。总之，NEG与NCE一样，也有理论保证，所以可以得到媲美NCE的效果。\n\n## $\\gamma$的取舍\n$\\widehat{u}(x;\\,\\widehat\\theta)=u(x;\\,\\theta) + \\gamma$里面的$\\gamma$可以去掉吗？\n如果$u(x;\\,\\theta)$的范围很大，可以去掉$\\gamma$，只学习$\\theta$，就能保证$u(x;\\,\\theta)$覆盖$u(x;\\,\\theta)+\\gamma$的尺度空间。如果$u(x;\\,\\theta)$的范围受限，比如推荐算法里用两个向量的$cos(vec_{1},\\,vec_{2}) \\in \\left[ -1,\\, 1 \\right]$作为$u(x;\\,\\theta)$，为了保证$\\widehat{u}(x;\\,\\widehat\\theta)$的范围足够大，建议保留可学习的参数$\\gamma$。工业界另一种常用方式是使用温度系数$\\frac{u(x;\\,\\theta)}{temperature}$将取值范围放大，也是有理论依据的。总之，要保证$\\eqref{p_x_detail}$能够覆盖区间$\\left[ 0,\\, 1 \\right]$。\n\n# 参考资料\n* [“噪声对比估计”杂谈：曲径通幽之妙](https://kexue.fm/archives/5617)\n* [Lei Mao Blog - Noise Contrastive Estimation](https://leimao.github.io/article/Noise-Contrastive-Estimation/)\n* [Notes on Noise Contrastive Estimation and Negative Sampling](http://arxiv.org/abs/1410.8251)\n* [Noise-contrastive estimation: A new estimation principle for unnormalized statistical models](https://proceedings.mlr.press/v9/gutmann10a.html)","tags":["Numerical-Optimization","Machine-Learning","Deep-Learning"],"categories":["tech"]},{"title":"TensorFlow多Worker Barrier同步实现","url":"/b2f65b04/","content":"\n<!-- toc -->\n\n## 背景\n同步机制在TensorFlow等分布式机器学习框架中非常重要，比如TensorFlow有以下场景需要做同步：\n* 当chief worker训练完一轮后，保存模型前需要等所有worker都完成再保存模型。\n* BSP方式的SGD训练，需要每个batch做同步。\n\n如果不做同步可能会出现如下问题：\n* TensorFlow大部分使用方案都是异步SGD，而且使用global_step做停止条件，不能保证所有worker负责的数据训练相同的轮数，速度快的worker所负责的数据将会获得更多step。\n* chief worker结束时会保存模型参数，但还存在其他worker没结束，所以模型没有完全训练完整。\n\n最优的方式应该是这样：\n```\nepoch = 0\nwhile epoch < max_epoch:\n  train_one_epoch # 跑一轮数据\n  barrier # 卡在这里，等所有worker都跑完一轮数据\n  save checkpoint # 保存这一轮的模型\n  do evaluation # 跑一遍验证集数据\n  epoch++ # 进入下一轮\n```\n那怎样实现barrier机制呢？\n\n## Barrier机制实现\n具体原理就是在`PS:0`节点上添加和worker数目一样的一组计数变量`counter_vars`，初始化时为0，每当worker结束一轮训练后，将自己的worker_index对应的`counter_var`增加1，然后依次遍历其他worker对应的`counter_var`，直到所有worker的`counter_var`都等于1说明所有worker都完成这一轮训练了，然后就可以进入下一轮训练。当然，barrier也可以用于其他任意同步的方式，比如退出时也可以加个barrier，等所有worker都结束后保存模型再退出。\n```python\nclass Barrier(object):\n  def __init__(self, worker_num, barrier_num, sleep_time_ms=10):\n    self._worker_num = worker_num\n    self._barrier_num = barrier_num\n    self._sleep_time_ms = sleep_time_ms\n    self._counter_vars = []\n    self._counter_add_ops = []\n    self._counter_reset_ops = []\n    ps_device = '/job:ps/task:0/cpu:0'\n    with tf.device(ps_device):\n      for i in range(self._barrier_num):\n        for j in range(self._worker_num):\n          counter_var = tf.get_variable(\n            'counter-{}_{}'.format(i, j),\n            (),\n            tf.int32,\n            initializer=tf.zeros_initializer\n          )\n          self._counter_vars.append(counter_var)\n          self._counter_add_ops.append(counter_var.assign_add(1, use_locking=True))\n          self._counter_reset_ops.append(counter_var.assign(0, use_locking=True))\n\n  def barrier_reset(self, session, worker_index, barrier_index):\n    index = barrier_index * self._worker_num + worker_index\n    session.run(self._counter_reset_ops[index])\n\n  def barrier(self, session, worker_index, barrier_index, epoch):\n    for task_index in range(self._worker_num):\n      if task_index == worker_index:\n        session.run(self._counter_add_ops[barrier_index * self._worker_num + worker_index])\n      index = barrier_index * self._worker_num + task_index\n      count = session.run(self._counter_vars[index])\n      retry_num = 0\n      while count < epoch:\n        time.sleep(self._sleep_time_ms)\n        retry_num += 1\n        count = session.run(self._counter_vars[index])\n        if retry_num == 1:\n          tf.logging.info(\"{} wait for {}_{} to be completed\".format(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()), task_index))\n```\n\n## 训练代码\n首先不能使用QueueRunner读数据，因为它无法实现按轮次读取，需要使用DataSet来读取数据，保证worker知道每轮数据读完了。\n```python\ndef _parse_function(example_proto):\n  features = {}\n  features['label'] = tf.FixedLenFeature([], tf.float32)\n  features['feature'] = tf.FixedLenFeature([100], tf.int64)\n  instance = tf.parse_example(example_proto, features)\n  label = instance['label']\n  feature = instance['feature']\n  return label, feature\n\nif job_name == 'ps':\n  with tf.device('/cpu:0'):\n    server.join()\nelif job_name == 'worker':\n  with tf.device(param_server_device):\n    # dataset input\n    dataset = tf.data.TFRecordDataset(file_name)\n    dataset = dataset.prefetch(buffer_size=batch_size*100)\n    dataset = dataset.shuffle(buffer_size=batch_size*10)\n    dataset = dataset.batch(batch_size)\n    dataset = dataset.map(_parse_function, num_parallel_calls=4)\n    train_iterator = dataset.make_initializable_iterator()\n    train_label, train_feature = train_iterator.get_next()\n\n    # forward pass\n    model = ...\n    train_logits = model.forward(train_feature)\n\n    # loss\n    train_label = tf.to_int64(train_label)\n    train_cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n            logits=train_logits, labels=train_label)\n    train_loss = tf.reduce_mean(train_cross_entropy, name='loss')\n\n    # optimizer\n    opt = tf.train.AdamOptimizer()\n    train_op = opt.minimize(train_loss)\n\n    # barrier\n    barrier_op = barrier.Barrier(self.num_worker, 2) # 下面需要两处做barrier，所以为2\n\n    # job process\n    with tf.Session() as sess:\n      # init\n      sess.run([tf.global_variables_initializer(), tf.local_variables_initializer()])\n\n      # training process\n      epoch_num = 0\n      barrier_op.barrier(sess, self.task_index, 0, epoch_num) # 等所有worker都启动再开始训练\n      while epoch_num < max_epoch:\n        sess.run(train_iterator.initializer) # 每轮开始先初始化数据\n        while True:\n          try:\n            sess.run(train_op)\n          except tf.errors.OutOfRangeError:\n            break\n        barrier_op.barrier(sess, self.task_index, 1, epoch_num) # 等所有worker结束这轮训练\n        #保存这一轮的checkpoint\n        epoch_num += 1 # 进入下一轮\n```","tags":["TensorFlow"],"categories":["tech"]},{"title":"TensorFlow分布式任务DataSet卡住问题","url":"/97b8f613/","content":"\n<!-- toc -->\n\n## 背景\n2018年，公司的分布式模型训练普遍向`TensorFlow on Yarn`迁移。在公司的Hadoop集群上，使用TensorFlow通过DataSet读数据方式进行分布式训练时，在每个Epoch的最后一个Batch会卡住，导致任务一直停在那里无法结束。集群节点都是`CentOS, linux kernel 3.10.0`。如果用老的Queue读取数据不会出现这个问题，并且这个问题不是必现，只有在分布式且节点比较多的时候发生的概率比较高。\n\n## 复现条件\n开始时我尝试将把线程数`inter_op_parallelism_threads`和`intra_op_parallelism_threads`都设为1，并且PS节点数等于1的话，一定不会出现这个问题。\n```\nserver_config = tf.ConfigProto(inter_op_parallelism_threads=1,\n                               intra_op_parallelism_threads=1)\nself.server = tf.train.Server(self.cluster, job_name=self.job_name,\n                              task_index=self.task_index, config=server_config)\n```\n经过多次尝试，发现只有在`inter_op_parallelism_threads`大于1并且PS节点数大于1的情况下才会偶发，其他条件下一定不会出现，和`intra_op_parallelism_threads`的设置没有关系。\n\n\n出现问题的代码，是最常用的使用方式。\n```\ndef data_iter(batch_size=1000):\n  def _parse_function(examples):\n    features = {}\n    features['label'] = tf.FixedLenFeature([1], tf.float32)\n    features['user_id'] = tf.FixedLenFeature([1], tf.int64)\n    features['item_id'] = tf.FixedLenFeature([1], tf.int64)\n    instance = tf.parse_example(examples, features)\n    return instance['user_id'], instance['item_id'], instance['label']\n  with tf.name_scope('input'):\n    files = tf.data.Dataset.list_files('./inputs/part-*')\n    dataset = files.apply(tf.contrib.data.parallel_interleave(\n                        lambda file: tf.data.TFRecordDataset(file),\n                        cycle_length=4, sloppy=True))\n    dataset = dataset.prefetch(buffer_size=batch_size*2)\n    dataset = dataset.batch(batch_size)\n    dataset = dataset.map(_parse_function, num_parallel_calls=4)\n    iterator = dataset.make_initializable_iterator()\n    return iterator\n\ndef model(user_id, item_id):\n  ...\n  user_embedding = tf.embedding_lookup(user_id)\n  item_embedding == tf.embedding_lookup(item_id)\n  return tf.reduce_sum(query_embedding * item_embedding, 1, keep_dims=True)\n\n# 图构造部分\ntrain_iterator = data_iter(FLAGS.batch_size)\ntrain_user_id, train_item_id, train_label = train_iterator.get_next()\ntrain_score = model(train_user_id, train_item_id)\ntrain_loss = some_loss_function(train_score, train_label)\nopt = tf.train.AdamOptimizer(learning_rate=0.001)\ntrain_op = opt.minimize(train_loss)\n\n# 执行部分\nwith tf.Session(...) as sess:\n  ...\n  while True:\n    try:\n      sess.run(train_op)\n    except tf.errors.OutOfRangeError:\n      break\n```\n\n## 解决方案\n从问题表象来看，是`DataSet`多线程读取数据的问题，看了一段时间TensorFlow这部分的代码，没有找到解决方案。但是，从使用层面来看，DataSet主要是解耦了数据读取和模型执行部分，因为大部分模型来说，数据部分往往是瓶颈，两部分解耦后并行执行可以大大提高训练的吞吐效率。另外，session执行时直接执行最终的`train_op`，`train_op`依赖的数据读取op会自动执行。\n```\nsess.run(train_op)\n```\n所以直觉上，我怀疑很可能是模型部分和读数据部分在线程协调上出了问题。通过将读数据和模型执行两部分强制分开，问题果然消失了。\n```\nfeature_value = sess.run([feature_dataset_next_iter])\nsess.run(train_op, feed_dict={feature_placeholder: feature_value})\n```\n\n完整的代码如下。\n```\ndef data_iter(batch_size=1000):\n  def _parse_function(examples):\n    features = {}\n    features['label'] = tf.FixedLenFeature([1], tf.float32)\n    features['user_id'] = tf.FixedLenFeature([1], tf.int64)\n    features['item_id'] = tf.FixedLenFeature([1], tf.int64)\n    instance = tf.parse_example(examples, features)\n    return instance['user_id'], instance['item_id'], instance['label']\n  with tf.name_scope('input'):\n    files = tf.data.Dataset.list_files('./inputs/part-*')\n    dataset = files.apply(tf.contrib.data.parallel_interleave(\n                        lambda file: tf.data.TFRecordDataset(file),\n                        cycle_length=4, sloppy=True))\n    dataset = dataset.prefetch(buffer_size=batch_size*2)\n    dataset = dataset.batch(batch_size)\n    dataset = dataset.map(_parse_function, num_parallel_calls=4)\n    iterator = dataset.make_initializable_iterator()\n    return iterator\n\ndef model(user_id, item_id):\n  ...\n  user_embedding = tf.embedding_lookup(user_id)\n  item_embedding == tf.embedding_lookup(item_id)\n\treturn tf.reduce_sum(query_embedding * item_embedding, 1, keep_dims=True)\n\n# 图构造部分: 模型部分使用placeholder，不直接使用DataSet.iterator的输出\ntrain_iterator = data_iter(FLAGS.batch_size)\ntrain_user_id, train_item_id, train_label = train_iterator.get_next()\ntrain_user_id_placeholder = tf.placeholder(tf.int64, [None, 1], name=\"train_user_id_placeholder\")\ntrain_item_id_placeholder = tf.placeholder(tf.int64, [None, 1], name=\"train_item_id_placeholder\")\ntrain_label_placeholder = tf.placeholder(tf.float32, [None, 1], name=\"train_label_placeholder\")\ntrain_score = model(train_user_id_placeholder, train_item_id_placeholder)\ntrain_loss = some_loss_function(train_score, train_label_placeholder)\nopt = tf.train.AdamOptimizer(learning_rate=0.001)\ntrain_op = opt.minimize(train_loss)\n\n# 执行部分\nwith tf.Session(...) as sess:\n  ...\n  while True:\n    try:\n      # 将数据读取和模型部分拆开，分别执行sess.run\n      train_user_id_val, train_item_id_val, train_label_val = sess.run([train_user_id, train_item_id, train_label])\n      sess.run(train_op,\n               feed_dict={\n                 train_user_id_placeholder: train_user_id_val,\n                 train_item_id_placeholder: train_item_id_val,\n                 train_label_placeholder: train_label_val\n               })\n    except tf.errors.OutOfRangeError:\n      break\n```\n\n这个问题在`TensorFlow-1.x`都存在，在2.x上就不了解了，下一步需要在核心层面解决这个问题。","tags":["TensorFlow"],"categories":["tech"]},{"title":"使用TensorFlow C++ API构建线上预测服务 - 篇3","url":"/832a0a1e/","content":"\n<!-- toc -->\n\n前两篇\n* [使用TensorFlow C++ API构建线上预测服务 - 篇1](https://mathmach.com/6d246b32/)\n* [使用TensorFlow C++ API构建线上预测服务 - 篇2](https://mathmach.com/f42d3a88/)\n\n在线下训练时，为了效率考虑，我们经常把数据转成TFRecord格式，然后直接调用TensorFlow提供的Reader来读入TFRecord数据。这样在生成的`graph.pb`中，Reader会对应多个节点，如果在c++中直接导入这个`graph.pb`我们就不能使用`std::vector<std::pair<std::string, tensorflow::Tensor>>`作为`session.Run(...)`的输入了。这篇文章讲解一下怎样处理这种情况。\n\n# Freeze Graph\n回顾一下上篇讲到的怎样使用freeze graph。\n```\npython ../../python/freeze_graph.py \\\n    --checkpoint_dir='./checkpoint' \\\n    --output_node_names='predict/add' \\\n    --output_dir='./model'\n```\n其实，这里还有一个可选输入，即`--graph_pb`，如果设定这个，相当于不用meta文件里的graph，而是用这个网络去freeze。\n这个参数不一定非要用训练时保存的网络，可以指定任何网络。讲到这里你可能就明白我们的方案是什么了。\n```\npython ../../python/freeze_graph.py \\\n    --checkpoint_dir='./checkpoint' \\\n    --graph_pb='./model/predict_graph.pb' \\\n    --output_node_names='predict/add' \\\n    --output_dir='./model'\n```\n\n# 具体方案\n由于`session.Run(...)`只能接受`std::vector<std::pair<std::string, tensorflow::Tensor>>`作为网络输入，那么我们可以构造一个新网络，这个网络和训练时的网络几乎一样，只不过输入部分不使用`Reader`，而是用`tf.Placeholder`代替。我们把新网络保存成`predict_graph.pb`，把它和训练产出的checkpoint进行freeze，即可得到可以用c++导入的一个新网络pb，用这个pb上线就可以了。\n\n## 例子\n### 训练网络\n```\n# input\nwith tf.name_scope('input'):\n    filename_queue = tf.train.string_input_producer(\n                tf.train.match_filenames_once(file_name), num_epochs=max_epoch)\n                serialized_example = self.Decode(filename_queue)\n                capacity = thread_num * batch_size + min_after_dequeue\n                batch_serialized_example = tf.train.shuffle_batch(\n                                    [serialized_example],\n                                    batch_size=batch_size,\n                                    num_threads=thread_num,\n                                    capacity=capacity,\n                                    min_after_dequeue=min_after_dequeue)\n    features = {}\n    features['label'] = tf.FixedLenFeature([], tf.float32)\n    features['sparse_id'] = tf.VarLenFeature(tf.int64)\n    features['sparse_val'] = tf.VarLenFeature(tf.float32)\n    instance = tf.parse_example(batch_serialized_example, features)\n    label = instance['label']\n    sparse_id = instance['sparse_id']\n    sparse_val = instance['sparse_val']\n\n# network   \nwith tf.variable_scope(\"emb_layer\"):\n    embedding_variable = tf.Variable(tf.truncated_normal([100000, 50], stddev=0.05), name='emb_var')\n    embedding = tf.nn.embedding_lookup_sparse(embedding_variable, sparse_id, sparse_val], \"mod\", combiner=\"sum\")\n...\n```\n### 预测网络\n```\n# input\nwith tf.name_scope('input'):\n    with tf.variable_scope('sparse_field'):\n        with tf.variable_scope('index'):\n            sparse_index = tf.placeholder(tf.int64)\n        with tf.variable_scope('id'):\n            sparse_ids = tf.placeholder(tf.int64)\n        with tf.variable_scope('value'):\n            sparse_vals = tf.placeholder(tf.float32)\n        with tf.variable_scope('shape'):\n            sparse_shape = tf.placeholder(tf.int64)\n        sparse_id = tf.SparseTensor(sparse_index, sparse_ids, self.sparse_shape)\n        sparse_val = tf.SparseTensor(sparse_index, sparse_vals, sparse_shape)\n    with tf.variable_scope('label'):\n        label = tf.placeholder(tf.float32)\n\n# network\nwith tf.variable_scope(\"emb_layer\"):\n    embedding_variable = tf.Variable(tf.truncated_normal([100000, 50], stddev=0.05), name='emb_var')\n    embedding = tf.nn.embedding_lookup_sparse(embedding_variable, sparse_id, sparse_val], \"mod\", combiner=\"sum\")\n...   \n```\n使用训练网络训练后保存checkpoint，然后保存预测网络的`graph.pb`，直接调用freeze把两者生成一个新的`graph.pb`即可，c++线上预测时只需为预测网络的输入部分构造所需几个Tensor作为输入即可。\n","tags":["TensorFlow"],"categories":["tech"]},{"title":"使用TensorFlow C++ API构建线上预测服务 - 篇2","url":"/f42d3a88/","content":"\n<!-- toc -->\n\n之前的一篇文章中[使用TensorFlow C++ API构建线上预测服务 - 篇1](https://mathmach.com/6d246b32/)，详细讲解了怎样用TensorFlow C++ API导入模型做预测，但模型`c = a * b`比较简单，只有模型结构，并没有参数，所以文章中并没讲到怎样导入参数。本文使用一个复杂的模型讲解，包括以下几个方面：\n* 针对稀疏数据的数据预处理\n* 训练中保存模型和参数\n* TensorFlow C++ API导入模型和参数\n* TensorFlow C++ API构造Sparse Tensor做模型输入\n\n# 稀疏数据下的数据预处理\n稀疏数据下，一般会调用TensorFlow的`embedding_lookup_sparse`。\n```python\nembedding_variable = tf.Variable(tf.truncated_normal([input_size, embedding_size], stddev=0.05), name='emb')\nembedding = tf.nn.embedding_lookup_sparse(embedding_variable, sparse_id, sparse_value, \"mod\", combiner=\"sum\")\n```\n上面代码中，`embedding_variable`就是需要学习的参数，其中`input_size`是矩阵的行数，`embedding_size`是矩阵的列数，比如我们有100万个稀疏id，每个id要embedding到50维向量，那么矩阵的大小是`[1000000, 50]`。sparse_id是要做向量化的一组id，用`SparseTensor`表示，`sparse_value`是每个id对用的一个value，用作权重，也用`SparseTensor`表示。\n这里要注意，如果id是用hash生成的，不保证id是`0,1,2,3, ...`这种连续表示，需要先把id排序后转成连续的，并且把`input_size`设成大于排序后最大的id，为了节省空间往往设成`排序后最大id+1`。因为用id去`embedding_variable`矩阵查询命中哪行的时候，使用`id mod Row(embedding_variable)`或其他策略作为命中的行数，如果不保证id连续，可能会出现多个id命中同一行的错误情况。另外，如果不把id排序后转成连续id，那`input_size`需要设成原始id中的最大id，如果是hash生成的那么最大id值非常大，做成矩阵非常大存不下和矩阵存在空间浪费，因为有些行肯定不会被命中。\n另外一个点，目前TensorFlow不支持sparse方式的查询和参数更新，每次查询更新都要pull&push一个矩阵全量数据，造成网络的堵塞，速度过慢，所以一般来说不要使用太大的embedding矩阵。\n\n# 训练中保存模型和参数\nTensorFlow保存模型时分为两部分，网络结构和参数是分开保存的。\n## 保存网络结构\n运行以下命令，成功后会看到一个名为`graph.pb`的pb二进制文件。后续如果使用TensorFlow官方提供的`freeze_graph.py`工具时必需这个文件，当然，如果对`freeze_graph.py`的代码比较熟悉，可以使用比较trick的方式，这样只需要参数文件，而不需要`graph.pb`了。\n```python\ntf.train.write_graph(sess.graph.as_graph_def(), FLAGS.model_dir, 'graph.pb', as_text=False)\n```\n## 保存模型参数\n运行以下命令，会在`FLAGS.model_dir`目录下保存多个前缀为`model.checkpoint`的文件。\n```python\nsaver = tf.train.Saver()\nsaver.save(sess, FLAGS.model_dir + \"/model.checkpoint\")\n```\n比如，成功后在`FLAGS.model_dir`目录下会看到以下几个文件。其中，`model.checkpoint.meta`包含了网络结构和一些其他信息，所以也包含了上面提到的`graph.pb`；`model.checkpoint.data-00000-of-00001`保存了模型参数，其他两个文件辅助作用。\n```\n-rw-r--r--  1 user  staff      89 10 11 11:32 checkpoint\n-rw-r--r--  1 user  staff  225136 10 11 11:32 model.checkpoint.data-00000-of-00001\n-rw-r--r--  1 user  staff    1506 10 11 11:32 model.checkpoint.index\n-rw-r--r--  1 user  staff  369379 10 11 11:32 model.checkpoint.meta\n```\n\n# TensorFlow C++ API导入模型和参数\n主要有两种方法：\n* 分别导入网络结构和模型参数\n* 线下先把网络结构和模型参数整合成一个文件，只用导入这个文件即可\n\n## 分别导入网络结构和模型参数\n### 导入网络结构\n以上文的graph.pb为例\n```C++\n// 导入网络结构\nGraphDef graph_def;\nstatus = ReadBinaryProto(Env::Default(), std::string(\"graph.pb\"), &graph_def);\nif (!status.ok()) {\n  throw runtime_error(\"Error loading graph: \" + status.ToString());\n}\n\n// 把网络设置到Session里\nstatus = session->Create(graph_def);\nif (!status.ok()) {\n  throw runtime_error(\"Error set graph to session: \" + status.ToString());\n}\n```\n### 导入模型参数\n这里注意要传入模型路径，既上文的`FLAGS.model_dir`。以`FLAGS.model_dir=\"your_checkpoint_path\"`为例\n```C++\n// 导入模型参数\nTensor checkpointPathTensor(DT_STRING, TensorShape());\ncheckpointPathTensor.scalar<std::string>()() = std::string(\"your_checkpoint_path\");\nstatus = session->Run(\n          {{ graph_def.saver_def().filename_tensor_name(), checkpointPathTensor },},\n          {},\n          {graph_def.saver_def().restore_op_name()},\n          nullptr);\nif (!status.ok()) {\n  throw runtime_error(\"Error loading checkpoint: \" + status.ToString());\n}\n```\n\n## 网络结构和模型参数整合成一个文件\n>  One confusing part about this is that the weights usually aren't stored inside the file format during training. Instead, they're held in separate checkpoint files, and there are Variable ops in the graph that load the latest values when they're initialized. It's often not very convenient to have separate files when you're deploying to production, so there's the freeze_graph.py script that takes a graph definition and a set of checkpoints and freezes them together into a single file.\n\n使用多个文件部署比较麻烦，如果能整个成一个独立文件会方便很多，因此，TensorFlow官方提供了`freeze_graph.py`工具。如果已经安装了TensorFlow，则在安装目录下可以找到，否则可以直接使用源码`tensorflow/python/tools`路径下`freeze_graph.py`。运行例子为：\n```bash\npython ${TF_HOME}/tensorflow/python/tools/freeze_graph.py \\\n    --input_graph=\"graph.pb\" \\\n    --input_checkpoint=\"your_checkpoint_path/checkpoint_prefix\" \\\n    --output_graph=\"your_checkpoint_path/freeze_graph.pb\" \\\n    --output_node_names=Softmax\n```\n其中，`input_graph`为网络结构pb文件，`input_checkpoint`为模型参数文件名前缀，`output_graph`为我们的目标文件，`output_node_names`为目标网络节点名称，因为网络包括前向和后向网络，在预测时后向网络其实是多余的，指定`output_node_names`后只保存从输入节点到这个节点的部分网络。如果不清楚自己想要的节点`output_node_names`是什么，可以用下面的代码把网络里的全部节点名字列出来，然后找到自己想要的那个就行了。\n```python\nfor op in tf.get_default_graph().get_operations():\n    print(op.name)\n```\n得到`freeze_graph.pb`后，只导入网络结构即可，不再需要另外导入模型参数。\n```C++\nGraphDef graph_def;\nstatus = ReadBinaryProto(Env::Default(), std::string(\"freeze_graph.pb\"), &graph_def);\n```\n`freeze_graph.py`的更多参数可以看它的代码。\n\n官方的`freeze_graph.py`工具需要在训练时同时调用`tf.train.write_graph`保存网络结构和`tf.train.Saver()`保存模型参数，之前讲过`tf.train.Saver()`保存的`meta`文件里其实已经包含了网络结构，所以就不用调用`tf.train.write_graph`保存网络结构，不过这时就不能直接调用官方的`freeze_graph.py`了，需要使用一点trick的方式将网络结构从`meta`文件里提取出来，具体代码可见`https://github.com/formath/tensorflow-predictor-cpp/blob/master/python/freeze_graph.py`，使用例子如下，其中`checkpoint_dir`的即上文的`FLAGS.model_dir`目录，`output_node_names`和官方`freeze_graph.py`的意思一致。\n```bash\n# this freeze_graph.py is https://github.com/formath/tensorflow-predictor-cpp/blob/master/python/freeze_graph.py\npython ../../python/freeze_graph.py \\\n    --checkpoint_dir='./checkpoint' \\\n    --output_node_names='predict/add' \\\n    --output_dir='./model'\n```\n\n# TensorFlow C++ API构造Sparse Tensor\n以`LibFM`格式数据为例，`label fieldid:featureid:value ...`。假如一个batch中有以下4条样本：\n```\n0 1:384:1 8:734:1\n0 3:73:1\n1 2:449:1 0:31:1\n0 5:465:1\n```\n四个`label`可以表示成一个稠密`Tensor`，即\n```C++\nauto label_tensor = test::AsTensor<float32>({0, 0, 1, 0});\n```\n剩余还有三个部分，分别是`fieldid`、`featureid`、`value`，每个部分都可以表示成一个`SparseTensor`，每个`SparseTensor`由3个`Tensor`组成。\n```C++\nInstance | SparseFieldId | SparseFeatureId |    SparseValue   |\n0        |    1, 8       |     384, 734    |     1.0, 1.0     |\n1        |    3          |     73          |     1.0          |\n2        |    2, 0       |     449, 31     |     1.0, 1.0     |\n3        |    5          |     465         |     1.0          |\n```\n以`SparseFieldId`部分为例，`SparseTensor`中的第一个`Tensor`表示每个id的行列坐标，比如`Instance=0`的`FieldId=1`为<0, 0>，`Instance=0`的`FieldId=8`为<0, 1>，`Instance=2`的`FieldId=0`为<2, 1>，总共6对，每对是个二元组，所以第一个`Tensor`为\n```C++\nauto fieldid_tensor_indices =\n      test::AsTensor<int64>({0, 0, 0, 1, 1, 0, 2, 0, 2, 1, 3, 0}, {6, 2});\n```\n`SparseTensor`中的第二个`Tensor`表示id值，即\n```C++\nauto fieldid_tensor_values = test::AsTensor<int64>({1, 8, 3, 2, 0, 5});\n```\n第三个`Tensor`表示样本行数和每条样本里最多有多少个id，所以是\n```C++\nauto fieldid_tensor_shape = TensorShape({4, 2});\n```\n最后，`fieldid`部分的`SparseTensor`表示为\n```C++\nsparse::SparseTensor fieldid_sparse_tensor(\n      fieldid_tensor_indices, fieldid_tensor_values, fieldid_tensor_shape);\n```\n其他两个部分，`featureid`和`value`同样可以用`SparseTensor`表示。最后，一个batch的libfm数据可以由4份数据来表示，这4份数据作为网络的`input`，运行`Session.run`即可得到输出。当然，线上预测时就没有`label`这一部分输入了。\n* label的`Tensor`\n* fieldid的`SparseTensor`\n* featureid的`SparseTensor`\n* value的`SparseTensor`\n\n# 参考\n* [Exporting trained TensorFlow models to C++ the RIGHT way!](https://medium.com/@hamedmp/exporting-trained-tensorflow-models-to-c-the-right-way-cf24b609d183)\n* [Freeze Graph](https://www.tensorflow.org/extend/tool_developers)\n* [TensorFlow: How to freeze a model and serve it with a python API](https://blog.metaflow.fr/tensorflow-how-to-freeze-a-model-and-serve-it-with-a-python-api-d4f3596b3adc)\n","tags":["TensorFlow"],"categories":["tech"]},{"title":"使用TensorFlow C++ API构建线上预测服务 - 篇1","url":"/6d246b32/","content":"\n<!-- toc -->\n\n目前，TensorFlow官方推荐使用Bazel编译源码和安装，但许多公司常用的构建工具是CMake。TensorFlow官方并没有提供CMake的编译示例，但提供了MakeFile文件，所以可以直接使用make进行编译安装。另一方面，模型训练成功后，官方提供了TensorFlow Servering进行预测的托管，但这个方案过于复杂。对于许多机器学习团队来说，一般都有自己的一套模型托管和预测服务，如果使用TensorFlow Servering对现存业务的侵入性太大，使用TensorFlow C++ API来导入模型并提供预测服务能方便的嵌入大部分已有业务方案，对这些团队来说比较合适。\n\n本文以一个简单网络介绍从线下训练到线上预测的整个流程，主要包括以下几点：\n* 使用Python接口训练模型\n* 使用make编译TensorFlow源码，得到静态库\n* 调用TensorFlow C++ API编写预测代码，使用CMake构建预测服务\n\n# 使用Python接口训练模型\n这里用一个简单的网络来介绍，主要目的是保存网络结构和参数，用于后续的预测。\n```python\nimport tensorflow as tf\nimport numpy as np\n\nwith tf.Session() as sess:\n    a = tf.Variable(5.0, name='a')\n    b = tf.Variable(6.0, name='b')\n    c = tf.multiply(a, b, name='c')\n\n    sess.run(tf.global_variables_initializer())\n\n    print(a.eval()) # 5.0\n    print(b.eval()) # 6.0\n    print(c.eval()) # 30.0\n\n    tf.train.write_graph(sess.graph_def, 'simple_model/', 'graph.pb', as_text=False)\n```\n这个网络有两个输入，a和b，输出是c，最后一行用来保存模型到simple_model目录。运行后会在simple_model目录下生成一个graph.pb的protobuf二进制文件，这个文件保存了网络的结构，由于这个例子里没有模型参数，所以没有保存checkpoint文件。\n\n# 源码编译TensorFlow\n官方详细介绍可以看这里[源码编译TensorFlow](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/makefile)。其实很简单，以maxOS为例，只要运行以下命令即可，其他操作系统也有相应的命令。编译过程大概需要半小时，成功后会在tensorflow/tensorflow/contrib/makefile/gen/lib下看到一个100多MB的libtensorflow-core.a库文件。maxOS需要使用`build_all_linux.sh`，并且只能用clang，因为有第三方依赖编译时把clang写死了。\n```bash\ngit clone https://github.com/tensorflow/tensorflow.git\ncd tensorflow\ntensorflow/contrib/makefile/build_all_linux.sh\n```\n后续如果要依赖TensorFlow的头文件和静态库做开发，tensorflow/tensorflow/contrib/makefile目录下的几个目录需要注意：\n* downloads 存放第三方依赖的一些头文件和静态库，比如nsync、Eigen等\n* gen 存放TensorFlow生成的C++ PB头文件、TensorFlow的静态库、ProtoBuf的头文件和静态库等等\n\n# 使用TensorFlow C++ API编写预测代码\n预测代码主要包括以下几个步骤：\n* 创建Session\n* 导入之前生成的模型\n* 将模型设置到创建的Session里\n* 设置模型输入输出，调用Session的Run做预测\n* 关闭Session\n\n## 创建Session\n```C++\nSession* session;\nStatus status = NewSession(SessionOptions(), &session);\nif (!status.ok()) {\n  std::cout << status.ToString() << std::endl;\n} else {\n  std::cout << \"Session created successfully\" << std::endl;\n}\n```\n\n## 导入模型\n```C++\nGraphDef graph_def;\nStatus status = ReadBinaryProto(Env::Default(), \"../demo/simple_model/graph.pb\", &graph_def);\nif (!status.ok()) {\n  std::cout << status.ToString() << std::endl;\n} else {\n  std::cout << \"Load graph protobuf successfully\" << std::endl;\n}\n```\n\n## 将模型设置到创建的Session里\n```C++\nStatus status = session->Create(graph_def);\nif (!status.ok()) {\n  std::cout << status.ToString() << std::endl;\n} else {\n  std::cout << \"Add graph to session successfully\" << std::endl;\n}\n```\n\n## 设置模型输入\n模型的输入输出都是Tensor或Sparse Tensor。\n```C++\nTensor a(DT_FLOAT, TensorShape()); // input a\na.scalar<float>()() = 3.0;\n\nTensor b(DT_FLOAT, TensorShape()); // input b\nb.scalar<float>()() = 2.0;\n```\n\n## 预测\n```C++\nstd::vector<std::pair<string, tensorflow::Tensor>> inputs = {\n  { \"a\", a },\n  { \"b\", b },\n}; // input\n\nstd::vector<tensorflow::Tensor> outputs; // output\n\nStatuc status = session->Run(inputs, {\"c\"}, {}, &outputs);\nif (!status.ok()) {\n  std::cout << status.ToString() << std::endl;\n} else {\n  std::cout << \"Run session successfully\" << std::endl;\n}\n```\n\n## 查看预测结果\n```C++\nauto c = outputs[0].scalar<float>();\nstd::cout << \"output value: \" << c() << std::endl;\n```\n\n## 关闭Session\n```C++\nsession->Close();\n```\n完整的代码在`https://github.com/formath/tensorflow-predictor-cpp`，路径为`src/simple_model.cc`。 \n\n# 使用CMake构建预测代码\n这里主要的问题是头文件和静态库的路径要正确，包括TensorFlow以及第三方依赖。 以macOS为例，其他平台路径会不一样。\n\n## 头文件路径\n```\ntensorflow // TensorFlow头文件\ntensorflow/tensorflow/contrib/makefile/gen/proto // TensorFlow PB文件生成的pb.h头文件\ntensorflow/tensorflow/contrib/makefile/gen/protobuf-host/include // ProtoBuf头文件\ntensorflow/tensorflow/contrib/makefile/downloads/eigen // eigen头文件\ntensorflow/tensorflow/contrib/makefile/downloads/nsync/public // nsync头文件\n```\n\n## 静态库路径\n```\ntensorflow/tensorflow/contrib/makefile/gen/lib // TensorFlow静态库\n/tensorflow/tensorflow/contrib/makefile/gen/protobuf-host/lib // protobuf静态库\n/tensorflow/tensorflow/contrib/makefile/downloads/nsync/builds/default.macos.c++11 // nsync静态库\n```\n编译时需要这些静态库\n```\nlibtensorflow-core.a\nlibprotobuf.a\nlibnsync.a\n其他: pthread m z\n```\n\n## CMake构建\n```bash\ngit clone https://github.com/formath/tensorflow-predictor-cpp.git\ncd tensorflow-predictor-cpp\nmkdir build && cd build\ncmake ..\nmake\n```\n构建完成后在bin路径下会看到一个simple_model可执行文件，运行`./simple_model`即可看到输出`output value: 6`。 需要注意的时，编译选项里一定要加这些`-undefined dynamic_lookup -all_load`，否则在编译和运行时会报错，原因可见[dynamic_lookup](https://www.tensorflow.org/versions/r1.1/extend/adding_an_op)和[Error issues](https://github.com/tensorflow/tensorflow/issues/3308)。\n\n以上用`c = a * b`一个简单的网络来介绍整个流程，只要简单的修改即可应用到复杂模型中去，更复杂的一个例子可见`src/deep_model.cc`。\n\n# 参考\n* [tensorflow c++ prediction example](https://github.com/formath/tensorflow-predictor-cpp)\n* [Various Models implemented in TensorFlow](https://github.com/formath/tensorflow-models)\n* [Loading a TensorFlow graph with the C++ API](https://medium.com/jim-fleming/loading-a-tensorflow-graph-with-the-c-api-4caaff88463f)\n* [Loading a tensorflow graph with the C++ API by using Mnist](http://jackytung8085.blogspot.jp/2016/06/loading-tensorflow-graph-with-c-api-by.html)\n* [Tensorflow Different ways to Export and Run graph in C++](https://stackoverflow.com/questions/35508866/tensorflow-different-ways-to-export-and-run-graph-in-c/43639305#43639305)\n* [dynamic_lookup](https://www.tensorflow.org/versions/r1.1/extend/adding_an_op)\n* [Error issues](https://github.com/tensorflow/tensorflow/issues/3308)\n","tags":["TensorFlow"],"categories":["tech"]},{"title":"广告CTR预估场景下的DNN调优实战","url":"/2442aa9e/","content":"\n<!-- toc -->\n\n## 特征\n\n* DNN需要组合特征\nLR模型的时候，我们需要构造许多组合特征，比如UserID与ItemID的组合，许多做DNN的都宣称简化了特征工程，由隐层学习特征交叉，但是隐层进行特征组合的方式并没有明确的理论解释，并且通过隐层参数学习的方式进行隐式的特征组合并不能保证收敛到最优解，通过显示的构造组合特征能给DNN提供一些先验信息，从实战来看，DNN加上显示的组合特征效果会好很多。\n\n* 稀疏特征过滤\n训练数据中出现频次过少的离散特征往往容易引起过拟合，需要统计频次并做过滤。效果比较好的一种方法是，比如生成`day`这一天的特征时，使用 `[day-delta_day_num, day-1]`之间的特征统计值来过滤`day`这一天的稀疏特征，相比使用`[day-delta_day_num, day]`之间的统计值，auc明显提升。`delta_day_num`可以设成14天，过滤阈值20，具体数据可以根据业务场景和实验效果来定。\n\n* DNN特征划分Group\n和FFM类似，需要把特征划分成多个Group，每个Group里的特征做Embedding后Sum起来。划分方法，可以根据特征语义层面（用户、Item、组合）和数量、粒度进行划分。比如把用户相关特征划分成下面几个Group：\n```\n用户细粒度Id: UserId\n用户画像：Age，Gender，收入，职业\n用户行为：近期浏览的ItemIds\n```\n\n## 模型\n\n* 每类特征设置不同`Embedding Size`\n特征包含的信息越丰富，越需要更大的`Embedding Size`来描述，特征包含信息的丰富程度可以通过特征的粒度和数量表现出来，具体的一个划分方式可以看下图。\n特征粒度方面，单特征相对算粗粒度，组合特征相对较细，组合的层次越深，粒度越细。比如`UserId#ItemId`的组合特征刻画了用户对商品的倾向，`UserId#ItemId#Hour`刻画了用户在某个时间对某个商品的倾向【比如外卖，举个例子，实际一般不会这么组合】。特征刻画的粒度越细，说明指代的越具体，包含的信息非常明确却单一，这类特征一般不需要再和其他特征进行组合，所以`Embedding Size`会更小。\n特征数量方面，一般数量越大，包含的信息越多，比如`UserId`和`ItemId`可以达到上亿，一个`UserId`可以描述这个用户的很多信息，像`Age`和`Gender`这类特征规模很小的特征所包含的信息相对有限。\n<div style=\"text-align: center\">\n<img src=\"/images/dnn_embedding_size.png\" alt=\"图片替换文本\" width=\"400\" height=\"400\" align=\"middle\" />\n</div>\n\n* 模型结构\nWide&Deep最靠谱，DeepFM、DCN之类的效果都不大行。在Wide&Deep模型的基础上，没有将Wide部分单独拿出来，而是和Deep在一起，通过特征划分Group后一起Embedding，效果可以超过Wide&Deep，并且右边添加了一个基于离散特征统计的历史CTR的网络，可以降低模型的Variance，效果提升非常明显。总之，不要过分迷信一些灌水论文。\n<div style=\"text-align: center\">\n<img src=\"/images/best_dnn.png\" alt=\"图片替换文本\" width=\"500\" height=\"400\" align=\"middle\" />\n</div>\n\n* 减少多余训练参数\n二分类模型下，最后一层全连接的代码经常是下面这样：\n```\nlayer = ... # 倒数第二层\nweight = tf.get_variable('weight', [dim, 2], initializer=...)\nbias = tf.get_variable('bias', [2], initializer=...)\nlogits = tf.matmul(layer, weight) + bias\n```\n因为是二分类，连接`label=0`的所有边其实不需要学习，`0`就是最优参数，如果加入学习的话，实际上多了一些冗余参数，而且梯度下降必定无法保障它们收敛到这个最优解。改成下面这样，`auc`可以较大提升。\n```\nlayer = ... # 倒数第二层\nweight_positive = tf.get_variable('weight_positive', [dim, 1], initializer=...)\nbias_positive = tf.get_variable('bias_positive', [1], initializer=...)\nweight_neg = tf.get_variable('weight_negative', initializer=tf.constant(np.zeros((dim, 1), dtype=np.float32)), trainable=False)\nlogits_positive = tf.matmul(layer, weight_positive) + bias_positive\nlogits_negative = tf.matmul(layer, weight_negative)\nlogits = tf.concat([logits_negative, logits_positive], 1)\n```\n\n* 选一个好的基线模型\n这些年DNN火起来后，大家都往DNN方向发展，很多团队宣称切换到了DNN，宣传文章写的也不错，但是从实际来看，真正把DNN用好的团队并不多。比如从GBDT切换到DNN的一些组，其实整个特征流程还是沿用的GBDT思路，用几百维连续特征来做DNN，或者简单加几个小规模离散特征，少数技术强悍的团队其实做的是百亿千亿特征、模型规模TGB级别的超大DNN，所以同样是Wide&Deep模型，不同的规模下其实天壤之别。DNN相对于GBDT来说，是非常容易做出成果的，主要还是把GBDT作为基线模型有点太简单了，其实在搜索推荐这类个性化很强的场景下把GBDT换成百亿千亿级别特征的超大规模LR或者FFM也会获得很大提升，所以如果要做DNN的话，推荐用FFM来做基线，实战来看DNN相对FFM要做出成果并没有那么简单。\n\n## 性能\n\n* 大规模离散特征`Embedding`\n稀疏特征的id做embedding时，由于TensorFlow内部使用一个`shape=[id_num, embedding_size]`的Variable做参数，需要把id映射成[0, id_num)间的一个数字。如果id量非常小的话，可以在特征提取后把id排序一遍生成从0开始的连续id值，但在工业界场景下id往往是用`murmur hash`生成的`uint64 id`，量级往往是百万到千亿级别，很难做排序。TensorFlow内部有一个Hash Table可以将`uint64`映射成从0开始的连续id，但可能将不同的id映射到`embedding_variable`的同一行，所以建议把`embedding_variable`的行数和`num_oov_buckets`设置的大一点，减小一点冲突。当然，最优方案应该是使用Map结构来实现`Embedding Variable`，现在官方并没有人做，我已经修改TensorFlow底层代码实现了一个，支持千亿离散特征的embedding，在公司内已经应用，这一块也可以参考阿里发布的TensorFlowRS。\n```\nembedding_variable = tf.get_variable('emb_var',\n                                     [2*id_num+2, embedding_size],\n                                     initializer=...)\nhash_table = tf.contrib.lookup.index_table_from_tensor(mapping=tf.constant([0]),\n                                                       num_oov_buckets=2*id_num,\n                                                       dtype=tf.int64)\nsparse_ids = hash_table.lookup(origin_sparse_ids)\nembedding = tf.nn.embedding_lookup_sparse(embedding_variable,\n                                          sparse_ids,\n                                          None,\n                                          partition_strategy=\"mod\")\n```\n\n* `Sparse Embedding`性能\n使用\n```\ndef embedding_lookup_sparse_with_distributed_aggregation(params,\n                                                         sp_ids,\n                                                         sp_weights,\n                                                         partition_strategy=\"mod\",\n                                                         name=None,\n                                                         combiner=None,\n                                                         max_norm=None)\n```\n代替\n```\ndef embedding_lookup_sparse(params,\n                            sp_ids,\n                            sp_weights,\n                            partition_strategy=\"mod\",\n                            name=None,\n                            combiner=None,\n                            max_norm=None)\n```\n。后者在ps端lookup出许多embedding后传给worker，在worker端做聚合，前者在ps端做多个embedding的聚合后传给worker，通信量会小很多。\n\n* 不要使用`TensorFlow Feature Columns`\n`TensorFlow Feature Columns`的性能很差，建议把特征相关的所有工作，包括离散化、组合等操作都放在单独的特征抽取工具里面，TensorFlow只包含模型部分代码。\n\n* `QueueRunner`批量读数据\n使用`read_up_to`接口批量读数据，性能提升非常大。\n```\nreader = tf.TFRecordReader()\n_, serialized_example = reader.read(filename_queue)\n_, serialized_example = reader.read_up_to(filename_queue, 1000)\n```\n\n* 使用`DataSet`接口读数据\nQueueRunner读数据时不能精确一轮一轮的读，很难做worker之间的barrier，TensorFlow DataSet可以实现精确读取一轮，在worker精确同步时比较有用[TensorFlow实现Barrier](https://mathmach.com/b2f65b04/)。而且DataSet的性能和QueueRunner差不多，主要是几个接口的使用顺序要注意。\n```\ndef _parse_function(examples_proto):\n    features = {}\n    features['label'] = tf.FixedLenFeature([], tf.float32)\n    features['feature'] = ...\n    instance = tf.parse_example(examples_proto, features)\n    label = instance['label']\n    feature = instance['feature']\n    return label, feature\n\ndataset = tf.data.TFRecordDataset(file_name_list)\ndataset = dataset.prefetch(buffer_size=batch_size*100)\ndataset = dataset.shuffle(buffer_size=batch_size*10)\ndataset = dataset.batch(batch_size)\ndataset = dataset.map(_parse_function, num_parallel_calls=4)\niterator = dataset.make_initializable_iterator()\n```\n\n*  `GPU vs CPU`\n现在很多做算法的言必GPU，其实很多场景下并不合适。CTR模型训练场景下，主要耗时操作是`Embedding Lookup`，不适合GPU，全连接层又很小，CPU足够应付。整体来看，`P40`比`Intel® Xeon® Processor E5-2650 v4 (30M Cache, 2.20 GHz) `快5%左右，但价格贵很多。2.7GHz的CPU性能可以提升30%，所以从性价比来看，推荐主频更快的CPU。这个一定要分应用场景，在场景下去做正确的决定，而不是人云亦云。\n\n## 其他\n* 训练千亿特征TGB级别参数的超大模型\n* 将单机无法加载的超大模型做线上预测服务\n* 秒级在线深度学习架构\n\n## 参考\n* [TensorFlow实现多Worker同步机制](https://mathmach.com/b2f65b04/)\n* [TensorFlow Feature Columns](https://www.tensorflow.org/guide/feature_columns)\n","tags":["点击率预估","Deep-Learning","CTR"],"categories":["tech"]},{"title":"分布式机器学习中的同步模式：ASP、BSP、SSP实验研究","url":"/c8ad6f19/","content":"\n<!-- toc -->\n\n## 背景\n之前在公司开发了一个`Parameter Server`架构的分布式机器学习系统，可以支持多种同步模式，支持异步的ASP、同步的BSP、半同步的SSP，但是在点击率预估等业务场景中，实际工作中最常用的还是ASP模式，其他两种模式并没有进行实验，这次想通过实验看下效果如何。\n\n## 实验配置\n* 使用[day-89, day]共90天的线上数据做训练，使用[day+1]一天的数据做验证\n* 训练数据4亿左右，特征千万量级\n* 集群：50 worker + 20 ps on Yarn, 8core,4g/node\n* 点击率预估，模型使用最简单的LR\n* 优化算法使用FTRL\n* 不同实验只有同步方式不同，其他配置保持一样\n\n## 效果\n\n### ASP模式\n\n```    \n19:48:18 epoch: 1, train-logloss: 0.0627267, train-auc: 0.690643, valid-logloss: 0.0555645, valid-auc: 0.698787\n19:54:11 epoch: 2, train-logloss: 0.0622153, train-auc: 0.704938, valid-logloss: 0.0554252, valid-auc: 0.702617\n20:00:21 epoch: 3, train-logloss: 0.0620013, train-auc: 0.710765, valid-logloss: 0.0553471, valid-auc: 0.704912\n20:06:43 epoch: 4, train-logloss: 0.0618459, train-auc: 0.714920, valid-logloss: 0.0552953, valid-auc: 0.706354\n20:12:43 epoch: 5, train-logloss: 0.0617195, train-auc: 0.718254, valid-logloss: 0.0552550, valid-auc: 0.707505\n20:18:48 epoch: 6, train-logloss: 0.0616111, train-auc: 0.721076, valid-logloss: 0.0552250, valid-auc: 0.708359\n20:24:47 epoch: 7, train-logloss: 0.0615152, train-auc: 0.723545, valid-logloss: 0.0552006, valid-auc: 0.709060\n20:28:27 epoch: 8, train-logloss: 0.0614288, train-auc: 0.725751, valid-logloss: 0.0551802, valid-auc: 0.709684\n```\n\n### BSP模式\n\n```\n19:55:23 epoch: 1, train-logloss: 0.0627236, train-auc: 0.690612, valid-logloss: 0.0555393, valid-auc: 0.699665\n20:06:58 epoch: 2, train-logloss: 0.0622139, train-auc: 0.704972, valid-logloss: 0.0554050, valid-auc: 0.703530\n20:20:43 epoch: 3, train-logloss: 0.0620007, train-auc: 0.710783, valid-logloss: 0.0553289, valid-auc: 0.705700\n20:29:27 epoch: 4, train-logloss: 0.0618450, train-auc: 0.714948, valid-logloss: 0.0552777, valid-auc: 0.707155\n20:39:05 epoch: 5, train-logloss: 0.0617186, train-auc: 0.718281, valid-logloss: 0.0552391, valid-auc: 0.708264\n20:48:00 epoch: 6, train-logloss: 0.0616103, train-auc: 0.721099, valid-logloss: 0.0552086, valid-auc: 0.709087\n20:57:18 epoch: 7, train-logloss: 0.0615145, train-auc: 0.723561, valid-logloss: 0.0551841, valid-auc: 0.709804\n21:09:38 epoch: 8, train-logloss: 0.0614279, train-auc: 0.725770, valid-logloss: 0.0551629, valid-auc: 0.710393\n```\n\n### SSP模式(threshold=50)\n\n```\n20:00:14 epoch: 1, train-logloss: 0.0627070, train-auc: 0.690925, valid-logloss: 0.0555584, valid-auc: 0.698937\n20:08:23 epoch: 2, train-logloss: 0.0622105, train-auc: 0.705031, valid-logloss: 0.0554257, valid-auc: 0.702801\n20:19:17 epoch: 3, train-logloss: 0.0619988, train-auc: 0.710817, valid-logloss: 0.0553514, valid-auc: 0.704902\n20:27:04 epoch: 4, train-logloss: 0.0618438, train-auc: 0.714971, valid-logloss: 0.0553002, valid-auc: 0.706375\n20:38:13 epoch: 5, train-logloss: 0.0617177, train-auc: 0.718299, valid-logloss: 0.0552611, valid-auc: 0.707476\n20:48:29 epoch: 6, train-logloss: 0.0616096, train-auc: 0.721115, valid-logloss: 0.0552307, valid-auc: 0.708315\n21:00:22 epoch: 7, train-logloss: 0.0615140, train-auc: 0.723575, valid-logloss: 0.0552069, valid-auc: 0.709004\n21:18:24 epoch: 8, train-logloss: 0.0614276, train-auc: 0.725779, valid-logloss: 0.0551862, valid-auc: 0.709557\n```\n\n### SSP模式(threshold=20)\n\n```\n20:38:15 epoch: 1, train-logloss: 0.0627076, train-auc: 0.690827, valid-logloss: 0.0555455, valid-auc: 0.699352\n20:46:12 epoch: 2, train-logloss: 0.0622098, train-auc: 0.705044, valid-logloss: 0.0554167, valid-auc: 0.703117\n20:55:21 epoch: 3, train-logloss: 0.0619982, train-auc: 0.710830, valid-logloss: 0.0553428, valid-auc: 0.705237\n21:06:25 epoch: 4, train-logloss: 0.0618434, train-auc: 0.714978, valid-logloss: 0.0552917, valid-auc: 0.706651\n21:20:52 epoch: 5, train-logloss: 0.0617175, train-auc: 0.718303, valid-logloss: 0.0552538, valid-auc: 0.707730\n21:32:35 epoch: 6, train-logloss: 0.0616095, train-auc: 0.721119, valid-logloss: 0.0552240, valid-auc: 0.708575\n21:45:32 epoch: 7, train-logloss: 0.0615138, train-auc: 0.723582, valid-logloss: 0.0552001, valid-auc: 0.709290\n21:56:57 epoch: 8, train-logloss: 0.0614273, train-auc: 0.725787, valid-logloss: 0.0551785, valid-auc: 0.709892\n```\n\n### SSP模式(threshold=10)\n\n```\n21:21:47 epoch: 1, train-logloss: 0.0627156, train-auc: 0.690705, valid-logloss: 0.0555498, valid-auc: 0.699055\n21:32:30 epoch: 2, train-logloss: 0.0622118, train-auc: 0.705017, valid-logloss: 0.0554154, valid-auc: 0.702878\n21:44:33 epoch: 3, train-logloss: 0.0619991, train-auc: 0.710818, valid-logloss: 0.0553408, valid-auc: 0.704996\n21:54:42 epoch: 4, train-logloss: 0.0618440, train-auc: 0.714974, valid-logloss: 0.0552890, valid-auc: 0.706415\n22:03:06 epoch: 5, trainlogloss: 0.06171790, train-auc: 0.718299, valid-logloss: 0.0552505, valid-auc: 0.707568\n22:11:31 epoch: 6, train-logloss: 0.0616097, train-auc: 0.721113, valid-logloss: 0.0552197, valid-auc: 0.708433\n22:19:22 epoch: 7, train-logloss: 0.0615140, train-auc: 0.723578, valid-logloss: 0.0551951, valid-auc: 0.709113\n22:29:30 epoch: 8, train-logloss: 0.0614275, train-auc: 0.725781, valid-logloss: 0.0551743, valid-auc: 0.709645\n```\n\n## 分析\n  可以看出，BSP效果还是比ASP好一些，但差距非常小，只有万分位差距。当然，在不同数据上可能差距有所不同。另外，BSP每轮需要10分钟，ASP只需要6分钟。而且，这次实验还是在集群负载比较空闲的时候测的，由于机器学习和Spark、MapReduce等大数据任务共享集群，如果集群负载较高，可能慢节点的情况会更严重，BSP估计速度会更慢。所以结合效果和速度来说，并没有使用BSP模式的必要。\n  \n  SSP模式，在阈值为50的情况下，效果基本就和ASP差不多了，但速度还是慢一些，和BSP的速度差不多，估计有个慢节点拖后腿了。随着阈值变小，SSP效果也在慢慢变好，和BSP的效果接近。每个配置只跑了一次，所以稍微有点波动。\n  \n  另外，这个测试是在CTR预估模型上做的，是个非常稀疏的模型，在梯度更新时冲突的情况不太大，所以ASP效果和BSP并没有明显差别。在图像领域，都是稠密模型，`All-Reduce`是最常使用的梯度汇聚和参数更新方式，是类似BSP的完全同步模式。在稠密模型上ASP效果怎么样呢？后续有时间再试试。","tags":["Machine-Learning","点击率预估","CTR"],"categories":["tech"]},{"title":"CMake管理第三方依赖","url":"/d183c0cd/","content":"\n<!-- toc -->\n\n## 背景\n最近在一个C++项目中尝试`Bazel`编译，编译依赖方式确实写着比较舒服和直观，但最后链接出来的二进制文件在执行时报`Segment error`，但用`CMake`编译出来的二进制文件就可以成功执行，Bazel编译的问题无从下手。另外，Bazel无法从系统目录查找头文件，这就不能忍了，有人建议从`cc_toolchain_config.bzl`查找问题，但toolchain实在是有点麻烦，就暂时放弃Bazel，继续使用CMake了。Bazel里提供的`git_repositry`等从外部源自动下载编译依赖的方式很好用，所以就思考在CMake里是不是也有类似的东西呢。之前使用CMake时，第三方依赖都是手动先在本地安装好，后来查找到了CMake里提供了类似Bazel的命令，那就是`ExternalProject`，不过这个命令只管下载编译等操作，但`git_repositry`更好使一些，它可以根据依赖自动判断是不是下载，而`ExternalProject`就没这么丝滑了，所以本文记录下在CMake怎样基于`ExternalProject`打造`git_repositry`那种丝滑的体验。\n\n## FindPackageHandleStandardArgs\n在具体方案之前，先看一下CMake里提供的这个函数，在后面的实现中它可是非常重要哦。\n```\nThis module provides a function intended to be used in Find Modules implementing find_package(<PackageName>) calls. It handles the REQUIRED, QUIET and version-related arguments of find_package. It also sets the <PackageName>_FOUND variable. The package is considered found if all variables listed contain valid results, e.g. valid filepaths.\n```\n我们在CMake里查找包时使用`find_package(<PackageName>)`，它判断一个包是否找到就是利用了`FindPackageHandleStandardArgs`。这个函数会判断几个关键变量是否有正确的值，如果都经过了验证，就设置`<PackageName>_FOUND`，既找到了包，否则就没找到。它有两个函数定义，一个简单的是这样滴：\n```\nfind_package_handle_standard_args(<PackageName>\n  (DEFAULT_MSG|<custom-failure-message>)\n  <required-var>...\n  )\n```\n简单的例子，\n```\nFIND_PACKAGE_HANDLE_STANDARD_ARGS(Gflags DEFAULT_MSG GFLAGS_INCLUDE_DIR GFLAGS_LIBRARY)\n```\n通过判断`GFLAGS_INCLUDE_DIR`和`GFLAGS_LIBRARY`是否有正确值来断定`GFlags`是否找到，这里的`GFLAGS_INCLUDE_DIR`和`GFLAGS_LIBRARY`当然是我们自己设定喽，比如我们这么设定它们，如果搜到了头文件和库文件，就设置正确路径，否则就没值嘛。注意，这里我们最好不要用`GFLAGS_LIBIARIES`这种变量，这种我们是用来作为链接依赖库的变量的呀，同理，头文件那个变量也一样哦。\n```\nfind_path(GFLAGS_INCLUDE_DIR gflags/gflags.h /usr/local/include)\nfind_library(GFLAGS_LIBRARY gflags HINTS /usr/local/lib)\n```\n\n## 具体方案\n我们比较常用的一种第三方库依赖的方式是这样嘛，先在本地系统目录搜下，如果找到就用，如果找不到就自动去下载编译。所以这里我们还是以`GFlags`为例，分两步：\n* 从系统目录查找GFlags，如果找到则设置`GFLAGS_INCLUDE_DIR`和`GFLAGS_LIBRARY`\n* 如果上一步没找到，那么从某个地方下载Gflags，本地编译安装后，再设置`GFLAGS_INCLUDE_DIR`和`GFLAGS_LIBRARY`\n\n我们把这两步定义成两个宏`DO_FIND_GFLAGS_SYSTEM`和`DO_FIND_GFLAGS_DOWNLOAD`，然后条件判断调用就行啦，像这样：\n```\nif(NOT GFLAGS_FOUND)\n\tDO_FIND_GFLAGS_SYSTEM()\nendif()\n\nif(NOT GFLAGS_FOUND)\n\tDO_FIND_GFLAGS_DOWNLOAD()\nendif()\n```\n\n`DO_FIND_GFLAGS_SYSTEM`比较简单哦，我们直接给出代码：\n```\nmacro(DO_FIND_GFLAGS_SYSTEM)\n\tfind_path(GFLAGS_INCLUDE_DIR gflags/gflags.h\n\t\tPATHS /usr/local/include /usr/include\n\t\t)\n\tmessage(\"GFLAGS_INCLUDE_DIR: \" ${GFLAGS_INCLUDE_DIR})\n\tfind_library(GFLAGS_LIBRARY\n\t\tNAMES gflags\n\t\tPATHS /usr/local/lib /usr/local/lib64 /usr/lib /usr/lib64\n\t\t)\n\tmessage(\"GFLAGS_LIBRARY: \" ${GFLAGS_LIBRARY})\n\tFIND_PACKAGE_HANDLE_STANDARD_ARGS(Gflags DEFAULT_MSG\n\t\tGFLAGS_INCLUDE_DIR GFLAGS_LIBRARY\n\t\t)\n\tset(GFLAGS_LIBRARIES ${GFLAGS_LIBRARY})\n\tset(GFLAGS_INCLUDE_DIRS ${GFLAGS_INCLUDE_DIR})\n\tmark_as_advanced(GFLAGS_LIBRARIES GFLAGS_INCLUDE_DIRS)\nendmacro()\n```\n先从系统目录查找头文件和库文件，如果找到就设置`GFLAGS_INCLUDE_DIR`和`GFLAGS_LIBRARY`，然后调用`FIND_PACKAGE_HANDLE_STANDARD_ARGS`来通过我们设置的两个变量来断定GFlags到底找没找到，如果找到它会自动设置`GFLAGS_FOUND`为`True`，那第二个宏，就是先下载再编译的那个，就不用执行喽。最后，我们设定`GFLAGS_INCLUDE_DIRS`和`GFLAGS_LIBRARIES`供我们的主程序作为头文件路径和库依赖路径使用呗。\n\n第二个宏，我们终于用到开头提到的`ExternalProject`这个牛逼玩意了^_^\n```\nThe ExternalProject_Add() function creates a custom target to drive download, update/patch, configure, build, install and test steps of an external project.\n```\n太多内容，咱就直接贴用法再讲解吧。\n```\nmacro(DO_FIND_GFLAGS_DOWNLOAD)\n\tinclude(ExternalProject)\n\tExternalProject_Add(\n\t\tGflags\n\t\tURL https://github.com/gflags/gflags/archive/v2.2.1.zip\n\t\tURL_HASH SHA256=4e44b69e709c826734dbbbd5208f61888a2faf63f239d73d8ba0011b2dccc97a\n\t\tUPDATE_COMMAND \"\"\n\t\tCONFIGURE_COMMAND cmake -DCMAKE_INSTALL_PREFIX=${GFLAGS_ROOT_DIR} -DBUILD_SHARED_LIBS=ON -DBUILD_STATIC_LIBS=ON -DGFLAGS_NAMESPACE=google .\n\t\tBUILD_COMMAND make\n\t\tBUILD_IN_SOURCE true\n\t\tINSTALL_COMMAND make install\n\t\tINSTALL_DIR ${GFLAGS_ROOT_DIR}\n\t\t)\n\n\tExternalProject_Get_Property(Gflags INSTALL_DIR)\n\tset(GFLAGS_INCLUDE_DIR ${INSTALL_DIR}/include)\n\tmessage(\"GFLAGS_INCLUDE_DIR: \" ${GFLAGS_INCLUDE_DIR})\n\tset(GFLAGS_LIBRARY ${INSTALL_DIR}/lib/${LIBRARY_PREFIX}gflags${LIBRARY_SUFFIX})\n\tmessage(\"GFLAGS_LIBRARY: \" ${GFLAGS_LIBRARY})\n\n\tFIND_PACKAGE_HANDLE_STANDARD_ARGS(Gflags DEFAULT_MSG\n\t\tGFLAGS_INCLUDE_DIR GFLAGS_LIBRARY\n\t\t)\n\tset(GFLAGS_LIBRARIES ${GFLAGS_LIBRARY})\n\tset(GFLAGS_INCLUDE_DIRS ${GFLAGS_INCLUDE_DIR})\n\tmark_as_advanced(GFLAGS_LIBRARIES GFLAGS_INCLUDE_DIRS)\nendmacro()\n```\n`ExternalProject_Add`从`gayhub`上下载gflags源码，然后通过一系列编译安装操作，本地某临时目录就安装好GFlags了。里面几个参数`CONFIGURE_COMMAND`、`BUILD_COMMAND`、`INSTALL_COMMAND`都是项目常用的流程，还是比较舒服滴。安装好后，我们通过`ExternalProject_Get_Property`获取到真实安装路径，就可以像上面第一步那样设置`GFLAGS_INCLUDE_DIR`和`GFLAGS_LIBRARY`啦，后面都一样一样的。\n\n## 回顾\n是不是很简单？那咱总结下里面的关键点吧。\n  1. 先使用`find_path`、`find_library`查找系统头文件和库，找到了设置`GFLAGS_INCLUDE_DIR`和`GFLAGS_LIBRARY`变量\n  2. 本地没找到，就用`ExternalProject_Add`从某地下载安装，然后设置`GFLAGS_INCLUDE_DIR`和`GFLAGS_LIBRARY`变量\n  3. 使用`FIND_PACKAGE_HANDLE_STANDARD_ARGS`来验证上面两个变量，验证没问题则`GFLAGS_FOUND`为`True`\n\n那主程序那边怎么用呢？上面那些东西都保存为`third_party/FindGflags.cmake`文件，在我们的`CMakeLists.txt`里把它导入就行啦。\n```\n# 导入FindGflags.cmake\nlist(APPEND CMAKE_MODULE_PATH ${CMAKE_SOURCE_DIR}/third_party)\n\n# 查找gflags\nfind_package(Gflags)\n\n# 主程序头文件搜索路径\ninclude_directories(\n  ${CMAKE_CURRENT_SOURCE_DIR}\n  ${GFLAGS_INCLUDE_DIRS}\n  )\n\n# 主目标\nadd_executable(hello hello.cc)\ntarget_link_libraries(\n  hello \n  ${GFLAGS_LIBRARIES}\n  )\n```\n\n最后，几个常用的第三方库都用这种方式实现了下，主要是我自己用的哦。\n\n<a href=\"https://github.com/formath/cmake_third_party\" target=\"_blank\">https://github.com/formath/cmake_third_party</a>\n","tags":["CMake"],"categories":["tech"]},{"title":"SGD优化算法的各种变体","url":"/f4f9dedd/","content":"\n<!-- toc -->\n\n## 背景\n2017年前，公司内部的算法团队还都是使用`XGBoost`来训练模型，手动构造的特征已经几百个，特征迭代效果微弱，但在搜索推荐场景下，有大规模的离散特征，这类特征记忆效果非常好，如果加入模型训练会获得不错的效果提升，但树模型并不适合大规模离散特征，所以我开发了一个基于`Parameter Server`架构的分布式机器学习框架，主要支持大规模离散的浅层模型，比如`Logistic Regression`、`Factorization Machine`、`Field-aware Factorization Machine`分类模型以及对应的回归模型和`SVD分解`。这个机器学习框架使用`Yarn`调度在公司的大数据集群上，在线上取得了非常不错的收益，框架后续又开始朝着深度模型和在线学习演化，目前公司算法团队已经基本往大规模离散DNN迁移完毕。这里主要记录一下训练框架支持的一些优化算法，公式脑子只能记个大概，还是写下来方便以后查阅。\n```\n窄的深度模型 -> 宽的浅层模型 -> 又宽又深的模型 -> 秒级在线更新的又宽又深的模型\n```\n\n## 优化算法\n* SGD with Momentum\n* Nesterov accelerated gradient\n* SGD-L1(Cumulative)\n* AdaGrad\n* RMSProp\n* AdaDelta\n* Adam\n* AdamW\n* FTRL\n* FTML\n\n<br>\n\n### SGD with Momentum\n超参数：\n\n* 学习率 $\\eta \\in (0, +\\infty) \\quad [default=0.001]$\n* 惯性项 $\\rho \\in (0, 1) \\quad [default=0.9]$\n\n更新公式：\n$$ init: \\quad \\Delta w_0 = 0, \\; t = 0 $$\n$$ t = t + 1 $$\n$$ g_t = \\nabla_w f_t(w_{t-1}) $$\n$$ \\Delta w_t = \\rho \\cdot \\Delta w_{t-1} + \\eta \\cdot g_t $$\n$$ w_{t+1} = w_t - \\Delta w_t $$\n\n### Nesterov accelerated gradient\n超参数：\n\n* 学习率 $\\eta \\in (0, +\\infty) \\quad [default=0.001]$\n* 惯性项 $\\rho \\in (0, 1) \\quad [default=0.9]$\n\n更新公式：\n$$ init: \\quad \\Delta w_0 = 0, \\; t = 0 $$\n$$ t = t + 1 $$\n$$ w_{t-\\frac{1}{2}} = w_t - \\rho \\cdot \\Delta w_{t-1} $$\n$$ g_t = \\nabla_w f_t(w_{t-\\frac{1}{2}}) $$\n$$ \\Delta w_t = \\rho \\cdot \\Delta w_{t-1} + \\eta \\cdot g_t $$\n$$ w_{t+1} = w_t - \\Delta w_t $$\n\n### SGD-L1(Cumulative)\n超参数：\n\n* 学习率 $\\eta \\in (0, +\\infty) \\quad [default=0.001]$\n* 学习率衰减 $\\alpha \\in (0, 1) \\quad [default=0.9]$\n* 学习率衰减步长 $\\tau \\in (1, +\\infty) \\quad [default=100]$\n* L1正则化项 $\\lambda_1 \\in [0, +\\infty) \\quad [default=0.001]$\n\n更新公式：\n$$ init: \\quad q_0 = 0, \\; u_0 = 0, \\; t = 0 $$\n$$ t = t + 1 $$\n$$ g_t = \\nabla_w f_t(w_{t-1}) $$\n$$ \\eta_t = \\eta \\cdot \\alpha^\\frac{t}{\\tau}$$\n$$ u_t = u_{t-1} + \\eta_t \\cdot \\lambda_1 $$\n$$ w_{t-\\frac{1}{2}} = w_{t-1} - \\eta_t \\cdot g_t $$\n$$ w_t = max(0, w_{t-\\frac{1}{2}}-(u_t + q_{t-1})) \\quad if \\quad w_{t-\\frac{1}{2}}>0 \\quad else \\quad min(0, w_{t-\\frac{1}{2}}+(u_t - q_{t-1})) $$\n$$ q_t = q_{t-1} + (w_t - w_{t-\\frac{1}{2}}) $$\n\n### AdaGrad\n超参数：\n\n* 学习率 $\\eta \\in (0, +\\infty) \\quad [default=0.001]$\n\n更新公式：\n$$ init: \\quad q_0 = 0, \\; t = 0 $$\n$$ t = t + 1 $$\n$$ g_t = \\nabla_w f_t(w_{t-1}) $$\n$$ q_t = q_{t-1} + g_t^2 $$\n$$ w_{t} = w_{t-1} - \\frac{\\eta}{\\sqrt{q_t + \\epsilon}} \\cdot g_t $$\n\n### RMSProp\n超参数：\n\n* 惯性项 $\\rho \\in [0, 1) \\quad [default=0.9]$\n* 学习率 $\\eta \\in (0, +\\infty) \\quad [default=0.001]$\n\n更新公式：\n$$ init: \\quad q_0 = 0, \\; t = 0 $$\n$$ t = t + 1 $$\n$$ g_t = \\nabla_w f_t(w_{t-1}) $$\n$$ q_t = \\rho \\cdot q_{t-1} + (1 - \\rho) \\cdot g_t^2 $$\n$$ w_{t} = w_{t-1} - \\frac{\\eta}{\\sqrt{q_t + \\epsilon}} \\cdot g_t $$\n\n### AdaDelta\n超参数：\n\n*  惯性项 $\\rho \\in [0, 1) \\quad [default=0.95]$\n\n更新公式：\n$$ init: \\quad E[g^2]_0 = 0, \\; E[\\Delta w^2]_0 = 0, \\; t = 0 $$\n$$ t = t + 1 $$\n$$ g_t = \\nabla_w f_t(w_{t-1}) $$\n$$ E[g^2]_t = \\rho \\cdot E[g^2]_{t-1} + (1 - \\rho) \\cdot g_t^2 $$\n$$ \\Delta w_t = - \\frac{\\sqrt{E[\\Delta w^2]_{t-1} + \\epsilon}}{\\sqrt{E[g^2]_t + \\epsilon}} \\cdot g_t $$\n$$ E[\\Delta w^2]_t = \\rho \\cdot E[\\Delta w^2]_{t-1} + (1 - \\rho) \\cdot \\Delta w_t^2 $$\n$$ w_{t} = w_{t-1} + \\Delta w_t $$\n\n### Adam\n超参数：\n\n* 学习率 $\\eta \\in (0, +\\infty) \\quad [default=0.001]$\n* 惯性项 $\\rho_1 \\in [0, 1) \\quad [default=0.9]$\n* 惯性项 $\\rho_2 \\in [0, 1) \\quad [default=0.999]$\n* 衰减率 $\\lambda \\in [0, 1] \\quad [default=1-10^{-8}]$\n\n更新公式：\n$$ init: \\quad m_0 = 0, \\; v_0 = 0, \\; t = 0 $$\n$$ t = t + 1 $$\n$$ \\rho_{1,t} = \\rho_1 \\cdot \\lambda^{t-1} $$\n$$ g_t = \\nabla_w f_t(w_{t-1}) $$\n$$ m_t = \\rho_{1,t} \\cdot m_{t-1} + (1 - \\rho_{1,t}) \\cdot g_t $$\n$$ v_t = \\rho_2 \\cdot v_{t-1} + (1 - \\rho_2) \\cdot g_t^2 $$\n$$ \\widehat{m_t} = \\frac{m_t}{(1 - \\rho_1^t)} $$\n$$ \\widehat{v_t} = \\frac{v_t}{(1 - \\rho_2^t)} $$\n$$ w_t = w_{t-1} - \\eta \\cdot \\frac{\\widehat{m_t}}{\\sqrt{\\widehat{v_t}} + \\epsilon} $$\n\n### AdamW\n超参数：\n\n* 学习率 $\\eta \\in (0, +\\infty) \\quad [default=0.001]$\n* 惯性项 $\\rho_1 \\in [0, 1) \\quad [default=0.9]$\n* 惯性项 $\\rho_2 \\in [0, 1) \\quad [default=0.999]$\n* 衰减率 $\\lambda \\in [0, 1] \\quad [default=1-10^{-8}]$\n* 正则项 $\\gamma \\in [0, 1) \\quad [default=10^{-4}]$\n\n更新公式：\n$$ init: \\quad m_0 = 0, \\; v_0 = 0, \\; t = 0 $$\n$$ t = t + 1 $$\n$$ \\rho_{1,t} = \\rho_1 \\cdot \\lambda^{t-1} $$\n$$ g_t = \\nabla_w f_t(w_{t-1})$$\n$$ m_t = \\rho_{1,t} \\cdot m_{t-1} + (1 - \\rho_{1,t}) \\cdot g_t $$\n$$ v_t = \\rho_2 \\cdot v_{t-1} + (1 - \\rho_2) \\cdot g_t^2 $$\n$$ \\widehat{m_t} = \\frac{m_t}{(1 - \\rho_1^t)} $$\n$$ \\widehat{v_t} = \\frac{v_t}{(1 - \\rho_2^t)} $$\n$$ w_t = w_{t-1} - \\eta \\cdot (\\frac{\\widehat{m_t}}{\\sqrt{\\widehat{v_t}} + \\epsilon} + \\gamma \\cdot w_{t-1}) $$\n\n### FTRL\n超参数：\n\n* L1正则化项 $\\lambda_1 \\in [0, +\\infty) \\quad [default=0.001]$\n* L2正则化项 $\\lambda_2 \\in [0, +\\infty) \\quad [default=0.001]$\n* 学习率控制 $\\alpha \\in (0, +\\infty) \\quad [default=0.01]$\n* 学习率控制 $\\beta \\in [0, +\\infty) \\quad [default=1.0]$\n\n更新公式：\n$$ init: \\quad q_0 = 0, \\; z_0 = 0, \\; t = 0 $$\n$$ t = t + 1 $$\n$$ g_t = \\nabla_w f_t(w_{t-1}) $$\n$$ \\sigma_t = \\frac{\\sqrt{q_{t-1} + g_t^2}-\\sqrt{q_{t-1}}}{\\alpha} $$\n$$ q_t = q_{t-1} + g_t^2 $$\n$$ z_t = z_{t-1} + g_t - \\sigma_t \\cdot w_{t-1} $$\n$$ w_{t} = \\frac{sign(z_t) \\cdot \\lambda_1 - z_t}{\\lambda_2 + \\frac{\\beta + \\sqrt{q_t}}{\\alpha}} \\quad if \\quad z_t > \\lambda_1 \\quad else \\quad 0 $$\n\n### FTML\n超参数：\n\n* 惯性项控制 $\\rho_1 \\in [0, 1) \\quad [default=0.6]$\n* 惯性项控制 $\\rho_2 \\in [0, 1) \\quad [default=0.999]$\n* 学习率 $\\eta \\in (0, +\\infty) \\quad [default=0.001]$\n\n更新公式：\n$$ init: \\quad d_0 = 0, \\; v_0 = 0, \\; z_0 = 0 $$\n$$ t = t + 1 $$\n$$ g_t = \\nabla_w f_t(w_{t-1}) $$\n$$ v_t = \\rho_2 \\cdot v_{t-1} + (1 - \\rho_2) \\cdot g_t^2 $$\n$$ d_t = \\frac{1 - \\rho_1^t}{\\eta} \\cdot (\\sqrt{\\frac{v_t}{1 - \\rho_2^t}} + \\epsilon) $$\n$$ \\sigma_t = d_t - \\rho_1 \\cdot d_{t-1} $$\n$$ z_t = \\rho_1 \\cdot z_{t-1} + (1 - \\rho_1) \\cdot g_t - \\sigma_t \\cdot w_{t-1} $$\n$$ w_t = - \\frac{z_t}{d_t} $$\n\n### 参考文献\n\n* Nesterov, Y. (1983). A method for unconstrained convex minimization problem with the rate of convergence o(1/k2). Doklady ANSSSR (translated as Soviet.Math.Docl.), vol. 269, pp. 543– 547.\n* Qian, N. (1999). On the momentum term in gradient descent learning algorithms. Neural Networks : The Official Journal of the International Neural Network Society, 12(1), 145–151.\n* Tsuruoka, Yoshimasa, Jun'ichi Tsujii, and Sophia Ananiadou. \"Stochastic gradient descent training for l1-regularized log-linear models with cumulative penalty.\" Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 1-Volume 1. Association for Computational Linguistics, 2009.\n* Duchi, John, Elad Hazan, and Yoram Singer. \"Adaptive subgradient methods for online learning and stochastic optimization.\" Journal of Machine Learning Research 12.Jul (2011): 2121-2159.\n* Tieleman T., Hinton G. Lecture 6.5-rmsprop: divide the gradient by a running average of its recent magnitude. COURSERA: Neural networks for machine learning, 4(2), 26–31 (2012).\n* Zeiler, Matthew D. \"ADADELTA: an adaptive learning rate method.\" arXiv preprint arXiv:1212.5701 (2012).\n* Kingma, Diederik, and Jimmy Ba. \"Adam: A method for stochastic optimization.\" arXiv preprint arXiv:1412.6980 (2014).\n* Ilya Loshchilov, Frank Hutter. \"Decoupled Weight Decay Regularization.\" arXiv preprint arXiv:1711.05101 (2017).\n* H. Brendan McMahan & M Streter. Adaptive Bound Optimization for Online Convex Optimization. In COLT, 2010\n* H. Brendan McMahan. Follow-the-Regularized-Leader and Mirror Descent: Equivalence Theorems and L1 Regularization. In AISTATS, 2011\n* H. Brendan McMahan, Gary Holt, D. Sculley, Michael Young, Dietmar Ebner, Julian Grady, Lan Nie, Todd Phillips, Eugene Davydov, Daniel Golovin, Sharat Chikkerur, Dan Liu, Martin Wattenberg, Arnar Mar Hrafnkelsson, Tom Boulos, Jeremy Kubica, Ad Click Prediction: a View from the Trenches. In ACM SIGKDD, 2013\n* Shuai Zheng, James T. Kwok. Follow the Moving Leader in Deep Learning. The 34th International Conference on Machine Learning (ICML), Sydney, Australia, August 2017","tags":["Numerical-Optimization","Machine-Learning","Deep-Learning"],"categories":["tech"]},{"title":"Hadoop平台上生成TensorFlow TFRecord的错误问题","url":"/ca93562e/","content":"\n<!-- toc -->\n\n## 背景\n使用MapReduce on Yarn或者Spark on Yarn来生成TFRecord的过程中，会发生Hadoop和TensorFlow依赖的Protobuf版本不一致导致冲突的问题。\n\n## 解决方案\n\n### 方案一\n在运行时不要指定胖jar包，通过libjars命令指定需要的protobuf版本。\n```\nexport HADOOP_CLASSPATH=${LIB_PATH}/*\n\nhadoop jar your_tfrecord.jar \\\n    your_class \\\n    -Dmapreduce.job.user.classpath.first=true \\\n    -libjars ${LIB_PATH}/protobuf-java-3.3.1.jar,${LIB_PATH}/tensorflow-hadoop-1.0.jar\n```\n\n### 方案二\n使用胖jar包，把需要用到的jar包中的类重命名，在程序中调用重命名后的类，避免和集群上低版本的jar包冲突。在pom.xml里添加下面的配置，`com.google`开头类换成`third.com.google`。\n```\n<build>\n    <plugins>\n        <plugin>\n              <groupId>org.apache.maven.plugins</groupId>\n              <artifactId>maven-shade-plugin</artifactId>\n              <version>3.1.1</version>\n              <executions>\n                  <execution>\n                      <phase>package</phase>\n                      <goals>\n                          <goal>shade</goal>\n                      </goals>\n                      <configuration>\n                          <filters>\n                              <filter>\n                                  <artifact>*:*</artifact>\n                                  <excludes>\n                                      <exclude>META-INF/*.SF</exclude>\n                                      <exclude>META-INF/*.DSA</exclude>\n                                      <exclude>META-INF/*.RSA</exclude>\n                                      <exclude>META-INF/LICENSE</exclude>\n                                  </excludes>\n                              </filter>\n                          </filters>\n                          <transformers>\n                              <transformer implementation=\"org.apache.maven.plugins.shade.resource.AppendingTransformer\">\n                                  <resource>META-INF/spring.handlers</resource>\n                              </transformer>\n                              <transformer implementation=\"org.apache.maven.plugins.shade.resource.AppendingTransformer\">\n                                  <resource>META-INF/spring.schemas</resource>\n                              </transformer>\n                          </transformers>\n                          <!-- 当protobuf、guava等冲突时，将com.google开头的类转换成third.com.google开头 -->\n                          <relocations>\n                              <relocation>\n                                  <pattern>com.google</pattern>\n                                  <shadedPattern>third.com.google</shadedPattern>\n                              </relocation>\n                          </relocations>\n                      </configuration>\n                  </execution>\n              </executions>\n        </plugin>\n    </plugins>\n  </build>\n```\n\n## 参考文档\n* [maven relocating class](https://maven.apache.org/plugins/maven-shade-plugin/examples/class-relocation.html)\n* [maven-shade-plugin入门指南](https://www.jianshu.com/p/7a0e20b30401)\n","tags":["TensorFlow"],"categories":["tech"]},{"title":"VSCode下Markdown和LaTex混合编辑环境配置","url":"/ddd6f979/","content":"\n<!-- toc -->\n\n## 背景\n最近在公司用Markdown写技术博客时，需要插入伪代码块，而Markdown里没有找到比较美观的实现方案，想到之前在学校写论文时一直用LaTex，里面的伪代码非常漂亮，所以就想在Markdown里混合使用LaTex，下面是从网上找到的解决方案汇总，方便自己以后查阅。\n\n## 安装Tex环境\n* 从以下网址下载Tex环境后安装，这个是MacOS下的Tex，Windows系统的是别的。\n```\nhttp://www.tug.org/mactex/mactex-download.html\n```\n\n## VSCode配置\n* 安装`LaTeX Workshop`插件\n* `settings.json`增加以下配置\n```\n\"latex-workshop.latex.recipes\": [\n        {\n            \"name\": \"xelatex\",\n            \"tools\": [\n                \"xelatex\"\n            ]\n        },\n        {\n            \"name\": \"latexmk\",\n            \"tools\": [\n                \"latexmk\"\n            ]\n        },\n        {\n            \"name\": \"pdflatex -> bibtex -> pdflatex*2\",\n            \"tools\": [\n                \"pdflatex\",\n                \"bibtex\",\n                \"pdflatex\",\n                \"pdflatex\"\n            ]\n        }\n    ],\n    \"latex-workshop.latex.tools\": [\n        {\n            \"name\": \"latexmk\",\n            \"command\": \"latexmk\",\n            \"args\": [\n                \"-synctex=1\",\n                \"-interaction=nonstopmode\",\n                \"-file-line-error\",\n                \"-pdf\",\n                \"%DOC%\"\n            ]\n        },\n        {\n            \"name\": \"xelatex\",\n            \"command\": \"xelatex\",\n            \"args\": [\n                \"-synctex=1\",\n                \"-interaction=nonstopmode\",\n                \"-file-line-error\",\n                \"%DOC%\"\n            ]\n        },\n        {\n            \"name\": \"pdflatex\",\n            \"command\": \"pdflatex\",\n            \"args\": [\n                \"-synctex=1\",\n                \"-interaction=nonstopmode\",\n                \"-file-line-error\",\n                \"%DOC%\"\n            ]\n        },\n        {\n            \"name\": \"bibtex\",\n            \"command\": \"bibtex\",\n            \"args\": [\n                \"%DOCFILE%\"\n            ]\n        }\n    ],\n    \"latex-workshop.view.pdf.viewer\": \"tab\",\n    \"latex-workshop.latex.clean.fileTypes\": [\n        \"*.aux\",\n        \"*.bbl\",\n        \"*.blg\",\n        \"*.idx\",\n        \"*.ind\",\n        \"*.lof\",\n        \"*.lot\",\n        \"*.out\",\n        \"*.toc\",\n        \"*.acn\",\n        \"*.acr\",\n        \"*.alg\",\n        \"*.glg\",\n        \"*.glo\",\n        \"*.gls\",\n        \"*.ist\",\n        \"*.fls\",\n        \"*.log\",\n        \"*.fdb_latexmk\"\n    ],\n```\n\n## 中文字体支持\n完成上面工作后，就可以在VSCode下编辑和预览Tex文件了。不过中文支持不好，需要按下面操作兼容中文字体。\n* 打开`~/.pandoc/templates/default.latex`文件，把以下内容填进去。\n```\n\\documentclass[$if(fontsize)$$fontsize$,$endif$$if(lang)$$lang$,$endif$$if(papersize)$$papersize$,$endif$]{$documentclass$}\n    \\usepackage{geometry}       % 設定邊界\n    \\geometry{\n      top=1in,\n      inner=1in,\n      outer=1in,\n      bottom=1in,\n      headheight=3ex,\n      headsep=2ex\n    }\n    \\usepackage[T1]{fontenc}\n    \\usepackage{lmodern}\n    \\usepackage{amssymb,amsmath}\n    \\usepackage{ifxetex,ifluatex}\n    \\usepackage{fixltx2e} % provides \\textsubscript\n    % use upquote if available, for straight quotes in verbatim environments\n    \\IfFileExists{upquote.sty}{\\usepackage{upquote}}{}\n    \\ifnum 0\\ifxetex 1\\fi\\ifluatex 1\\fi=0 % if pdftex\n      \\usepackage[utf8]{inputenc}\n    $if(euro)$\n      \\usepackage{eurosym}\n    $endif$\n    \\else % if luatex or xelatex\n      \\usepackage{fontspec}     % 允許設定字體\n      \\usepackage{xeCJK}        % 分開設置中英文字型\n      \\setCJKmainfont{STSong}   % 設定中文字型\n      \\setmainfont{Georgia}     % 設定英文字型\n      \\setromanfont{Georgia}    % 字型\n      \\setmonofont{Courier New}\n      \\linespread{1.2}\\selectfont   % 行距\n      \\XeTeXlinebreaklocale \"zh\"    % 針對中文自動換行\n      \\XeTeXlinebreakskip = 0pt plus 1pt % 字與字之間加入0pt至1pt的間距，確保左右對整齊\n      \\parindent 0em        % 段落縮進\n      \\setlength{\\parskip}{20pt}    % 段落之間的距離\n      \\ifxetex\n        \\usepackage{xltxtra,xunicode}\n      \\fi\n      \\defaultfontfeatures{Mapping=tex-text,Scale=MatchLowercase}\n      \\newcommand{\\euro}{€}\n    $if(mainfont)$\n        \\setmainfont{$mainfont$}\n    $endif$\n    $if(sansfont)$\n        \\setsansfont{$sansfont$}\n    $endif$\n    $if(monofont)$\n        \\setmonofont{$monofont$}\n    $endif$\n    $if(mathfont)$\n        \\setmathfont{$mathfont$}\n    $endif$\n    \\fi\n    % use microtype if available\n    \\IfFileExists{microtype.sty}{\\usepackage{microtype}}{}\n    $if(geometry)$\n    \\usepackage[$for(geometry)$$geometry$$sep$,$endfor$]{geometry}\n    $endif$\n    $if(natbib)$\n    \\usepackage{natbib}\n    \\bibliographystyle{plainnat}\n    $endif$\n    $if(biblatex)$\n    \\usepackage{biblatex}\n    $if(biblio-files)$\n    \\bibliography{$biblio-files$}\n    $endif$\n    $endif$\n    $if(listings)$\n    \\usepackage{listings}\n    $endif$\n    $if(lhs)$\n    \\lstnewenvironment{code}{\\lstset{language=Haskell,basicstyle=\\small\\ttfamily}}{}\n    $endif$\n    $if(highlighting-macros)$\n    $highlighting-macros$\n    $endif$\n    $if(verbatim-in-note)$\n    \\usepackage{fancyvrb}\n    $endif$\n    $if(tables)$\n    \\usepackage{longtable}\n    $endif$\n    $if(graphics)$\n    \\usepackage{graphicx}\n    % We will generate all images so they have a width \\maxwidth. This means\n    % that they will get their normal width if they fit onto the page, but\n    % are scaled down if they would overflow the margins.\n    \\makeatletter\n    \\def\\maxwidth{\\ifdim\\Gin@nat@width>\\linewidth\\linewidth\n    \\else\\Gin@nat@width\\fi}\n    \\makeatother\n    \\let\\Oldincludegraphics\\includegraphics\n    \\renewcommand{\\includegraphics}[1]{\\Oldincludegraphics[width=\\maxwidth]{#1}}\n    $endif$\n    \\ifxetex\n      \\usepackage[setpagesize=false, % page size defined by xetex\n                  unicode=false, % unicode breaks when used with xetex\n                  xetex]{hyperref}\n    \\else\n      \\usepackage[unicode=true]{hyperref}\n    \\fi\n    \\hypersetup{breaklinks=true,\n                bookmarks=true,\n                pdfauthor={$author-meta$},\n                pdftitle={$title-meta$},\n                colorlinks=true,\n                urlcolor=$if(urlcolor)$$urlcolor$$else$blue$endif$,\n                linkcolor=$if(linkcolor)$$linkcolor$$else$magenta$endif$,\n                pdfborder={0 0 0}}\n    \\urlstyle{same}  % don't use monospace font for urls\n    $if(links-as-notes)$\n    % Make links footnotes instead of hotlinks:\n    \\renewcommand{\\href}[2]{#2\\footnote{\\url{#1}}}\n    $endif$\n    $if(strikeout)$\n    \\usepackage[normalem]{ulem}\n    % avoid problems with \\sout in headers with hyperref:\n    \\pdfstringdefDisableCommands{\\renewcommand{\\sout}{}}\n    $endif$\n    \\setlength{\\parindent}{0pt}\n    %\\setlength{\\parskip}{6pt plus 2pt minus 1pt}\n    \\setlength{\\emergencystretch}{3em}  % prevent overfull lines\n\n    \\title{\\huge 在OSX平台上的XeLaTeX中文測試} % 設置標題，使用巨大字體\n    \\author{FoolEgg.com}        % 設置作者\n    \\date{February 2013}        % 設置日期\n    \\usepackage{titling}\n    \\setlength{\\droptitle}{-8em}    % 將標題移動至頁面的上面\n\n    \\usepackage{fancyhdr}\n    \\usepackage{lastpage}\n    \\pagestyle{fancyplain}\n\n    $if(numbersections)$\n    \\setcounter{secnumdepth}{5}\n    $else$\n    \\setcounter{secnumdepth}{0}\n    $endif$\n    $if(verbatim-in-note)$\n    \\VerbatimFootnotes % allows verbatim text in footnotes\n    $endif$\n    $if(lang)$\n    \\ifxetex\n      \\usepackage{polyglossia}\n      \\setmainlanguage{$mainlang$}\n    \\else\n      \\usepackage[$lang$]{babel}\n    \\fi\n    $endif$\n    $for(header-includes)$\n    $header-includes$\n    $endfor$\n\n    $if(title)$\n    \\title{$title$}\n    $endif$\n    \\author{$for(author)$$author$$sep$ \\and $endfor$}\n    \\date{$date$}\n\n    \\begin{document}\n    $if(title)$\n    \\maketitle\n    $endif$\n\n    $for(include-before)$\n    $include-before$\n\n    $endfor$\n    $if(toc)$\n    {\n    \\hypersetup{linkcolor=black}\n    \\setcounter{tocdepth}{$toc-depth$}\n    \\tableofcontents\n    }\n    $endif$\n    $body$\n\n    $if(natbib)$\n    $if(biblio-files)$\n    $if(biblio-title)$\n    $if(book-class)$\n    \\renewcommand\\bibname{$biblio-title$}\n    $else$\n    \\renewcommand\\refname{$biblio-title$}\n    $endif$\n    $endif$\n    \\bibliography{$biblio-files$}\n\n    $endif$\n    $endif$\n    $if(biblatex)$\n    \\printbibliography$if(biblio-title)$[title=$biblio-title$]$endif$\n\n    $endif$\n    $for(include-after)$\n    $include-after$\n\n    $endfor$\n    \\end{document}\n```\n\n## Markdown里使用LaTex\nMarkdown文件里可以插入LaTex片段，可以通过Pacdoc将md文件转成pdf或其他文件。\n\n* 新建一个`md`文件`test.md`，填入以下内容\n```\n## 背景\nMacOS LaTex环境配置\n\n## 嵌入latex算法伪代码\n\n---\nheader-includes:\n  - \\usepackage[ruled,vlined,linesnumbered]{algorithm2e}\n---\n# Algorithm 1\nJust a sample algorithmn\n\\begin{algorithm}[H]\n\\DontPrintSemicolon\n\\SetAlgoLined\n\\KwResult{Write here the result}\n\\SetKwInOut{Input}{Input}\\SetKwInOut{Output}{Output}\n\\Input{Write here the input}\n\\Output{Write here the output}\n\\BlankLine\n\\While{While condition}{\n    instructions\\;\n    \\eIf{condition}{\n        instructions1\\;\n        instructions2\\;\n    }{\n        instructions3\\;\n    }\n}\n\\caption{While loop with If/Else condition}\n\\end{algorithm} \n```\n\n* md生成pdf\n```\npandoc --pdf-engine=xelatex test.md -o test.pdf\n```\n或者\n```\npandoc --pdf-engine=xelatex --template=[template.latex的路径] test.md -o test.pdf\n```\n不配置`--template`时默认使用`~/.pandoc/templates/default.latex`。\n\n## 总结\n在VSCode里，如果Markdown文件里有LaTex片段的话，是无法完全预览的，所以只能用Pandoc转成pdf来看效果，如果你有更好的办法欢迎提供。","tags":["Latex","Markdown"],"categories":["tech"]},{"title":"MXNet之NNVM代码简析","url":"/91c8dd96/","content":"\n<!-- toc -->\n\n## Op\n\nclass Op: 每个OP都有name和desc，还有argument和attr\nattr都保存在全局的OpManager里。\n有同一个name的Op不一定是同一个Op，比如ElementWiseSum可能在不同的Node里会有不同数量的input，每个不同node里的Op都会生成一个Op实例，虽然这些Op名字一样，但attr不一样。不同的Op其实是用Op*或Op.index来区分，Op.index就是这个Op加入OpManager的序号\n静态属性是在注册Op时确定的；动态属性（输入个数等）是在创建Node时由node的attr来确定的，创建时会调用op.parse(node.attr)。\n\n## Symbol\nSymbol里只有vector<DataEntry> outputs\nDataEntry可以把Node串起来\n\n## Graph\nSymbol提供了许多图的接口，便于前端访问，而Graph里面没几个接口，主要就是有个indexed_graph，便于底层训练时快速访问。Symbol非常灵活，以后有可能支持动态图，但每次动态变化后都要先转成Graph，底层不太支持动态度，因为都是vector用index来索引node，不太适合中间插入一个node。\n\n里面有vector<DataEntry> outputs\nindexed_graph和attr\n\n每个graph有以下attr：\n网络结构的json string\n每个NodeEntry的TShape的vector<TShape>\n每个NodeEntry的dtype的vector<int>\n每个NodeEntry的storage_id的vector<int>\n每个operator的device的vector<int>\n每个operator的device的unordered_map<string, int>\n\n## IndexedGraph\n里面一堆vector，把Graph里的node映射成vector的下标，方便快速访问。提供了一些类似Symbol里的接口，不过主要根据node的index来访问vector，而Symbol更像通过node组成的DAG来访问。\n\n# GraphExecutor\n\n## InitFullGraph\n做一件事，生成带backward的Graph，调用了nnvm::pass::Gradient\ncopy_op，消除重复，比如`c=a+b`，`dc/da`和`dc/db`其实一样，只用一个NodeEntry表示就行了。\n\n## AssignContext\n只做一件事，设置graph的两个属性：g.attr[\"context\"]和g.attr[\"device\"]，里面会调用nnvm::pass::PlaceDevice。\n设置了in_arg、auc_arg和arg_grad的context，中间节点的context在哪里设置的？猜测可能在PlaceDevice里面\n\n## InferShape&InferType&InferStorageType\n调用nnvm::pass::InferXXX推断，做的事情很类似，主要是设置g.attr[\"shape\"]、g.attr[\"dtype\"]、g.attr[\"storage_type\"]属性\n\n## InitArguments\n使用用户提供的参数初始化g里面的数据，g.data_entry_、g.grad_store_等\n有一个版本是带shared_buffer的\n\n## FinishInitGraph\n设置g.attrs[\"dispatch_stypes\"]和g.attrs[\"storage\"]等属性\n\n###AttachOpExecs&AttachOpResources\n遍历g，为每个节点生成一个OpExecutor，设置g.attrs[\"op_execs\"]属性。\n有三种类型Op：1）带状态的FStatefulCompute 2）backward节点，使用forward的State 3）普通节点FCompute\n\n### InitDataEntryMemory\n为data_entry_里的每个数据生成一个NDArray\n\n### InitCachedOps\n使用g.attrs[\"op_execs\"],初始化op_nodes_，里面的每个元素都是一个Engine可执行的Op，包括了use_vars\\mutute_vars等\n\n### InitOpSegs\n把上一步的op_nodes_分段隔成多个大的exec，每个大exec里其实有许多小exec，内部按序执行小exec，大exec的use_vars和mutute_vars是小exec的汇总，这样在engine里大exec作为一个执行单元\n","tags":["Deep-Learning","MXNet"],"categories":["tech"]},{"title":"联系博主","url":"/5c3de5e6/","content":"\n### 联系\n\n* WeChat: ForMath\n","categories":["author"]},{"url":"/about/index.html","content":"\n### 联系\n\n* WeChat: ForMath\n"},{"title":"Tags","url":"/tags/index.html"}]